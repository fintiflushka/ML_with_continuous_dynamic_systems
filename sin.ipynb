{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from msalib import layers\n",
    "from msalib import network\n",
    "from msalib import train\n",
    "from msalib.utils import Dataset\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(x):\n",
    "    \"\"\"Target function\n",
    "\n",
    "    Arguments:\n",
    "        x {numpy array} -- input\n",
    "\n",
    "    Returns:\n",
    "        numpy array -- output\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sin(x)\n",
    "\n",
    "\n",
    "def loss_func(output, label):\n",
    "    \"\"\"Loss function (l2)\n",
    "\n",
    "    Arguments:\n",
    "        output {tf tensor} -- output from network\n",
    "        label {tf tensor} -- labels\n",
    "\n",
    "    Returns:\n",
    "        tf tensor -- loss\n",
    "    \"\"\"\n",
    "\n",
    "    output = tf.reduce_sum(output, axis=1, keepdims=True)\n",
    "    return tf.losses.mean_squared_error(\n",
    "        predictions=output, labels=label)\n",
    "\n",
    "\n",
    "def generate_datasets(config):\n",
    "    \"\"\"Generate synthetic dataset\n",
    "\n",
    "    Arguments:\n",
    "        config {dict} -- read from config.yml\n",
    "\n",
    "    Returns:\n",
    "        tuple of Dataset objects -- train/test sets\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(config['seed'])\n",
    "    num_train, num_test = config['num_train'], config['num_test']\n",
    "    x_train = np.random.uniform(-np.pi, np.pi, (num_train, 1))\n",
    "    x_test = np.random.uniform(-np.pi, np.pi, (num_test, 1))\n",
    "    y_train, y_test = target(x_train), target(x_test)\n",
    "    x_train = np.repeat(x_train, axis=1, repeats=config['num_nodes'])\n",
    "    x_test = np.repeat(x_test, axis=1, repeats=config['num_nodes'])\n",
    "    trainset = Dataset(data=x_train, target=y_train)\n",
    "    testset = Dataset(data=x_test, target=y_test)\n",
    "    return trainset, testset\n",
    "\n",
    "\n",
    "def run_toy(config):\n",
    "    \"\"\"Run toy problem\n",
    "\n",
    "    Arguments:\n",
    "        config {dict} -- read from config.yml\n",
    "    \"\"\"\n",
    "\n",
    "    print('='*100)\n",
    "    print('Run toy model')\n",
    "    print('='*100)\n",
    "    print('Config:')\n",
    "    for key, value in config.items():\n",
    "        print('{:20} ({})'.format(key, value))\n",
    "    print('='*100)\n",
    "\n",
    "    tf.logging.set_verbosity(\n",
    "        getattr(tf.logging, config['verbosity']))\n",
    "    tf.set_random_seed(config['seed'])\n",
    "\n",
    "    # Generate data\n",
    "    trainset, testset = generate_datasets(config)\n",
    "\n",
    "    # Build network\n",
    "    input = tf.placeholder(\n",
    "        tf.float32, [None, config['num_nodes']], name='input')\n",
    "    output = tf.placeholder(tf.float32, [None, 1], name='output')\n",
    "    net = network.Network(name='msa_net')\n",
    "\n",
    "    for n in range(config['num_layers']):\n",
    "        if n == 0:\n",
    "            net.add(layers.ResidualDense(\n",
    "                input_shape=(input.shape[1:]),\n",
    "                units=config['num_nodes'], activation=config['activation'],\n",
    "                msa_rho=config['rho'], msa_reg=config['reg'],\n",
    "                kernel_initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=config['kernel_init']),\n",
    "                bias_initializer=tf.constant_initializer(config['bias_init']),\n",
    "                delta=config['delta'], name='residualdense_{}'.format(n)))\n",
    "        else:\n",
    "            net.add(layers.ResidualDense(\n",
    "                units=config['num_nodes'], activation=config['activation'],\n",
    "                msa_rho=config['rho'], msa_reg=config['reg'],\n",
    "                kernel_initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=config['kernel_init']),\n",
    "                bias_initializer=tf.constant_initializer(config['bias_init']),\n",
    "                delta=config['delta'], name='residualdense_{}'.format(n)))\n",
    "\n",
    "    net.msa_compute_x(input)\n",
    "    net.msa_compute_p(output, loss_func)\n",
    "    net.summary()\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # MSA trainer\n",
    "    msa_trainer = train.MSATrainer(\n",
    "        network=net,\n",
    "        name='MSA_trainer',\n",
    "        maxiter=config['msa_maxiter'],\n",
    "        perturb_init=config['msa_perturb_init'])\n",
    "    msa_trainer.initialize(sess)\n",
    "    msa_trainer.train(\n",
    "        session=sess,\n",
    "        trainset=trainset,\n",
    "        testset=testset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_epochs=config['num_epochs'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        print_step=config['print_step'])\n",
    "\n",
    "    # SGD trainer\n",
    "    sgd_trainer = train.BPTrainer(\n",
    "        network=net, name='SGD_trainer',\n",
    "        method='GradientDescentOptimizer',\n",
    "        args={'learning_rate': config['sgd_lr']})\n",
    "    sgd_trainer.initialize(sess)\n",
    "    sgd_trainer.train(\n",
    "        session=sess,\n",
    "        trainset=trainset,\n",
    "        testset=testset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_epochs=config['num_epochs'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        print_step=config['print_step'])\n",
    "\n",
    "    # Adagrad trainer\n",
    "    adagrad_trainer = train.BPTrainer(\n",
    "        network=net, name='Adagrad_trainer',\n",
    "        method='AdagradOptimizer',\n",
    "        args={'learning_rate': config['adagrad_lr']})\n",
    "    adagrad_trainer.initialize(sess)\n",
    "    adagrad_trainer.train(\n",
    "        session=sess,\n",
    "        trainset=trainset,\n",
    "        testset=testset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_epochs=config['num_epochs'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        print_step=config['print_step'])\n",
    "\n",
    "    # Adam trainer\n",
    "    Adam_trainer = train.BPTrainer(\n",
    "        network=net, name='Adam_trainer',\n",
    "        method='AdamOptimizer',\n",
    "        args={'learning_rate': config['adam_lr']})\n",
    "    Adam_trainer.initialize(sess)\n",
    "    Adam_trainer.train(\n",
    "        session=sess,\n",
    "        trainset=trainset,\n",
    "        testset=testset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_epochs=config['num_epochs'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        print_step=config['print_step'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"main_toy.py\", line 5, in <module>\r\n",
      "    from msalib import train\r\n",
      "  File \"/mnt/d/lesenok/Linux/CNN/msalib/train.py\", line 3, in <module>\r\n",
      "    from msalib.scipyoptimizer import ScipyOptimizer\r\n",
      "  File \"/mnt/d/lesenok/Linux/CNN/msalib/scipyoptimizer.py\", line 5, in <module>\r\n",
      "    Base = tf.contrib.opt.ScipyOptimizerInterface\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/lazy_loader.py\", line 61, in __getattr__\r\n",
      "    module = self._load()\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/lazy_loader.py\", line 44, in _load\r\n",
      "    module = importlib.import_module(self.__name__)\r\n",
      "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/__init__.py\", line 95, in <module>\r\n",
      "    from tensorflow.contrib.optimizer_v2 import optimizer_v2_symbols as optimizer_v2\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/optimizer_v2/optimizer_v2_symbols.py\", line 22, in <module>\r\n",
      "    from tensorflow.contrib.optimizer_v2.adadelta import AdadeltaOptimizer\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 916, in get_data\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -u main_toy.py | tee sin.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Run toy model\n",
      "====================================================================================================\n",
      "Config:\n",
      "seed                 (1)\n",
      "verbosity            (WARN)\n",
      "print_step           (False)\n",
      "num_train            (100)\n",
      "num_test             (100)\n",
      "num_nodes            (5)\n",
      "num_layers           (20)\n",
      "rho                  (1.0)\n",
      "reg                  (0.001)\n",
      "activation           (tanh)\n",
      "kernel_init          (0.0)\n",
      "bias_init            (0.0)\n",
      "delta                (0.25)\n",
      "batch_size           (100)\n",
      "buffer_size          (100)\n",
      "num_epochs           (500)\n",
      "sgd_lr               (0.1)\n",
      "adagrad_lr           (0.1)\n",
      "adam_lr              (0.005)\n",
      "msa_maxiter          (10)\n",
      "msa_perturb_init     (0.005)\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "residualdense_0 (ResidualDen (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_1 (ResidualDen (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_2 (ResidualDen (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_3 (ResidualDen (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_4 (ResidualDen (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_5 (ResidualDen (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_6 (ResidualDen (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_7 (ResidualDen (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_8 (ResidualDen (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_9 (ResidualDen (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_10 (ResidualDe (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_11 (ResidualDe (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_12 (ResidualDe (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_13 (ResidualDe (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_14 (ResidualDe (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_15 (ResidualDe (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_16 (ResidualDe (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_17 (ResidualDe (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_18 (ResidualDe (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "residualdense_19 (ResidualDe (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 600\n",
      "Trainable params: 600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "====================================================================================================\n",
      "Trainer: MSATrainer (MSA_trainer)\n",
      "Settings: {'maxiter': 10, 'perturb_init': 0.005}\n",
      "====================================================================================================\n",
      "Epoch: init\n",
      "Train loss: 76.24555969238281\n",
      "Test loss: 89.11460876464844\n",
      "Epoch: 0\n",
      "Train loss: 0.4291010797023773\n",
      "Test loss: 0.533036470413208\n",
      "Epoch: 1\n",
      "Train loss: 0.3200250267982483\n",
      "Test loss: 0.38131582736968994\n",
      "Epoch: 2\n",
      "Train loss: 0.28554973006248474\n",
      "Test loss: 0.3286866545677185\n",
      "Epoch: 3\n",
      "Train loss: 0.2721496820449829\n",
      "Test loss: 0.3058263063430786\n",
      "Epoch: 4\n",
      "Train loss: 0.26629599928855896\n",
      "Test loss: 0.294454962015152\n",
      "Epoch: 5\n",
      "Train loss: 0.26352739334106445\n",
      "Test loss: 0.2882392108440399\n",
      "Epoch: 6\n",
      "Train loss: 0.2621302604675293\n",
      "Test loss: 0.2845951318740845\n",
      "Epoch: 7\n",
      "Train loss: 0.2613726556301117\n",
      "Test loss: 0.2823159098625183\n",
      "Epoch: 8\n",
      "Train loss: 0.260890394449234\n",
      "Test loss: 0.28080788254737854\n",
      "Epoch: 9\n",
      "Train loss: 0.2604561150074005\n",
      "Test loss: 0.279649943113327\n",
      "Epoch: 10\n",
      "Train loss: 0.2594914734363556\n",
      "Test loss: 0.2782907783985138\n",
      "Epoch: 11\n",
      "Train loss: 0.2535797953605652\n",
      "Test loss: 0.27277255058288574\n",
      "Epoch: 12\n",
      "Train loss: 0.1798527091741562\n",
      "Test loss: 0.22023729979991913\n",
      "Epoch: 13\n",
      "Train loss: 16.26152992248535\n",
      "Test loss: 15.258635520935059\n",
      "Epoch: 14\n",
      "Train loss: 59.1369743347168\n",
      "Test loss: 67.24027252197266\n",
      "Epoch: 15\n",
      "Train loss: 22.32686996459961\n",
      "Test loss: 25.091720581054688\n",
      "Epoch: 16\n",
      "Train loss: 10.28443431854248\n",
      "Test loss: 11.579834938049316\n",
      "Epoch: 17\n",
      "Train loss: 5.780032157897949\n",
      "Test loss: 6.5716729164123535\n",
      "Epoch: 18\n",
      "Train loss: 3.5562188625335693\n",
      "Test loss: 4.0804853439331055\n",
      "Epoch: 19\n",
      "Train loss: 2.3613085746765137\n",
      "Test loss: 2.7363171577453613\n",
      "Epoch: 20\n",
      "Train loss: 1.685981273651123\n",
      "Test loss: 1.9752998352050781\n",
      "Epoch: 21\n",
      "Train loss: 1.2594857215881348\n",
      "Test loss: 1.486795425415039\n",
      "Epoch: 22\n",
      "Train loss: 0.9737343788146973\n",
      "Test loss: 1.1595230102539062\n",
      "Epoch: 23\n",
      "Train loss: 0.79671710729599\n",
      "Test loss: 0.9525659680366516\n",
      "Epoch: 24\n",
      "Train loss: 0.6684496998786926\n",
      "Test loss: 0.8024607300758362\n",
      "Epoch: 25\n",
      "Train loss: 0.5735737085342407\n",
      "Test loss: 0.689102292060852\n",
      "Epoch: 26\n",
      "Train loss: 0.5025319457054138\n",
      "Test loss: 0.6029479503631592\n",
      "Epoch: 27\n",
      "Train loss: 0.4430015981197357\n",
      "Test loss: 0.5285961627960205\n",
      "Epoch: 28\n",
      "Train loss: 0.3912176489830017\n",
      "Test loss: 0.46314552426338196\n",
      "Epoch: 29\n",
      "Train loss: 0.34248167276382446\n",
      "Test loss: 0.40110260248184204\n",
      "Epoch: 30\n",
      "Train loss: 0.30317485332489014\n",
      "Test loss: 0.35000598430633545\n",
      "Epoch: 31\n",
      "Train loss: 0.27121806144714355\n",
      "Test loss: 0.30817556381225586\n",
      "Epoch: 32\n",
      "Train loss: 0.2458479106426239\n",
      "Test loss: 0.27458417415618896\n",
      "Epoch: 33\n",
      "Train loss: 0.22351399064064026\n",
      "Test loss: 0.24604740738868713\n",
      "Epoch: 34\n",
      "Train loss: 0.20563723146915436\n",
      "Test loss: 0.22337844967842102\n",
      "Epoch: 35\n",
      "Train loss: 0.18954342603683472\n",
      "Test loss: 0.20331339538097382\n",
      "Epoch: 36\n",
      "Train loss: 0.17644044756889343\n",
      "Test loss: 0.18731528520584106\n",
      "Epoch: 37\n",
      "Train loss: 0.16589754819869995\n",
      "Test loss: 0.1746366322040558\n",
      "Epoch: 38\n",
      "Train loss: 0.1572844535112381\n",
      "Test loss: 0.1645336151123047\n",
      "Epoch: 39\n",
      "Train loss: 0.14921581745147705\n",
      "Test loss: 0.15505564212799072\n",
      "Epoch: 40\n",
      "Train loss: 0.14235563576221466\n",
      "Test loss: 0.14711681008338928\n",
      "Epoch: 41\n",
      "Train loss: 0.13658642768859863\n",
      "Test loss: 0.1405756026506424\n",
      "Epoch: 42\n",
      "Train loss: 0.13177496194839478\n",
      "Test loss: 0.13509342074394226\n",
      "Epoch: 43\n",
      "Train loss: 0.12678222358226776\n",
      "Test loss: 0.12960253655910492\n",
      "Epoch: 44\n",
      "Train loss: 0.12229878455400467\n",
      "Test loss: 0.1247115433216095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45\n",
      "Train loss: 0.11878462880849838\n",
      "Test loss: 0.12081722170114517\n",
      "Epoch: 46\n",
      "Train loss: 0.11566396057605743\n",
      "Test loss: 0.11742264032363892\n",
      "Epoch: 47\n",
      "Train loss: 0.1127704381942749\n",
      "Test loss: 0.11432130634784698\n",
      "Epoch: 48\n",
      "Train loss: 0.1096314862370491\n",
      "Test loss: 0.11101696640253067\n",
      "Epoch: 49\n",
      "Train loss: 0.10689201951026917\n",
      "Test loss: 0.10819574445486069\n",
      "Epoch: 50\n",
      "Train loss: 0.10355933010578156\n",
      "Test loss: 0.10460850596427917\n",
      "Epoch: 51\n",
      "Train loss: 0.101222924888134\n",
      "Test loss: 0.10218772292137146\n",
      "Epoch: 52\n",
      "Train loss: 0.099486343562603\n",
      "Test loss: 0.1003652811050415\n",
      "Epoch: 53\n",
      "Train loss: 0.09693531692028046\n",
      "Test loss: 0.097777359187603\n",
      "Epoch: 54\n",
      "Train loss: 0.09542710334062576\n",
      "Test loss: 0.09622682631015778\n",
      "Epoch: 55\n",
      "Train loss: 0.09394341707229614\n",
      "Test loss: 0.09467333555221558\n",
      "Epoch: 56\n",
      "Train loss: 0.0917004719376564\n",
      "Test loss: 0.09243670105934143\n",
      "Epoch: 57\n",
      "Train loss: 0.09005194157361984\n",
      "Test loss: 0.09080243110656738\n",
      "Epoch: 58\n",
      "Train loss: 0.08838255703449249\n",
      "Test loss: 0.08916185796260834\n",
      "Epoch: 59\n",
      "Train loss: 0.08686785399913788\n",
      "Test loss: 0.08760423958301544\n",
      "Epoch: 60\n",
      "Train loss: 0.08545464277267456\n",
      "Test loss: 0.08616410940885544\n",
      "Epoch: 61\n",
      "Train loss: 0.08425527811050415\n",
      "Test loss: 0.08496677875518799\n",
      "Epoch: 62\n",
      "Train loss: 0.08308400958776474\n",
      "Test loss: 0.08374761790037155\n",
      "Epoch: 63\n",
      "Train loss: 0.0817716121673584\n",
      "Test loss: 0.08235859870910645\n",
      "Epoch: 64\n",
      "Train loss: 0.08054201304912567\n",
      "Test loss: 0.0811348706483841\n",
      "Epoch: 65\n",
      "Train loss: 0.07945067435503006\n",
      "Test loss: 0.08002857118844986\n",
      "Epoch: 66\n",
      "Train loss: 0.07830854505300522\n",
      "Test loss: 0.07884933799505234\n",
      "Epoch: 67\n",
      "Train loss: 0.07746393978595734\n",
      "Test loss: 0.07800237834453583\n",
      "Epoch: 68\n",
      "Train loss: 0.07644523680210114\n",
      "Test loss: 0.07700065523386002\n",
      "Epoch: 69\n",
      "Train loss: 0.07552499324083328\n",
      "Test loss: 0.07608401030302048\n",
      "Epoch: 70\n",
      "Train loss: 0.07468586415052414\n",
      "Test loss: 0.07519076019525528\n",
      "Epoch: 71\n",
      "Train loss: 0.07386026531457901\n",
      "Test loss: 0.0743507519364357\n",
      "Epoch: 72\n",
      "Train loss: 0.07313548773527145\n",
      "Test loss: 0.07362888753414154\n",
      "Epoch: 73\n",
      "Train loss: 0.07235056161880493\n",
      "Test loss: 0.07282483577728271\n",
      "Epoch: 74\n",
      "Train loss: 0.07161171734333038\n",
      "Test loss: 0.07205736637115479\n",
      "Epoch: 75\n",
      "Train loss: 0.07080358266830444\n",
      "Test loss: 0.07126551866531372\n",
      "Epoch: 76\n",
      "Train loss: 0.07007062435150146\n",
      "Test loss: 0.07049159705638885\n",
      "Epoch: 77\n",
      "Train loss: 0.06933806091547012\n",
      "Test loss: 0.0697922557592392\n",
      "Epoch: 78\n",
      "Train loss: 0.06858769804239273\n",
      "Test loss: 0.06904099136590958\n",
      "Epoch: 79\n",
      "Train loss: 0.0679238960146904\n",
      "Test loss: 0.068349689245224\n",
      "Epoch: 80\n",
      "Train loss: 0.06739377975463867\n",
      "Test loss: 0.0678229108452797\n",
      "Epoch: 81\n",
      "Train loss: 0.0667288526892662\n",
      "Test loss: 0.0671263188123703\n",
      "Epoch: 82\n",
      "Train loss: 0.06611019372940063\n",
      "Test loss: 0.06647554039955139\n",
      "Epoch: 83\n",
      "Train loss: 0.06542976200580597\n",
      "Test loss: 0.06579400599002838\n",
      "Epoch: 84\n",
      "Train loss: 0.06492146849632263\n",
      "Test loss: 0.06529664993286133\n",
      "Epoch: 85\n",
      "Train loss: 0.06433549523353577\n",
      "Test loss: 0.0646924078464508\n",
      "Epoch: 86\n",
      "Train loss: 0.06379052251577377\n",
      "Test loss: 0.06411563605070114\n",
      "Epoch: 87\n",
      "Train loss: 0.06314119696617126\n",
      "Test loss: 0.06346989423036575\n",
      "Epoch: 88\n",
      "Train loss: 0.06263305991888046\n",
      "Test loss: 0.06296482682228088\n",
      "Epoch: 89\n",
      "Train loss: 0.062062062323093414\n",
      "Test loss: 0.062424030154943466\n",
      "Epoch: 90\n",
      "Train loss: 0.06149439513683319\n",
      "Test loss: 0.0618196576833725\n",
      "Epoch: 91\n",
      "Train loss: 0.06106068938970566\n",
      "Test loss: 0.06139916926622391\n",
      "Epoch: 92\n",
      "Train loss: 0.060570720583200455\n",
      "Test loss: 0.06091222167015076\n",
      "Epoch: 93\n",
      "Train loss: 0.06019585579633713\n",
      "Test loss: 0.06054450571537018\n",
      "Epoch: 94\n",
      "Train loss: 0.059770241379737854\n",
      "Test loss: 0.060117173939943314\n",
      "Epoch: 95\n",
      "Train loss: 0.0592435859143734\n",
      "Test loss: 0.05959551781415939\n",
      "Epoch: 96\n",
      "Train loss: 0.05878447741270065\n",
      "Test loss: 0.05913766473531723\n",
      "Epoch: 97\n",
      "Train loss: 0.058319948613643646\n",
      "Test loss: 0.058679547160863876\n",
      "Epoch: 98\n",
      "Train loss: 0.05793098732829094\n",
      "Test loss: 0.058300863951444626\n",
      "Epoch: 99\n",
      "Train loss: 0.05745021998882294\n",
      "Test loss: 0.05780632048845291\n",
      "Epoch: 100\n",
      "Train loss: 0.05700818449258804\n",
      "Test loss: 0.0573519729077816\n",
      "Epoch: 101\n",
      "Train loss: 0.05663524568080902\n",
      "Test loss: 0.05702672898769379\n",
      "Epoch: 102\n",
      "Train loss: 0.0562657006084919\n",
      "Test loss: 0.056656330823898315\n",
      "Epoch: 103\n",
      "Train loss: 0.055855680257081985\n",
      "Test loss: 0.05623764544725418\n",
      "Epoch: 104\n",
      "Train loss: 0.055527277290821075\n",
      "Test loss: 0.055882446467876434\n",
      "Epoch: 105\n",
      "Train loss: 0.05522431433200836\n",
      "Test loss: 0.05559399351477623\n",
      "Epoch: 106\n",
      "Train loss: 0.054837945848703384\n",
      "Test loss: 0.05520639941096306\n",
      "Epoch: 107\n",
      "Train loss: 0.05448225885629654\n",
      "Test loss: 0.05484820529818535\n",
      "Epoch: 108\n",
      "Train loss: 0.05422201007604599\n",
      "Test loss: 0.05460658669471741\n",
      "Epoch: 109\n",
      "Train loss: 0.053932107985019684\n",
      "Test loss: 0.05429771542549133\n",
      "Epoch: 110\n",
      "Train loss: 0.05367211624979973\n",
      "Test loss: 0.05405345931649208\n",
      "Epoch: 111\n",
      "Train loss: 0.05335179716348648\n",
      "Test loss: 0.05371331050992012\n",
      "Epoch: 112\n",
      "Train loss: 0.052958857268095016\n",
      "Test loss: 0.05332845076918602\n",
      "Epoch: 113\n",
      "Train loss: 0.05263010412454605\n",
      "Test loss: 0.05300555005669594\n",
      "Epoch: 114\n",
      "Train loss: 0.052315738052129745\n",
      "Test loss: 0.05270511656999588\n",
      "Epoch: 115\n",
      "Train loss: 0.0520605631172657\n",
      "Test loss: 0.05243668705224991\n",
      "Epoch: 116\n",
      "Train loss: 0.051758285611867905\n",
      "Test loss: 0.052114564925432205\n",
      "Epoch: 117\n",
      "Train loss: 0.05143701657652855\n",
      "Test loss: 0.05175293982028961\n",
      "Epoch: 118\n",
      "Train loss: 0.051201146095991135\n",
      "Test loss: 0.05153512582182884\n",
      "Epoch: 119\n",
      "Train loss: 0.05092692747712135\n",
      "Test loss: 0.05127986520528793\n",
      "Epoch: 120\n",
      "Train loss: 0.05059108883142471\n",
      "Test loss: 0.050932761281728745\n",
      "Epoch: 121\n",
      "Train loss: 0.05030513182282448\n",
      "Test loss: 0.05064002797007561\n",
      "Epoch: 122\n",
      "Train loss: 0.050043344497680664\n",
      "Test loss: 0.050410911440849304\n",
      "Epoch: 123\n",
      "Train loss: 0.04982377216219902\n",
      "Test loss: 0.050176601856946945\n",
      "Epoch: 124\n",
      "Train loss: 0.049489788711071014\n",
      "Test loss: 0.049853093922138214\n",
      "Epoch: 125\n",
      "Train loss: 0.04926656186580658\n",
      "Test loss: 0.04963045194745064\n",
      "Epoch: 126\n",
      "Train loss: 0.04901297390460968\n",
      "Test loss: 0.04938466101884842\n",
      "Epoch: 127\n",
      "Train loss: 0.048776909708976746\n",
      "Test loss: 0.04915611445903778\n",
      "Epoch: 128\n",
      "Train loss: 0.04853859916329384\n",
      "Test loss: 0.048908740282058716\n",
      "Epoch: 129\n",
      "Train loss: 0.04831450805068016\n",
      "Test loss: 0.04869599640369415\n",
      "Epoch: 130\n",
      "Train loss: 0.048052724450826645\n",
      "Test loss: 0.0484241247177124\n",
      "Epoch: 131\n",
      "Train loss: 0.0478799045085907\n",
      "Test loss: 0.04825146123766899\n",
      "Epoch: 132\n",
      "Train loss: 0.047674760222435\n",
      "Test loss: 0.048056162893772125\n",
      "Epoch: 133\n",
      "Train loss: 0.04751028120517731\n",
      "Test loss: 0.04788517951965332\n",
      "Epoch: 134\n",
      "Train loss: 0.04734412580728531\n",
      "Test loss: 0.047724928706884384\n",
      "Epoch: 135\n",
      "Train loss: 0.04715035483241081\n",
      "Test loss: 0.04752861708402634\n",
      "Epoch: 136\n",
      "Train loss: 0.04695713520050049\n",
      "Test loss: 0.047332633286714554\n",
      "Epoch: 137\n",
      "Train loss: 0.046788252890110016\n",
      "Test loss: 0.047168005257844925\n",
      "Epoch: 138\n",
      "Train loss: 0.04659992828965187\n",
      "Test loss: 0.046974584460258484\n",
      "Epoch: 139\n",
      "Train loss: 0.04637642949819565\n",
      "Test loss: 0.046761542558670044\n",
      "Epoch: 140\n",
      "Train loss: 0.046208061277866364\n",
      "Test loss: 0.046585019677877426\n",
      "Epoch: 141\n",
      "Train loss: 0.04597428813576698\n",
      "Test loss: 0.04636096954345703\n",
      "Epoch: 142\n",
      "Train loss: 0.04576212912797928\n",
      "Test loss: 0.04614439234137535\n",
      "Epoch: 143\n",
      "Train loss: 0.04558996483683586\n",
      "Test loss: 0.045996490865945816\n",
      "Epoch: 144\n",
      "Train loss: 0.045417580753564835\n",
      "Test loss: 0.04580850154161453\n",
      "Epoch: 145\n",
      "Train loss: 0.04523574188351631\n",
      "Test loss: 0.04564216732978821\n",
      "Epoch: 146\n",
      "Train loss: 0.04505039379000664\n",
      "Test loss: 0.04547454044222832\n",
      "Epoch: 147\n",
      "Train loss: 0.0448489710688591\n",
      "Test loss: 0.045253027230501175\n",
      "Epoch: 148\n",
      "Train loss: 0.04467804729938507\n",
      "Test loss: 0.04512645676732063\n",
      "Epoch: 149\n",
      "Train loss: 0.04447823017835617\n",
      "Test loss: 0.04487762600183487\n",
      "Epoch: 150\n",
      "Train loss: 0.04430907964706421\n",
      "Test loss: 0.044735390692949295\n",
      "Epoch: 151\n",
      "Train loss: 0.04416695237159729\n",
      "Test loss: 0.044608648866415024\n",
      "Epoch: 152\n",
      "Train loss: 0.0439971461892128\n",
      "Test loss: 0.044451985508203506\n",
      "Epoch: 153\n",
      "Train loss: 0.043827857822179794\n",
      "Test loss: 0.04427807778120041\n",
      "Epoch: 154\n",
      "Train loss: 0.04368826746940613\n",
      "Test loss: 0.04413224756717682\n",
      "Epoch: 155\n",
      "Train loss: 0.04351846128702164\n",
      "Test loss: 0.043954573571681976\n",
      "Epoch: 156\n",
      "Train loss: 0.04334717243909836\n",
      "Test loss: 0.04377443343400955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 157\n",
      "Train loss: 0.04318348318338394\n",
      "Test loss: 0.043638937175273895\n",
      "Epoch: 158\n",
      "Train loss: 0.043049704283475876\n",
      "Test loss: 0.04349137842655182\n",
      "Epoch: 159\n",
      "Train loss: 0.04289362207055092\n",
      "Test loss: 0.04331818222999573\n",
      "Epoch: 160\n",
      "Train loss: 0.04272432252764702\n",
      "Test loss: 0.043149787932634354\n",
      "Epoch: 161\n",
      "Train loss: 0.04255576431751251\n",
      "Test loss: 0.04297732561826706\n",
      "Epoch: 162\n",
      "Train loss: 0.042432110756635666\n",
      "Test loss: 0.042862337082624435\n",
      "Epoch: 163\n",
      "Train loss: 0.04227207973599434\n",
      "Test loss: 0.04270379990339279\n",
      "Epoch: 164\n",
      "Train loss: 0.04211978614330292\n",
      "Test loss: 0.042541906237602234\n",
      "Epoch: 165\n",
      "Train loss: 0.04193400591611862\n",
      "Test loss: 0.04235941544175148\n",
      "Epoch: 166\n",
      "Train loss: 0.04178958386182785\n",
      "Test loss: 0.042221490293741226\n",
      "Epoch: 167\n",
      "Train loss: 0.04166826605796814\n",
      "Test loss: 0.042111415416002274\n",
      "Epoch: 168\n",
      "Train loss: 0.04154011234641075\n",
      "Test loss: 0.04198412224650383\n",
      "Epoch: 169\n",
      "Train loss: 0.04139349237084389\n",
      "Test loss: 0.041829731315374374\n",
      "Epoch: 170\n",
      "Train loss: 0.04126393422484398\n",
      "Test loss: 0.041708804666996\n",
      "Epoch: 171\n",
      "Train loss: 0.041146114468574524\n",
      "Test loss: 0.04159638658165932\n",
      "Epoch: 172\n",
      "Train loss: 0.04102028161287308\n",
      "Test loss: 0.04148891940712929\n",
      "Epoch: 173\n",
      "Train loss: 0.040869954973459244\n",
      "Test loss: 0.04132010415196419\n",
      "Epoch: 174\n",
      "Train loss: 0.04073921591043472\n",
      "Test loss: 0.041170574724674225\n",
      "Epoch: 175\n",
      "Train loss: 0.04059259593486786\n",
      "Test loss: 0.04104319587349892\n",
      "Epoch: 176\n",
      "Train loss: 0.0404643714427948\n",
      "Test loss: 0.0408971905708313\n",
      "Epoch: 177\n",
      "Train loss: 0.040305618196725845\n",
      "Test loss: 0.04077056050300598\n",
      "Epoch: 178\n",
      "Train loss: 0.04021488502621651\n",
      "Test loss: 0.04069363325834274\n",
      "Epoch: 179\n",
      "Train loss: 0.040068887174129486\n",
      "Test loss: 0.040532492101192474\n",
      "Epoch: 180\n",
      "Train loss: 0.039938800036907196\n",
      "Test loss: 0.040409721434116364\n",
      "Epoch: 181\n",
      "Train loss: 0.039819080382585526\n",
      "Test loss: 0.0402664989233017\n",
      "Epoch: 182\n",
      "Train loss: 0.03968718647956848\n",
      "Test loss: 0.04013244807720184\n",
      "Epoch: 183\n",
      "Train loss: 0.03957381099462509\n",
      "Test loss: 0.04002624750137329\n",
      "Epoch: 184\n",
      "Train loss: 0.03943922743201256\n",
      "Test loss: 0.03990517184138298\n",
      "Epoch: 185\n",
      "Train loss: 0.039322856813669205\n",
      "Test loss: 0.039782311767339706\n",
      "Epoch: 186\n",
      "Train loss: 0.03920454531908035\n",
      "Test loss: 0.03967916592955589\n",
      "Epoch: 187\n",
      "Train loss: 0.03908538073301315\n",
      "Test loss: 0.039556846022605896\n",
      "Epoch: 188\n",
      "Train loss: 0.038975998759269714\n",
      "Test loss: 0.03944571688771248\n",
      "Epoch: 189\n",
      "Train loss: 0.03885941579937935\n",
      "Test loss: 0.03932277858257294\n",
      "Epoch: 190\n",
      "Train loss: 0.03875883296132088\n",
      "Test loss: 0.039223335683345795\n",
      "Epoch: 191\n",
      "Train loss: 0.038631897419691086\n",
      "Test loss: 0.03910398110747337\n",
      "Epoch: 192\n",
      "Train loss: 0.038505636155605316\n",
      "Test loss: 0.038976605981588364\n",
      "Epoch: 193\n",
      "Train loss: 0.03839171305298805\n",
      "Test loss: 0.03884182125329971\n",
      "Epoch: 194\n",
      "Train loss: 0.0382719449698925\n",
      "Test loss: 0.038750700652599335\n",
      "Epoch: 195\n",
      "Train loss: 0.03820308297872543\n",
      "Test loss: 0.03867867961525917\n",
      "Epoch: 196\n",
      "Train loss: 0.038098178803920746\n",
      "Test loss: 0.038563281297683716\n",
      "Epoch: 197\n",
      "Train loss: 0.03801922872662544\n",
      "Test loss: 0.03848765790462494\n",
      "Epoch: 198\n",
      "Train loss: 0.037896547466516495\n",
      "Test loss: 0.038366224616765976\n",
      "Epoch: 199\n",
      "Train loss: 0.03779708594083786\n",
      "Test loss: 0.038265079259872437\n",
      "Epoch: 200\n",
      "Train loss: 0.03767234459519386\n",
      "Test loss: 0.03812682628631592\n",
      "Epoch: 201\n",
      "Train loss: 0.037562549114227295\n",
      "Test loss: 0.03800291568040848\n",
      "Epoch: 202\n",
      "Train loss: 0.037457603961229324\n",
      "Test loss: 0.03792353346943855\n",
      "Epoch: 203\n",
      "Train loss: 0.03737059608101845\n",
      "Test loss: 0.037828072905540466\n",
      "Epoch: 204\n",
      "Train loss: 0.03725248575210571\n",
      "Test loss: 0.037711236625909805\n",
      "Epoch: 205\n",
      "Train loss: 0.03714202344417572\n",
      "Test loss: 0.037623483687639236\n",
      "Epoch: 206\n",
      "Train loss: 0.037022873759269714\n",
      "Test loss: 0.03750002384185791\n",
      "Epoch: 207\n",
      "Train loss: 0.03693331778049469\n",
      "Test loss: 0.03740859776735306\n",
      "Epoch: 208\n",
      "Train loss: 0.03683643415570259\n",
      "Test loss: 0.03732703998684883\n",
      "Epoch: 209\n",
      "Train loss: 0.03675264120101929\n",
      "Test loss: 0.03723108768463135\n",
      "Epoch: 210\n",
      "Train loss: 0.03665315732359886\n",
      "Test loss: 0.037112731486558914\n",
      "Epoch: 211\n",
      "Train loss: 0.036559492349624634\n",
      "Test loss: 0.037023384124040604\n",
      "Epoch: 212\n",
      "Train loss: 0.036485686898231506\n",
      "Test loss: 0.03695160895586014\n",
      "Epoch: 213\n",
      "Train loss: 0.036401547491550446\n",
      "Test loss: 0.036880504339933395\n",
      "Epoch: 214\n",
      "Train loss: 0.03630060702562332\n",
      "Test loss: 0.03678233548998833\n",
      "Epoch: 215\n",
      "Train loss: 0.03621462732553482\n",
      "Test loss: 0.03671615198254585\n",
      "Epoch: 216\n",
      "Train loss: 0.03611798956990242\n",
      "Test loss: 0.03661860153079033\n",
      "Epoch: 217\n",
      "Train loss: 0.036033764481544495\n",
      "Test loss: 0.03651689365506172\n",
      "Epoch: 218\n",
      "Train loss: 0.03592924773693085\n",
      "Test loss: 0.03641260787844658\n",
      "Epoch: 219\n",
      "Train loss: 0.03584930673241615\n",
      "Test loss: 0.03631933033466339\n",
      "Epoch: 220\n",
      "Train loss: 0.03574554994702339\n",
      "Test loss: 0.036222267895936966\n",
      "Epoch: 221\n",
      "Train loss: 0.03567568212747574\n",
      "Test loss: 0.03617585077881813\n",
      "Epoch: 222\n",
      "Train loss: 0.035579510033130646\n",
      "Test loss: 0.0360727496445179\n",
      "Epoch: 223\n",
      "Train loss: 0.03550703078508377\n",
      "Test loss: 0.03602242469787598\n",
      "Epoch: 224\n",
      "Train loss: 0.035451024770736694\n",
      "Test loss: 0.035967111587524414\n",
      "Epoch: 225\n",
      "Train loss: 0.035357628017663956\n",
      "Test loss: 0.03585730120539665\n",
      "Epoch: 226\n",
      "Train loss: 0.035274796187877655\n",
      "Test loss: 0.03577563911676407\n",
      "Epoch: 227\n",
      "Train loss: 0.0351778008043766\n",
      "Test loss: 0.03566427528858185\n",
      "Epoch: 228\n",
      "Train loss: 0.0350937619805336\n",
      "Test loss: 0.03560966998338699\n",
      "Epoch: 229\n",
      "Train loss: 0.03499528765678406\n",
      "Test loss: 0.03549877554178238\n",
      "Epoch: 230\n",
      "Train loss: 0.034933533519506454\n",
      "Test loss: 0.035445209592580795\n",
      "Epoch: 231\n",
      "Train loss: 0.034864090383052826\n",
      "Test loss: 0.03537781536579132\n",
      "Epoch: 232\n",
      "Train loss: 0.0348048098385334\n",
      "Test loss: 0.03532169759273529\n",
      "Epoch: 233\n",
      "Train loss: 0.03472772240638733\n",
      "Test loss: 0.03523910045623779\n",
      "Epoch: 234\n",
      "Train loss: 0.03464096784591675\n",
      "Test loss: 0.03515433147549629\n",
      "Epoch: 235\n",
      "Train loss: 0.03455277159810066\n",
      "Test loss: 0.035053547471761703\n",
      "Epoch: 236\n",
      "Train loss: 0.03445332869887352\n",
      "Test loss: 0.034969013184309006\n",
      "Epoch: 237\n",
      "Train loss: 0.03439408540725708\n",
      "Test loss: 0.03490634262561798\n",
      "Epoch: 238\n",
      "Train loss: 0.034303706139326096\n",
      "Test loss: 0.03479737415909767\n",
      "Epoch: 239\n",
      "Train loss: 0.034220434725284576\n",
      "Test loss: 0.03473455458879471\n",
      "Epoch: 240\n",
      "Train loss: 0.034141331911087036\n",
      "Test loss: 0.03463975712656975\n",
      "Epoch: 241\n",
      "Train loss: 0.03407719358801842\n",
      "Test loss: 0.03458479046821594\n",
      "Epoch: 242\n",
      "Train loss: 0.03399644047021866\n",
      "Test loss: 0.034503888338804245\n",
      "Epoch: 243\n",
      "Train loss: 0.03393303602933884\n",
      "Test loss: 0.034422945231199265\n",
      "Epoch: 244\n",
      "Train loss: 0.033862143754959106\n",
      "Test loss: 0.0343543104827404\n",
      "Epoch: 245\n",
      "Train loss: 0.033809173852205276\n",
      "Test loss: 0.03427901491522789\n",
      "Epoch: 246\n",
      "Train loss: 0.03373358026146889\n",
      "Test loss: 0.034220293164253235\n",
      "Epoch: 247\n",
      "Train loss: 0.03366478905081749\n",
      "Test loss: 0.03418543562293053\n",
      "Epoch: 248\n",
      "Train loss: 0.03358190879225731\n",
      "Test loss: 0.03409353643655777\n",
      "Epoch: 249\n",
      "Train loss: 0.033516500145196915\n",
      "Test loss: 0.0340304933488369\n",
      "Epoch: 250\n",
      "Train loss: 0.03345642611384392\n",
      "Test loss: 0.03394470363855362\n",
      "Epoch: 251\n",
      "Train loss: 0.0333796888589859\n",
      "Test loss: 0.033881690353155136\n",
      "Epoch: 252\n",
      "Train loss: 0.03331703692674637\n",
      "Test loss: 0.03383700177073479\n",
      "Epoch: 253\n",
      "Train loss: 0.033250682055950165\n",
      "Test loss: 0.033772990107536316\n",
      "Epoch: 254\n",
      "Train loss: 0.0331706628203392\n",
      "Test loss: 0.03369491919875145\n",
      "Epoch: 255\n",
      "Train loss: 0.033106729388237\n",
      "Test loss: 0.033615000545978546\n",
      "Epoch: 256\n",
      "Train loss: 0.03304172679781914\n",
      "Test loss: 0.03356116637587547\n",
      "Epoch: 257\n",
      "Train loss: 0.0329633504152298\n",
      "Test loss: 0.033475134521722794\n",
      "Epoch: 258\n",
      "Train loss: 0.03287122771143913\n",
      "Test loss: 0.03338172286748886\n",
      "Epoch: 259\n",
      "Train loss: 0.032819122076034546\n",
      "Test loss: 0.03333952650427818\n",
      "Epoch: 260\n",
      "Train loss: 0.03275906667113304\n",
      "Test loss: 0.03329060226678848\n",
      "Epoch: 261\n",
      "Train loss: 0.03269891440868378\n",
      "Test loss: 0.033222079277038574\n",
      "Epoch: 262\n",
      "Train loss: 0.03264357149600983\n",
      "Test loss: 0.033164460211992264\n",
      "Epoch: 263\n",
      "Train loss: 0.03259233012795448\n",
      "Test loss: 0.03314798325300217\n",
      "Epoch: 264\n",
      "Train loss: 0.0325276181101799\n",
      "Test loss: 0.033071041107177734\n",
      "Epoch: 265\n",
      "Train loss: 0.03246038779616356\n",
      "Test loss: 0.032988931983709335\n",
      "Epoch: 266\n",
      "Train loss: 0.0323958545923233\n",
      "Test loss: 0.03293957561254501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 267\n",
      "Train loss: 0.032345954328775406\n",
      "Test loss: 0.0328943207859993\n",
      "Epoch: 268\n",
      "Train loss: 0.032289162278175354\n",
      "Test loss: 0.03282513841986656\n",
      "Epoch: 269\n",
      "Train loss: 0.03223578631877899\n",
      "Test loss: 0.03277904540300369\n",
      "Epoch: 270\n",
      "Train loss: 0.03217713162302971\n",
      "Test loss: 0.03272049501538277\n",
      "Epoch: 271\n",
      "Train loss: 0.03212563320994377\n",
      "Test loss: 0.03266539424657822\n",
      "Epoch: 272\n",
      "Train loss: 0.03209228813648224\n",
      "Test loss: 0.03262634947896004\n",
      "Epoch: 273\n",
      "Train loss: 0.03205612301826477\n",
      "Test loss: 0.03259371966123581\n",
      "Epoch: 274\n",
      "Train loss: 0.03200172632932663\n",
      "Test loss: 0.03252768889069557\n",
      "Epoch: 275\n",
      "Train loss: 0.031924229115247726\n",
      "Test loss: 0.03245112672448158\n",
      "Epoch: 276\n",
      "Train loss: 0.03187728300690651\n",
      "Test loss: 0.032411057502031326\n",
      "Epoch: 277\n",
      "Train loss: 0.03182768449187279\n",
      "Test loss: 0.03235883638262749\n",
      "Epoch: 278\n",
      "Train loss: 0.03177323937416077\n",
      "Test loss: 0.03230869024991989\n",
      "Epoch: 279\n",
      "Train loss: 0.031732309609651566\n",
      "Test loss: 0.03225790336728096\n",
      "Epoch: 280\n",
      "Train loss: 0.031677111983299255\n",
      "Test loss: 0.032208651304244995\n",
      "Epoch: 281\n",
      "Train loss: 0.03161177411675453\n",
      "Test loss: 0.032161783427000046\n",
      "Epoch: 282\n",
      "Train loss: 0.03156060352921486\n",
      "Test loss: 0.03211222589015961\n",
      "Epoch: 283\n",
      "Train loss: 0.031512923538684845\n",
      "Test loss: 0.0320701040327549\n",
      "Epoch: 284\n",
      "Train loss: 0.031465865671634674\n",
      "Test loss: 0.032027240842580795\n",
      "Epoch: 285\n",
      "Train loss: 0.031418170779943466\n",
      "Test loss: 0.031969014555215836\n",
      "Epoch: 286\n",
      "Train loss: 0.03134884685277939\n",
      "Test loss: 0.03191705048084259\n",
      "Epoch: 287\n",
      "Train loss: 0.03129732608795166\n",
      "Test loss: 0.03185185790061951\n",
      "Epoch: 288\n",
      "Train loss: 0.031254660338163376\n",
      "Test loss: 0.03183646872639656\n",
      "Epoch: 289\n",
      "Train loss: 0.031202221289277077\n",
      "Test loss: 0.03177129849791527\n",
      "Epoch: 290\n",
      "Train loss: 0.031151186674833298\n",
      "Test loss: 0.03174331784248352\n",
      "Epoch: 291\n",
      "Train loss: 0.031074997037649155\n",
      "Test loss: 0.03166406601667404\n",
      "Epoch: 292\n",
      "Train loss: 0.031026046723127365\n",
      "Test loss: 0.031602658331394196\n",
      "Epoch: 293\n",
      "Train loss: 0.03099162131547928\n",
      "Test loss: 0.03157845884561539\n",
      "Epoch: 294\n",
      "Train loss: 0.03093065321445465\n",
      "Test loss: 0.0315084233880043\n",
      "Epoch: 295\n",
      "Train loss: 0.03088911436498165\n",
      "Test loss: 0.031459227204322815\n",
      "Epoch: 296\n",
      "Train loss: 0.03085475228726864\n",
      "Test loss: 0.03143310546875\n",
      "Epoch: 297\n",
      "Train loss: 0.030818164348602295\n",
      "Test loss: 0.031398650258779526\n",
      "Epoch: 298\n",
      "Train loss: 0.030773406848311424\n",
      "Test loss: 0.03133322671055794\n",
      "Epoch: 299\n",
      "Train loss: 0.03071744553744793\n",
      "Test loss: 0.031292352825403214\n",
      "Epoch: 300\n",
      "Train loss: 0.03067917190492153\n",
      "Test loss: 0.03125164657831192\n",
      "Epoch: 301\n",
      "Train loss: 0.030619103461503983\n",
      "Test loss: 0.03117254190146923\n",
      "Epoch: 302\n",
      "Train loss: 0.030567273497581482\n",
      "Test loss: 0.031122108921408653\n",
      "Epoch: 303\n",
      "Train loss: 0.03053184412419796\n",
      "Test loss: 0.031072916463017464\n",
      "Epoch: 304\n",
      "Train loss: 0.03048405982553959\n",
      "Test loss: 0.031016238033771515\n",
      "Epoch: 305\n",
      "Train loss: 0.03043537214398384\n",
      "Test loss: 0.030971907079219818\n",
      "Epoch: 306\n",
      "Train loss: 0.030386634171009064\n",
      "Test loss: 0.03093484416604042\n",
      "Epoch: 307\n",
      "Train loss: 0.030334362760186195\n",
      "Test loss: 0.03086056187748909\n",
      "Epoch: 308\n",
      "Train loss: 0.030289657413959503\n",
      "Test loss: 0.03081740438938141\n",
      "Epoch: 309\n",
      "Train loss: 0.03024326264858246\n",
      "Test loss: 0.030765505507588387\n",
      "Epoch: 310\n",
      "Train loss: 0.030200159177184105\n",
      "Test loss: 0.030738776549696922\n",
      "Epoch: 311\n",
      "Train loss: 0.030145805329084396\n",
      "Test loss: 0.030677370727062225\n",
      "Epoch: 312\n",
      "Train loss: 0.030108066275715828\n",
      "Test loss: 0.03063828870654106\n",
      "Epoch: 313\n",
      "Train loss: 0.030054058879613876\n",
      "Test loss: 0.030605416744947433\n",
      "Epoch: 314\n",
      "Train loss: 0.03000400960445404\n",
      "Test loss: 0.030559709295630455\n",
      "Epoch: 315\n",
      "Train loss: 0.029955077916383743\n",
      "Test loss: 0.030481530353426933\n",
      "Epoch: 316\n",
      "Train loss: 0.02988577075302601\n",
      "Test loss: 0.030415784567594528\n",
      "Epoch: 317\n",
      "Train loss: 0.029843809083104134\n",
      "Test loss: 0.03036869876086712\n",
      "Epoch: 318\n",
      "Train loss: 0.02979240193963051\n",
      "Test loss: 0.030327998101711273\n",
      "Epoch: 319\n",
      "Train loss: 0.029740409925580025\n",
      "Test loss: 0.03027414157986641\n",
      "Epoch: 320\n",
      "Train loss: 0.029711604118347168\n",
      "Test loss: 0.03023633360862732\n",
      "Epoch: 321\n",
      "Train loss: 0.029668180271983147\n",
      "Test loss: 0.030191872268915176\n",
      "Epoch: 322\n",
      "Train loss: 0.029619071632623672\n",
      "Test loss: 0.030124066397547722\n",
      "Epoch: 323\n",
      "Train loss: 0.029571574181318283\n",
      "Test loss: 0.030089912936091423\n",
      "Epoch: 324\n",
      "Train loss: 0.029536932706832886\n",
      "Test loss: 0.030057208612561226\n",
      "Epoch: 325\n",
      "Train loss: 0.029499877244234085\n",
      "Test loss: 0.03003021702170372\n",
      "Epoch: 326\n",
      "Train loss: 0.029464397579431534\n",
      "Test loss: 0.029993409290909767\n",
      "Epoch: 327\n",
      "Train loss: 0.029417168349027634\n",
      "Test loss: 0.029966233298182487\n",
      "Epoch: 328\n",
      "Train loss: 0.029378576204180717\n",
      "Test loss: 0.029913779348134995\n",
      "Epoch: 329\n",
      "Train loss: 0.02933892421424389\n",
      "Test loss: 0.029892878606915474\n",
      "Epoch: 330\n",
      "Train loss: 0.029290473088622093\n",
      "Test loss: 0.029821133241057396\n",
      "Epoch: 331\n",
      "Train loss: 0.029248937964439392\n",
      "Test loss: 0.029792506247758865\n",
      "Epoch: 332\n",
      "Train loss: 0.02920544147491455\n",
      "Test loss: 0.029753081500530243\n",
      "Epoch: 333\n",
      "Train loss: 0.029169762507081032\n",
      "Test loss: 0.029731707647442818\n",
      "Epoch: 334\n",
      "Train loss: 0.029119785875082016\n",
      "Test loss: 0.02966088056564331\n",
      "Epoch: 335\n",
      "Train loss: 0.029057497158646584\n",
      "Test loss: 0.029598040506243706\n",
      "Epoch: 336\n",
      "Train loss: 0.02902197651565075\n",
      "Test loss: 0.029596975073218346\n",
      "Epoch: 337\n",
      "Train loss: 0.028983112424612045\n",
      "Test loss: 0.02954019419848919\n",
      "Epoch: 338\n",
      "Train loss: 0.028946343809366226\n",
      "Test loss: 0.029488006606698036\n",
      "Epoch: 339\n",
      "Train loss: 0.028908565640449524\n",
      "Test loss: 0.02948835864663124\n",
      "Epoch: 340\n",
      "Train loss: 0.02886022813618183\n",
      "Test loss: 0.029446233063936234\n",
      "Epoch: 341\n",
      "Train loss: 0.02882135473191738\n",
      "Test loss: 0.029388006776571274\n",
      "Epoch: 342\n",
      "Train loss: 0.028790410608053207\n",
      "Test loss: 0.029359523206949234\n",
      "Epoch: 343\n",
      "Train loss: 0.028741445392370224\n",
      "Test loss: 0.029301660135388374\n",
      "Epoch: 344\n",
      "Train loss: 0.028705960139632225\n",
      "Test loss: 0.029273055493831635\n",
      "Epoch: 345\n",
      "Train loss: 0.028688102960586548\n",
      "Test loss: 0.029257632791996002\n",
      "Epoch: 346\n",
      "Train loss: 0.028654413297772408\n",
      "Test loss: 0.02923988364636898\n",
      "Epoch: 347\n",
      "Train loss: 0.028610169887542725\n",
      "Test loss: 0.029177147895097733\n",
      "Epoch: 348\n",
      "Train loss: 0.028564387932419777\n",
      "Test loss: 0.02913753315806389\n",
      "Epoch: 349\n",
      "Train loss: 0.028527049347758293\n",
      "Test loss: 0.029062611982226372\n",
      "Epoch: 350\n",
      "Train loss: 0.028489092364907265\n",
      "Test loss: 0.02905150316655636\n",
      "Epoch: 351\n",
      "Train loss: 0.028470128774642944\n",
      "Test loss: 0.029012691229581833\n",
      "Epoch: 352\n",
      "Train loss: 0.02844730205833912\n",
      "Test loss: 0.02900630608201027\n",
      "Epoch: 353\n",
      "Train loss: 0.028404153883457184\n",
      "Test loss: 0.028965018689632416\n",
      "Epoch: 354\n",
      "Train loss: 0.028363626450300217\n",
      "Test loss: 0.028891246765851974\n",
      "Epoch: 355\n",
      "Train loss: 0.02832621894776821\n",
      "Test loss: 0.028874479234218597\n",
      "Epoch: 356\n",
      "Train loss: 0.028279243037104607\n",
      "Test loss: 0.02881612814962864\n",
      "Epoch: 357\n",
      "Train loss: 0.02824288047850132\n",
      "Test loss: 0.02881409041583538\n",
      "Epoch: 358\n",
      "Train loss: 0.02820996567606926\n",
      "Test loss: 0.028775719925761223\n",
      "Epoch: 359\n",
      "Train loss: 0.028175992891192436\n",
      "Test loss: 0.028758417814970016\n",
      "Epoch: 360\n",
      "Train loss: 0.02811621129512787\n",
      "Test loss: 0.02866353653371334\n",
      "Epoch: 361\n",
      "Train loss: 0.02809234708547592\n",
      "Test loss: 0.028666360303759575\n",
      "Epoch: 362\n",
      "Train loss: 0.028057262301445007\n",
      "Test loss: 0.028625965118408203\n",
      "Epoch: 363\n",
      "Train loss: 0.028020484372973442\n",
      "Test loss: 0.028585294261574745\n",
      "Epoch: 364\n",
      "Train loss: 0.027982845902442932\n",
      "Test loss: 0.028577124699950218\n",
      "Epoch: 365\n",
      "Train loss: 0.027948785573244095\n",
      "Test loss: 0.028531523421406746\n",
      "Epoch: 366\n",
      "Train loss: 0.027924777939915657\n",
      "Test loss: 0.028499312698841095\n",
      "Epoch: 367\n",
      "Train loss: 0.027886344119906425\n",
      "Test loss: 0.028483115136623383\n",
      "Epoch: 368\n",
      "Train loss: 0.027843404561281204\n",
      "Test loss: 0.028432399034500122\n",
      "Epoch: 369\n",
      "Train loss: 0.02780495025217533\n",
      "Test loss: 0.02841034159064293\n",
      "Epoch: 370\n",
      "Train loss: 0.027755770832300186\n",
      "Test loss: 0.028339965268969536\n",
      "Epoch: 371\n",
      "Train loss: 0.027736425399780273\n",
      "Test loss: 0.02833525836467743\n",
      "Epoch: 372\n",
      "Train loss: 0.027700847014784813\n",
      "Test loss: 0.02828812412917614\n",
      "Epoch: 373\n",
      "Train loss: 0.027664199471473694\n",
      "Test loss: 0.028258653357625008\n",
      "Epoch: 374\n",
      "Train loss: 0.027630090713500977\n",
      "Test loss: 0.02821090817451477\n",
      "Epoch: 375\n",
      "Train loss: 0.027592435479164124\n",
      "Test loss: 0.02818564511835575\n",
      "Epoch: 376\n",
      "Train loss: 0.027568314224481583\n",
      "Test loss: 0.028158601373434067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 377\n",
      "Train loss: 0.027524227276444435\n",
      "Test loss: 0.028110669925808907\n",
      "Epoch: 378\n",
      "Train loss: 0.027484171092510223\n",
      "Test loss: 0.028072591871023178\n",
      "Epoch: 379\n",
      "Train loss: 0.02745509333908558\n",
      "Test loss: 0.028025474399328232\n",
      "Epoch: 380\n",
      "Train loss: 0.027423713356256485\n",
      "Test loss: 0.02800414338707924\n",
      "Epoch: 381\n",
      "Train loss: 0.027393681928515434\n",
      "Test loss: 0.02799397148191929\n",
      "Epoch: 382\n",
      "Train loss: 0.027351222932338715\n",
      "Test loss: 0.027928084135055542\n",
      "Epoch: 383\n",
      "Train loss: 0.027322599664330482\n",
      "Test loss: 0.027896001935005188\n",
      "Epoch: 384\n",
      "Train loss: 0.027288563549518585\n",
      "Test loss: 0.027845710515975952\n",
      "Epoch: 385\n",
      "Train loss: 0.027273930609226227\n",
      "Test loss: 0.02785268984735012\n",
      "Epoch: 386\n",
      "Train loss: 0.02724010869860649\n",
      "Test loss: 0.027829254046082497\n",
      "Epoch: 387\n",
      "Train loss: 0.027213364839553833\n",
      "Test loss: 0.027789950370788574\n",
      "Epoch: 388\n",
      "Train loss: 0.027189111337065697\n",
      "Test loss: 0.027790963649749756\n",
      "Epoch: 389\n",
      "Train loss: 0.02715689316391945\n",
      "Test loss: 0.027751363813877106\n",
      "Epoch: 390\n",
      "Train loss: 0.027119819074869156\n",
      "Test loss: 0.02772008627653122\n",
      "Epoch: 391\n",
      "Train loss: 0.027099695056676865\n",
      "Test loss: 0.027683626860380173\n",
      "Epoch: 392\n",
      "Train loss: 0.027081066742539406\n",
      "Test loss: 0.027678275480866432\n",
      "Epoch: 393\n",
      "Train loss: 0.0270521380007267\n",
      "Test loss: 0.02764900028705597\n",
      "Epoch: 394\n",
      "Train loss: 0.02701454423367977\n",
      "Test loss: 0.027610573917627335\n",
      "Epoch: 395\n",
      "Train loss: 0.026996279135346413\n",
      "Test loss: 0.02757740020751953\n",
      "Epoch: 396\n",
      "Train loss: 0.026957500725984573\n",
      "Test loss: 0.027538686990737915\n",
      "Epoch: 397\n",
      "Train loss: 0.026924259960651398\n",
      "Test loss: 0.02752525918185711\n",
      "Epoch: 398\n",
      "Train loss: 0.026901988312602043\n",
      "Test loss: 0.027483148500323296\n",
      "Epoch: 399\n",
      "Train loss: 0.02687055990099907\n",
      "Test loss: 0.027462810277938843\n",
      "Epoch: 400\n",
      "Train loss: 0.026835761964321136\n",
      "Test loss: 0.027423180639743805\n",
      "Epoch: 401\n",
      "Train loss: 0.02680259197950363\n",
      "Test loss: 0.027387047186493874\n",
      "Epoch: 402\n",
      "Train loss: 0.02677769958972931\n",
      "Test loss: 0.027347207069396973\n",
      "Epoch: 403\n",
      "Train loss: 0.026749787852168083\n",
      "Test loss: 0.027321288362145424\n",
      "Epoch: 404\n",
      "Train loss: 0.02671770006418228\n",
      "Test loss: 0.027311336249113083\n",
      "Epoch: 405\n",
      "Train loss: 0.02670656330883503\n",
      "Test loss: 0.027279766276478767\n",
      "Epoch: 406\n",
      "Train loss: 0.02668609470129013\n",
      "Test loss: 0.02723325975239277\n",
      "Epoch: 407\n",
      "Train loss: 0.026657091453671455\n",
      "Test loss: 0.02722260169684887\n",
      "Epoch: 408\n",
      "Train loss: 0.02663036249577999\n",
      "Test loss: 0.027202390134334564\n",
      "Epoch: 409\n",
      "Train loss: 0.02660716138780117\n",
      "Test loss: 0.02718118205666542\n",
      "Epoch: 410\n",
      "Train loss: 0.02658967860043049\n",
      "Test loss: 0.027156170457601547\n",
      "Epoch: 411\n",
      "Train loss: 0.026562437415122986\n",
      "Test loss: 0.027157219126820564\n",
      "Epoch: 412\n",
      "Train loss: 0.026534879580140114\n",
      "Test loss: 0.027119886130094528\n",
      "Epoch: 413\n",
      "Train loss: 0.026504557579755783\n",
      "Test loss: 0.0270881038159132\n",
      "Epoch: 414\n",
      "Train loss: 0.02647995389997959\n",
      "Test loss: 0.027069447562098503\n",
      "Epoch: 415\n",
      "Train loss: 0.026465369388461113\n",
      "Test loss: 0.027047855779528618\n",
      "Epoch: 416\n",
      "Train loss: 0.026448151096701622\n",
      "Test loss: 0.027025528252124786\n",
      "Epoch: 417\n",
      "Train loss: 0.026415454223752022\n",
      "Test loss: 0.02702915295958519\n",
      "Epoch: 418\n",
      "Train loss: 0.026402637362480164\n",
      "Test loss: 0.027029894292354584\n",
      "Epoch: 419\n",
      "Train loss: 0.026378285139799118\n",
      "Test loss: 0.026974011212587357\n",
      "Epoch: 420\n",
      "Train loss: 0.02634584531188011\n",
      "Test loss: 0.026940401643514633\n",
      "Epoch: 421\n",
      "Train loss: 0.02632422000169754\n",
      "Test loss: 0.026919633150100708\n",
      "Epoch: 422\n",
      "Train loss: 0.026295753195881844\n",
      "Test loss: 0.02688908949494362\n",
      "Epoch: 423\n",
      "Train loss: 0.026259886100888252\n",
      "Test loss: 0.026836590841412544\n",
      "Epoch: 424\n",
      "Train loss: 0.026240555569529533\n",
      "Test loss: 0.026844261214137077\n",
      "Epoch: 425\n",
      "Train loss: 0.026222839951515198\n",
      "Test loss: 0.02680402807891369\n",
      "Epoch: 426\n",
      "Train loss: 0.026190519332885742\n",
      "Test loss: 0.02678087167441845\n",
      "Epoch: 427\n",
      "Train loss: 0.02616969496011734\n",
      "Test loss: 0.026757875457406044\n",
      "Epoch: 428\n",
      "Train loss: 0.02614734135568142\n",
      "Test loss: 0.026734618470072746\n",
      "Epoch: 429\n",
      "Train loss: 0.02612956240773201\n",
      "Test loss: 0.02669196017086506\n",
      "Epoch: 430\n",
      "Train loss: 0.026097102090716362\n",
      "Test loss: 0.026682056486606598\n",
      "Epoch: 431\n",
      "Train loss: 0.026068611070513725\n",
      "Test loss: 0.026675637811422348\n",
      "Epoch: 432\n",
      "Train loss: 0.02604636922478676\n",
      "Test loss: 0.026625944301486015\n",
      "Epoch: 433\n",
      "Train loss: 0.02601717785000801\n",
      "Test loss: 0.026612643152475357\n",
      "Epoch: 434\n",
      "Train loss: 0.025985462591052055\n",
      "Test loss: 0.026587018743157387\n",
      "Epoch: 435\n",
      "Train loss: 0.025954926386475563\n",
      "Test loss: 0.026557669043540955\n",
      "Epoch: 436\n",
      "Train loss: 0.025930255651474\n",
      "Test loss: 0.02653525583446026\n",
      "Epoch: 437\n",
      "Train loss: 0.02589254267513752\n",
      "Test loss: 0.026509858667850494\n",
      "Epoch: 438\n",
      "Train loss: 0.025869164615869522\n",
      "Test loss: 0.02651028521358967\n",
      "Epoch: 439\n",
      "Train loss: 0.025844231247901917\n",
      "Test loss: 0.02649748884141445\n",
      "Epoch: 440\n",
      "Train loss: 0.025824910029768944\n",
      "Test loss: 0.026461809873580933\n",
      "Epoch: 441\n",
      "Train loss: 0.025777947157621384\n",
      "Test loss: 0.026420999318361282\n",
      "Epoch: 442\n",
      "Train loss: 0.02576678805053234\n",
      "Test loss: 0.026407327502965927\n",
      "Epoch: 443\n",
      "Train loss: 0.02575310319662094\n",
      "Test loss: 0.026398159563541412\n",
      "Epoch: 444\n",
      "Train loss: 0.025735152885317802\n",
      "Test loss: 0.02635006606578827\n",
      "Epoch: 445\n",
      "Train loss: 0.025719135999679565\n",
      "Test loss: 0.02634526416659355\n",
      "Epoch: 446\n",
      "Train loss: 0.025691097602248192\n",
      "Test loss: 0.026313578709959984\n",
      "Epoch: 447\n",
      "Train loss: 0.02566966973245144\n",
      "Test loss: 0.02628357522189617\n",
      "Epoch: 448\n",
      "Train loss: 0.02564886212348938\n",
      "Test loss: 0.026267658919095993\n",
      "Epoch: 449\n",
      "Train loss: 0.025620225816965103\n",
      "Test loss: 0.02624838426709175\n",
      "Epoch: 450\n",
      "Train loss: 0.02559349313378334\n",
      "Test loss: 0.026222599670290947\n",
      "Epoch: 451\n",
      "Train loss: 0.025575809180736542\n",
      "Test loss: 0.02621152251958847\n",
      "Epoch: 452\n",
      "Train loss: 0.025539075955748558\n",
      "Test loss: 0.026152532547712326\n",
      "Epoch: 453\n",
      "Train loss: 0.02551436610519886\n",
      "Test loss: 0.026121018454432487\n",
      "Epoch: 454\n",
      "Train loss: 0.02549261786043644\n",
      "Test loss: 0.026140842586755753\n",
      "Epoch: 455\n",
      "Train loss: 0.0254688523709774\n",
      "Test loss: 0.0261033046990633\n",
      "Epoch: 456\n",
      "Train loss: 0.025435037910938263\n",
      "Test loss: 0.026062283664941788\n",
      "Epoch: 457\n",
      "Train loss: 0.02540428191423416\n",
      "Test loss: 0.026043009012937546\n",
      "Epoch: 458\n",
      "Train loss: 0.02537737414240837\n",
      "Test loss: 0.02599313110113144\n",
      "Epoch: 459\n",
      "Train loss: 0.02535387873649597\n",
      "Test loss: 0.025965014472603798\n",
      "Epoch: 460\n",
      "Train loss: 0.02532990649342537\n",
      "Test loss: 0.02596825547516346\n",
      "Epoch: 461\n",
      "Train loss: 0.025307675823569298\n",
      "Test loss: 0.025939099490642548\n",
      "Epoch: 462\n",
      "Train loss: 0.02529049664735794\n",
      "Test loss: 0.025925977155566216\n",
      "Epoch: 463\n",
      "Train loss: 0.025267615914344788\n",
      "Test loss: 0.025889864191412926\n",
      "Epoch: 464\n",
      "Train loss: 0.025249050930142403\n",
      "Test loss: 0.025868453085422516\n",
      "Epoch: 465\n",
      "Train loss: 0.025227101519703865\n",
      "Test loss: 0.025831181555986404\n",
      "Epoch: 466\n",
      "Train loss: 0.025208154693245888\n",
      "Test loss: 0.025833752006292343\n",
      "Epoch: 467\n",
      "Train loss: 0.0251888670027256\n",
      "Test loss: 0.025798041373491287\n",
      "Epoch: 468\n",
      "Train loss: 0.02517125755548477\n",
      "Test loss: 0.0258018895983696\n",
      "Epoch: 469\n",
      "Train loss: 0.02514961175620556\n",
      "Test loss: 0.02574121579527855\n",
      "Epoch: 470\n",
      "Train loss: 0.025125157088041306\n",
      "Test loss: 0.025709249079227448\n",
      "Epoch: 471\n",
      "Train loss: 0.025108996778726578\n",
      "Test loss: 0.025702590122818947\n",
      "Epoch: 472\n",
      "Train loss: 0.025095168501138687\n",
      "Test loss: 0.02569439820945263\n",
      "Epoch: 473\n",
      "Train loss: 0.02507917210459709\n",
      "Test loss: 0.025676235556602478\n",
      "Epoch: 474\n",
      "Train loss: 0.025059476494789124\n",
      "Test loss: 0.02564110793173313\n",
      "Epoch: 475\n",
      "Train loss: 0.02503831684589386\n",
      "Test loss: 0.025648964568972588\n",
      "Epoch: 476\n",
      "Train loss: 0.025013776496052742\n",
      "Test loss: 0.025609774515032768\n",
      "Epoch: 477\n",
      "Train loss: 0.024989429861307144\n",
      "Test loss: 0.0255906879901886\n",
      "Epoch: 478\n",
      "Train loss: 0.024961194023489952\n",
      "Test loss: 0.025562476366758347\n",
      "Epoch: 479\n",
      "Train loss: 0.024931060150265694\n",
      "Test loss: 0.02551545761525631\n",
      "Epoch: 480\n",
      "Train loss: 0.024903934448957443\n",
      "Test loss: 0.02547110989689827\n",
      "Epoch: 481\n",
      "Train loss: 0.02487460896372795\n",
      "Test loss: 0.0254688560962677\n",
      "Epoch: 482\n",
      "Train loss: 0.024851053953170776\n",
      "Test loss: 0.02542916126549244\n",
      "Epoch: 483\n",
      "Train loss: 0.02483857423067093\n",
      "Test loss: 0.0254302266985178\n",
      "Epoch: 484\n",
      "Train loss: 0.02481391467154026\n",
      "Test loss: 0.02540135756134987\n",
      "Epoch: 485\n",
      "Train loss: 0.024790199473500252\n",
      "Test loss: 0.025355596095323563\n",
      "Epoch: 486\n",
      "Train loss: 0.024782104417681694\n",
      "Test loss: 0.0253826342523098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 487\n",
      "Train loss: 0.024766067042946815\n",
      "Test loss: 0.025350211188197136\n",
      "Epoch: 488\n",
      "Train loss: 0.024752173572778702\n",
      "Test loss: 0.0253571979701519\n",
      "Epoch: 489\n",
      "Train loss: 0.024721024557948112\n",
      "Test loss: 0.025307297706604004\n",
      "Epoch: 490\n",
      "Train loss: 0.02470240369439125\n",
      "Test loss: 0.025297660380601883\n",
      "Epoch: 491\n",
      "Train loss: 0.0246895644813776\n",
      "Test loss: 0.02527872659265995\n",
      "Epoch: 492\n",
      "Train loss: 0.024668877944350243\n",
      "Test loss: 0.02525186724960804\n",
      "Epoch: 493\n",
      "Train loss: 0.024648843333125114\n",
      "Test loss: 0.02526356279850006\n",
      "Epoch: 494\n",
      "Train loss: 0.024636581540107727\n",
      "Test loss: 0.02521977573633194\n",
      "Epoch: 495\n",
      "Train loss: 0.024616412818431854\n",
      "Test loss: 0.025234008207917213\n",
      "Epoch: 496\n",
      "Train loss: 0.024598507210612297\n",
      "Test loss: 0.025185536593198776\n",
      "Epoch: 497\n",
      "Train loss: 0.024578765034675598\n",
      "Test loss: 0.02518301084637642\n",
      "Epoch: 498\n",
      "Train loss: 0.024555956944823265\n",
      "Test loss: 0.025137506425380707\n",
      "Epoch: 499\n",
      "Train loss: 0.02452935092151165\n",
      "Test loss: 0.025137577205896378\n",
      "====================================================================================================\n",
      "Trainer: BPTrainer (SGD_trainer)\n",
      "Settings: {'method': 'GradientDescentOptimizer', 'args': {'learning_rate': 0.1}}\n",
      "====================================================================================================\n",
      "Epoch: init\n",
      "Train loss: 76.24555969238281\n",
      "Test loss: 89.11460876464844\n",
      "Epoch: 0\n",
      "Train loss: 0.6546657085418701\n",
      "Test loss: 0.6438865661621094\n",
      "Epoch: 1\n",
      "Train loss: 0.6545373797416687\n",
      "Test loss: 0.6439255475997925\n",
      "Epoch: 2\n",
      "Train loss: 0.6544207334518433\n",
      "Test loss: 0.6439657211303711\n",
      "Epoch: 3\n",
      "Train loss: 0.6543142795562744\n",
      "Test loss: 0.6440062522888184\n",
      "Epoch: 4\n",
      "Train loss: 0.6542168259620667\n",
      "Test loss: 0.6440466046333313\n",
      "Epoch: 5\n",
      "Train loss: 0.6541272401809692\n",
      "Test loss: 0.6440861821174622\n",
      "Epoch: 6\n",
      "Train loss: 0.6540445685386658\n",
      "Test loss: 0.6441245079040527\n",
      "Epoch: 7\n",
      "Train loss: 0.6539679765701294\n",
      "Test loss: 0.6441614031791687\n",
      "Epoch: 8\n",
      "Train loss: 0.653896689414978\n",
      "Test loss: 0.6441965103149414\n",
      "Epoch: 9\n",
      "Train loss: 0.6538301706314087\n",
      "Test loss: 0.6442297101020813\n",
      "Epoch: 10\n",
      "Train loss: 0.6537678241729736\n",
      "Test loss: 0.6442608833312988\n",
      "Epoch: 11\n",
      "Train loss: 0.6537089943885803\n",
      "Test loss: 0.6442896127700806\n",
      "Epoch: 12\n",
      "Train loss: 0.6536535024642944\n",
      "Test loss: 0.6443161964416504\n",
      "Epoch: 13\n",
      "Train loss: 0.6536006331443787\n",
      "Test loss: 0.6443403363227844\n",
      "Epoch: 14\n",
      "Train loss: 0.6535503268241882\n",
      "Test loss: 0.644362211227417\n",
      "Epoch: 15\n",
      "Train loss: 0.6535022258758545\n",
      "Test loss: 0.6443817019462585\n",
      "Epoch: 16\n",
      "Train loss: 0.6534558534622192\n",
      "Test loss: 0.6443988084793091\n",
      "Epoch: 17\n",
      "Train loss: 0.653411328792572\n",
      "Test loss: 0.6444136500358582\n",
      "Epoch: 18\n",
      "Train loss: 0.6533682346343994\n",
      "Test loss: 0.6444262862205505\n",
      "Epoch: 19\n",
      "Train loss: 0.6533263921737671\n",
      "Test loss: 0.6444366574287415\n",
      "Epoch: 20\n",
      "Train loss: 0.6532857418060303\n",
      "Test loss: 0.6444449424743652\n",
      "Epoch: 21\n",
      "Train loss: 0.6532460451126099\n",
      "Test loss: 0.6444512009620667\n",
      "Epoch: 22\n",
      "Train loss: 0.6532072424888611\n",
      "Test loss: 0.6444553732872009\n",
      "Epoch: 23\n",
      "Train loss: 0.6531692147254944\n",
      "Test loss: 0.6444576978683472\n",
      "Epoch: 24\n",
      "Train loss: 0.6531318426132202\n",
      "Test loss: 0.6444581747055054\n",
      "Epoch: 25\n",
      "Train loss: 0.6530951857566833\n",
      "Test loss: 0.6444568634033203\n",
      "Epoch: 26\n",
      "Train loss: 0.6530589461326599\n",
      "Test loss: 0.6444538831710815\n",
      "Epoch: 27\n",
      "Train loss: 0.6530232429504395\n",
      "Test loss: 0.6444492340087891\n",
      "Epoch: 28\n",
      "Train loss: 0.6529878377914429\n",
      "Test loss: 0.6444430351257324\n",
      "Epoch: 29\n",
      "Train loss: 0.6529529094696045\n",
      "Test loss: 0.6444354057312012\n",
      "Epoch: 30\n",
      "Train loss: 0.6529182195663452\n",
      "Test loss: 0.6444264054298401\n",
      "Epoch: 31\n",
      "Train loss: 0.6528838276863098\n",
      "Test loss: 0.644416093826294\n",
      "Epoch: 32\n",
      "Train loss: 0.6528496146202087\n",
      "Test loss: 0.6444045305252075\n",
      "Epoch: 33\n",
      "Train loss: 0.6528157591819763\n",
      "Test loss: 0.644391655921936\n",
      "Epoch: 34\n",
      "Train loss: 0.6527819633483887\n",
      "Test loss: 0.6443777084350586\n",
      "Epoch: 35\n",
      "Train loss: 0.6527484655380249\n",
      "Test loss: 0.64436274766922\n",
      "Epoch: 36\n",
      "Train loss: 0.6527150869369507\n",
      "Test loss: 0.6443467140197754\n",
      "Epoch: 37\n",
      "Train loss: 0.6526817679405212\n",
      "Test loss: 0.6443297863006592\n",
      "Epoch: 38\n",
      "Train loss: 0.6526486277580261\n",
      "Test loss: 0.6443119645118713\n",
      "Epoch: 39\n",
      "Train loss: 0.652615487575531\n",
      "Test loss: 0.6442931890487671\n",
      "Epoch: 40\n",
      "Train loss: 0.652582585811615\n",
      "Test loss: 0.6442736387252808\n",
      "Epoch: 41\n",
      "Train loss: 0.6525497436523438\n",
      "Test loss: 0.6442534327507019\n",
      "Epoch: 42\n",
      "Train loss: 0.6525169014930725\n",
      "Test loss: 0.6442323923110962\n",
      "Epoch: 43\n",
      "Train loss: 0.6524841785430908\n",
      "Test loss: 0.6442107558250427\n",
      "Epoch: 44\n",
      "Train loss: 0.6524514555931091\n",
      "Test loss: 0.644188404083252\n",
      "Epoch: 45\n",
      "Train loss: 0.6524187922477722\n",
      "Test loss: 0.6441654562950134\n",
      "Epoch: 46\n",
      "Train loss: 0.6523862481117249\n",
      "Test loss: 0.6441420316696167\n",
      "Epoch: 47\n",
      "Train loss: 0.6523536443710327\n",
      "Test loss: 0.6441180109977722\n",
      "Epoch: 48\n",
      "Train loss: 0.6523212194442749\n",
      "Test loss: 0.6440935134887695\n",
      "Epoch: 49\n",
      "Train loss: 0.6522886753082275\n",
      "Test loss: 0.6440685987472534\n",
      "Epoch: 50\n",
      "Train loss: 0.6522562503814697\n",
      "Test loss: 0.6440432071685791\n",
      "Epoch: 51\n",
      "Train loss: 0.6522237658500671\n",
      "Test loss: 0.6440173387527466\n",
      "Epoch: 52\n",
      "Train loss: 0.6521913409233093\n",
      "Test loss: 0.6439911127090454\n",
      "Epoch: 53\n",
      "Train loss: 0.6521589756011963\n",
      "Test loss: 0.6439645290374756\n",
      "Epoch: 54\n",
      "Train loss: 0.6521266102790833\n",
      "Test loss: 0.6439375877380371\n",
      "Epoch: 55\n",
      "Train loss: 0.6520942449569702\n",
      "Test loss: 0.6439103484153748\n",
      "Epoch: 56\n",
      "Train loss: 0.652061939239502\n",
      "Test loss: 0.6438828706741333\n",
      "Epoch: 57\n",
      "Train loss: 0.6520296931266785\n",
      "Test loss: 0.6438550353050232\n",
      "Epoch: 58\n",
      "Train loss: 0.6519973278045654\n",
      "Test loss: 0.643826961517334\n",
      "Epoch: 59\n",
      "Train loss: 0.6519650816917419\n",
      "Test loss: 0.6437985897064209\n",
      "Epoch: 60\n",
      "Train loss: 0.6519327163696289\n",
      "Test loss: 0.6437699794769287\n",
      "Epoch: 61\n",
      "Train loss: 0.6519005298614502\n",
      "Test loss: 0.6437411308288574\n",
      "Epoch: 62\n",
      "Train loss: 0.6518683433532715\n",
      "Test loss: 0.6437121629714966\n",
      "Epoch: 63\n",
      "Train loss: 0.6518360376358032\n",
      "Test loss: 0.6436830163002014\n",
      "Epoch: 64\n",
      "Train loss: 0.6518038511276245\n",
      "Test loss: 0.6436536312103271\n",
      "Epoch: 65\n",
      "Train loss: 0.6517716646194458\n",
      "Test loss: 0.6436240673065186\n",
      "Epoch: 66\n",
      "Train loss: 0.6517394185066223\n",
      "Test loss: 0.6435943841934204\n",
      "Epoch: 67\n",
      "Train loss: 0.6517072319984436\n",
      "Test loss: 0.6435645222663879\n",
      "Epoch: 68\n",
      "Train loss: 0.6516750454902649\n",
      "Test loss: 0.6435345411300659\n",
      "Epoch: 69\n",
      "Train loss: 0.6516428589820862\n",
      "Test loss: 0.6435044407844543\n",
      "Epoch: 70\n",
      "Train loss: 0.6516107320785522\n",
      "Test loss: 0.6434742212295532\n",
      "Epoch: 71\n",
      "Train loss: 0.6515785455703735\n",
      "Test loss: 0.6434438824653625\n",
      "Epoch: 72\n",
      "Train loss: 0.6515463590621948\n",
      "Test loss: 0.6434134244918823\n",
      "Epoch: 73\n",
      "Train loss: 0.6515142321586609\n",
      "Test loss: 0.6433829069137573\n",
      "Epoch: 74\n",
      "Train loss: 0.651482105255127\n",
      "Test loss: 0.6433522701263428\n",
      "Epoch: 75\n",
      "Train loss: 0.6514499187469482\n",
      "Test loss: 0.6433215141296387\n",
      "Epoch: 76\n",
      "Train loss: 0.6514177918434143\n",
      "Test loss: 0.643290638923645\n",
      "Epoch: 77\n",
      "Train loss: 0.6513857245445251\n",
      "Test loss: 0.6432597637176514\n",
      "Epoch: 78\n",
      "Train loss: 0.6513535976409912\n",
      "Test loss: 0.6432288885116577\n",
      "Epoch: 79\n",
      "Train loss: 0.651321530342102\n",
      "Test loss: 0.643197774887085\n",
      "Epoch: 80\n",
      "Train loss: 0.6512894630432129\n",
      "Test loss: 0.6431667804718018\n",
      "Epoch: 81\n",
      "Train loss: 0.6512573957443237\n",
      "Test loss: 0.6431355476379395\n",
      "Epoch: 82\n",
      "Train loss: 0.6512253284454346\n",
      "Test loss: 0.6431044340133667\n",
      "Epoch: 83\n",
      "Train loss: 0.6511931419372559\n",
      "Test loss: 0.6430732011795044\n",
      "Epoch: 84\n",
      "Train loss: 0.6511610746383667\n",
      "Test loss: 0.6430418491363525\n",
      "Epoch: 85\n",
      "Train loss: 0.6511289477348328\n",
      "Test loss: 0.6430105566978455\n",
      "Epoch: 86\n",
      "Train loss: 0.6510969996452332\n",
      "Test loss: 0.6429791450500488\n",
      "Epoch: 87\n",
      "Train loss: 0.6510649919509888\n",
      "Test loss: 0.642947793006897\n",
      "Epoch: 88\n",
      "Train loss: 0.6510329246520996\n",
      "Test loss: 0.6429163217544556\n",
      "Epoch: 89\n",
      "Train loss: 0.6510008573532104\n",
      "Test loss: 0.6428848505020142\n",
      "Epoch: 90\n",
      "Train loss: 0.6509687304496765\n",
      "Test loss: 0.6428533792495728\n",
      "Epoch: 91\n",
      "Train loss: 0.6509367823600769\n",
      "Test loss: 0.6428218483924866\n",
      "Epoch: 92\n",
      "Train loss: 0.6509047746658325\n",
      "Test loss: 0.6427903175354004\n",
      "Epoch: 93\n",
      "Train loss: 0.6508727073669434\n",
      "Test loss: 0.6427587270736694\n",
      "Epoch: 94\n",
      "Train loss: 0.6508407592773438\n",
      "Test loss: 0.6427271962165833\n",
      "Epoch: 95\n",
      "Train loss: 0.6508088111877441\n",
      "Test loss: 0.6426955461502075\n",
      "Epoch: 96\n",
      "Train loss: 0.6507768034934998\n",
      "Test loss: 0.6426638960838318\n",
      "Epoch: 97\n",
      "Train loss: 0.6507447957992554\n",
      "Test loss: 0.6426323056221008\n",
      "Epoch: 98\n",
      "Train loss: 0.6507128477096558\n",
      "Test loss: 0.6426005959510803\n",
      "Epoch: 99\n",
      "Train loss: 0.6506808400154114\n",
      "Test loss: 0.6425688862800598\n",
      "Epoch: 100\n",
      "Train loss: 0.6506488919258118\n",
      "Test loss: 0.6425372362136841\n",
      "Epoch: 101\n",
      "Train loss: 0.6506170034408569\n",
      "Test loss: 0.6425055265426636\n",
      "Epoch: 102\n",
      "Train loss: 0.6505850553512573\n",
      "Test loss: 0.6424738764762878\n",
      "Epoch: 103\n",
      "Train loss: 0.6505529880523682\n",
      "Test loss: 0.6424421668052673\n",
      "Epoch: 104\n",
      "Train loss: 0.6505210399627686\n",
      "Test loss: 0.6424104571342468\n",
      "Epoch: 105\n",
      "Train loss: 0.6504892110824585\n",
      "Test loss: 0.6423786878585815\n",
      "Epoch: 106\n",
      "Train loss: 0.6504572629928589\n",
      "Test loss: 0.6423469185829163\n",
      "Epoch: 107\n",
      "Train loss: 0.6504253149032593\n",
      "Test loss: 0.642315149307251\n",
      "Epoch: 108\n",
      "Train loss: 0.6503934264183044\n",
      "Test loss: 0.6422834396362305\n",
      "Epoch: 109\n",
      "Train loss: 0.6503614187240601\n",
      "Test loss: 0.6422516703605652\n",
      "Epoch: 110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6503296494483948\n",
      "Test loss: 0.6422199606895447\n",
      "Epoch: 111\n",
      "Train loss: 0.6502977013587952\n",
      "Test loss: 0.6421881318092346\n",
      "Epoch: 112\n",
      "Train loss: 0.6502658128738403\n",
      "Test loss: 0.6421564817428589\n",
      "Epoch: 113\n",
      "Train loss: 0.6502339839935303\n",
      "Test loss: 0.6421247124671936\n",
      "Epoch: 114\n",
      "Train loss: 0.6502020955085754\n",
      "Test loss: 0.6420928835868835\n",
      "Epoch: 115\n",
      "Train loss: 0.6501702070236206\n",
      "Test loss: 0.6420611143112183\n",
      "Epoch: 116\n",
      "Train loss: 0.6501383781433105\n",
      "Test loss: 0.6420294046401978\n",
      "Epoch: 117\n",
      "Train loss: 0.6501064896583557\n",
      "Test loss: 0.6419975757598877\n",
      "Epoch: 118\n",
      "Train loss: 0.6500746607780457\n",
      "Test loss: 0.6419658660888672\n",
      "Epoch: 119\n",
      "Train loss: 0.6500428318977356\n",
      "Test loss: 0.6419341564178467\n",
      "Epoch: 120\n",
      "Train loss: 0.6500109434127808\n",
      "Test loss: 0.6419022679328918\n",
      "Epoch: 121\n",
      "Train loss: 0.6499791145324707\n",
      "Test loss: 0.6418704986572266\n",
      "Epoch: 122\n",
      "Train loss: 0.6499473452568054\n",
      "Test loss: 0.641838788986206\n",
      "Epoch: 123\n",
      "Train loss: 0.6499154567718506\n",
      "Test loss: 0.6418070793151855\n",
      "Epoch: 124\n",
      "Train loss: 0.6498837471008301\n",
      "Test loss: 0.6417753100395203\n",
      "Epoch: 125\n",
      "Train loss: 0.64985191822052\n",
      "Test loss: 0.641743540763855\n",
      "Epoch: 126\n",
      "Train loss: 0.64982008934021\n",
      "Test loss: 0.6417118310928345\n",
      "Epoch: 127\n",
      "Train loss: 0.6497882604598999\n",
      "Test loss: 0.6416800618171692\n",
      "Epoch: 128\n",
      "Train loss: 0.6497564911842346\n",
      "Test loss: 0.6416482925415039\n",
      "Epoch: 129\n",
      "Train loss: 0.6497247219085693\n",
      "Test loss: 0.6416165828704834\n",
      "Epoch: 130\n",
      "Train loss: 0.649692952632904\n",
      "Test loss: 0.6415848731994629\n",
      "Epoch: 131\n",
      "Train loss: 0.6496611833572388\n",
      "Test loss: 0.6415531039237976\n",
      "Epoch: 132\n",
      "Train loss: 0.6496294140815735\n",
      "Test loss: 0.6415213942527771\n",
      "Epoch: 133\n",
      "Train loss: 0.6495976448059082\n",
      "Test loss: 0.6414896845817566\n",
      "Epoch: 134\n",
      "Train loss: 0.6495659351348877\n",
      "Test loss: 0.6414579153060913\n",
      "Epoch: 135\n",
      "Train loss: 0.6495342254638672\n",
      "Test loss: 0.6414262652397156\n",
      "Epoch: 136\n",
      "Train loss: 0.6495024561882019\n",
      "Test loss: 0.6413944959640503\n",
      "Epoch: 137\n",
      "Train loss: 0.6494706869125366\n",
      "Test loss: 0.6413628458976746\n",
      "Epoch: 138\n",
      "Train loss: 0.6494389176368713\n",
      "Test loss: 0.641331136226654\n",
      "Epoch: 139\n",
      "Train loss: 0.6494072675704956\n",
      "Test loss: 0.6412994265556335\n",
      "Epoch: 140\n",
      "Train loss: 0.6493755578994751\n",
      "Test loss: 0.6412677764892578\n",
      "Epoch: 141\n",
      "Train loss: 0.6493438482284546\n",
      "Test loss: 0.6412361264228821\n",
      "Epoch: 142\n",
      "Train loss: 0.6493121385574341\n",
      "Test loss: 0.6412044167518616\n",
      "Epoch: 143\n",
      "Train loss: 0.6492804884910583\n",
      "Test loss: 0.6411727070808411\n",
      "Epoch: 144\n",
      "Train loss: 0.6492488384246826\n",
      "Test loss: 0.6411410570144653\n",
      "Epoch: 145\n",
      "Train loss: 0.6492171287536621\n",
      "Test loss: 0.6411094069480896\n",
      "Epoch: 146\n",
      "Train loss: 0.6491854190826416\n",
      "Test loss: 0.6410777568817139\n",
      "Epoch: 147\n",
      "Train loss: 0.6491537094116211\n",
      "Test loss: 0.6410461068153381\n",
      "Epoch: 148\n",
      "Train loss: 0.6491220593452454\n",
      "Test loss: 0.6410144567489624\n",
      "Epoch: 149\n",
      "Train loss: 0.6490904688835144\n",
      "Test loss: 0.6409828066825867\n",
      "Epoch: 150\n",
      "Train loss: 0.6490587592124939\n",
      "Test loss: 0.6409511566162109\n",
      "Epoch: 151\n",
      "Train loss: 0.6490271091461182\n",
      "Test loss: 0.64091956615448\n",
      "Epoch: 152\n",
      "Train loss: 0.6489955186843872\n",
      "Test loss: 0.640887975692749\n",
      "Epoch: 153\n",
      "Train loss: 0.6489639282226562\n",
      "Test loss: 0.6408563256263733\n",
      "Epoch: 154\n",
      "Train loss: 0.6489323377609253\n",
      "Test loss: 0.6408246755599976\n",
      "Epoch: 155\n",
      "Train loss: 0.6489006876945496\n",
      "Test loss: 0.6407930850982666\n",
      "Epoch: 156\n",
      "Train loss: 0.6488690972328186\n",
      "Test loss: 0.6407614946365356\n",
      "Epoch: 157\n",
      "Train loss: 0.6488375067710876\n",
      "Test loss: 0.6407299041748047\n",
      "Epoch: 158\n",
      "Train loss: 0.6488058567047119\n",
      "Test loss: 0.640698254108429\n",
      "Epoch: 159\n",
      "Train loss: 0.648774266242981\n",
      "Test loss: 0.6406667232513428\n",
      "Epoch: 160\n",
      "Train loss: 0.64874267578125\n",
      "Test loss: 0.6406351327896118\n",
      "Epoch: 161\n",
      "Train loss: 0.6487111449241638\n",
      "Test loss: 0.6406035423278809\n",
      "Epoch: 162\n",
      "Train loss: 0.6486796140670776\n",
      "Test loss: 0.6405720114707947\n",
      "Epoch: 163\n",
      "Train loss: 0.6486480236053467\n",
      "Test loss: 0.6405404210090637\n",
      "Epoch: 164\n",
      "Train loss: 0.648616373538971\n",
      "Test loss: 0.6405088901519775\n",
      "Epoch: 165\n",
      "Train loss: 0.6485848426818848\n",
      "Test loss: 0.6404773592948914\n",
      "Epoch: 166\n",
      "Train loss: 0.6485533118247986\n",
      "Test loss: 0.6404457688331604\n",
      "Epoch: 167\n",
      "Train loss: 0.6485217809677124\n",
      "Test loss: 0.6404142379760742\n",
      "Epoch: 168\n",
      "Train loss: 0.648490309715271\n",
      "Test loss: 0.6403827667236328\n",
      "Epoch: 169\n",
      "Train loss: 0.64845871925354\n",
      "Test loss: 0.6403511762619019\n",
      "Epoch: 170\n",
      "Train loss: 0.6484272480010986\n",
      "Test loss: 0.6403197050094604\n",
      "Epoch: 171\n",
      "Train loss: 0.6483957171440125\n",
      "Test loss: 0.6402881741523743\n",
      "Epoch: 172\n",
      "Train loss: 0.6483641862869263\n",
      "Test loss: 0.6402566432952881\n",
      "Epoch: 173\n",
      "Train loss: 0.6483326554298401\n",
      "Test loss: 0.6402251124382019\n",
      "Epoch: 174\n",
      "Train loss: 0.6483012437820435\n",
      "Test loss: 0.6401937007904053\n",
      "Epoch: 175\n",
      "Train loss: 0.6482697129249573\n",
      "Test loss: 0.6401622295379639\n",
      "Epoch: 176\n",
      "Train loss: 0.6482383012771606\n",
      "Test loss: 0.6401308178901672\n",
      "Epoch: 177\n",
      "Train loss: 0.6482068300247192\n",
      "Test loss: 0.6400992274284363\n",
      "Epoch: 178\n",
      "Train loss: 0.6481753587722778\n",
      "Test loss: 0.6400677561759949\n",
      "Epoch: 179\n",
      "Train loss: 0.6481438875198364\n",
      "Test loss: 0.6400362849235535\n",
      "Epoch: 180\n",
      "Train loss: 0.648112416267395\n",
      "Test loss: 0.6400048732757568\n",
      "Epoch: 181\n",
      "Train loss: 0.6480808854103088\n",
      "Test loss: 0.6399733424186707\n",
      "Epoch: 182\n",
      "Train loss: 0.6480494141578674\n",
      "Test loss: 0.639941930770874\n",
      "Epoch: 183\n",
      "Train loss: 0.6480180025100708\n",
      "Test loss: 0.6399105191230774\n",
      "Epoch: 184\n",
      "Train loss: 0.647986650466919\n",
      "Test loss: 0.6398791074752808\n",
      "Epoch: 185\n",
      "Train loss: 0.6479551792144775\n",
      "Test loss: 0.6398476362228394\n",
      "Epoch: 186\n",
      "Train loss: 0.6479237675666809\n",
      "Test loss: 0.6398162245750427\n",
      "Epoch: 187\n",
      "Train loss: 0.6478923559188843\n",
      "Test loss: 0.6397848725318909\n",
      "Epoch: 188\n",
      "Train loss: 0.6478609442710876\n",
      "Test loss: 0.6397534608840942\n",
      "Epoch: 189\n",
      "Train loss: 0.647829532623291\n",
      "Test loss: 0.6397220492362976\n",
      "Epoch: 190\n",
      "Train loss: 0.6477981805801392\n",
      "Test loss: 0.639690637588501\n",
      "Epoch: 191\n",
      "Train loss: 0.6477668285369873\n",
      "Test loss: 0.6396592855453491\n",
      "Epoch: 192\n",
      "Train loss: 0.6477354764938354\n",
      "Test loss: 0.6396279335021973\n",
      "Epoch: 193\n",
      "Train loss: 0.647704005241394\n",
      "Test loss: 0.6395965218544006\n",
      "Epoch: 194\n",
      "Train loss: 0.6476726531982422\n",
      "Test loss: 0.6395651698112488\n",
      "Epoch: 195\n",
      "Train loss: 0.6476413011550903\n",
      "Test loss: 0.6395338177680969\n",
      "Epoch: 196\n",
      "Train loss: 0.6476098895072937\n",
      "Test loss: 0.6395024657249451\n",
      "Epoch: 197\n",
      "Train loss: 0.6475785970687866\n",
      "Test loss: 0.6394711136817932\n",
      "Epoch: 198\n",
      "Train loss: 0.6475472450256348\n",
      "Test loss: 0.6394398212432861\n",
      "Epoch: 199\n",
      "Train loss: 0.6475158929824829\n",
      "Test loss: 0.6394084692001343\n",
      "Epoch: 200\n",
      "Train loss: 0.647484540939331\n",
      "Test loss: 0.6393771171569824\n",
      "Epoch: 201\n",
      "Train loss: 0.647453248500824\n",
      "Test loss: 0.6393457651138306\n",
      "Epoch: 202\n",
      "Train loss: 0.6474219560623169\n",
      "Test loss: 0.6393144130706787\n",
      "Epoch: 203\n",
      "Train loss: 0.6473905444145203\n",
      "Test loss: 0.6392831206321716\n",
      "Epoch: 204\n",
      "Train loss: 0.647359311580658\n",
      "Test loss: 0.6392518281936646\n",
      "Epoch: 205\n",
      "Train loss: 0.6473280191421509\n",
      "Test loss: 0.6392205357551575\n",
      "Epoch: 206\n",
      "Train loss: 0.6472967267036438\n",
      "Test loss: 0.6391892433166504\n",
      "Epoch: 207\n",
      "Train loss: 0.6472654342651367\n",
      "Test loss: 0.6391579508781433\n",
      "Epoch: 208\n",
      "Train loss: 0.6472341418266296\n",
      "Test loss: 0.6391266584396362\n",
      "Epoch: 209\n",
      "Train loss: 0.6472028493881226\n",
      "Test loss: 0.6390954256057739\n",
      "Epoch: 210\n",
      "Train loss: 0.6471716165542603\n",
      "Test loss: 0.6390640735626221\n",
      "Epoch: 211\n",
      "Train loss: 0.6471403241157532\n",
      "Test loss: 0.6390328407287598\n",
      "Epoch: 212\n",
      "Train loss: 0.6471090912818909\n",
      "Test loss: 0.6390016078948975\n",
      "Epoch: 213\n",
      "Train loss: 0.6470777988433838\n",
      "Test loss: 0.6389703750610352\n",
      "Epoch: 214\n",
      "Train loss: 0.6470465660095215\n",
      "Test loss: 0.6389391422271729\n",
      "Epoch: 215\n",
      "Train loss: 0.6470153331756592\n",
      "Test loss: 0.6389079093933105\n",
      "Epoch: 216\n",
      "Train loss: 0.6469841003417969\n",
      "Test loss: 0.6388766765594482\n",
      "Epoch: 217\n",
      "Train loss: 0.6469528675079346\n",
      "Test loss: 0.6388453841209412\n",
      "Epoch: 218\n",
      "Train loss: 0.6469216346740723\n",
      "Test loss: 0.6388142108917236\n",
      "Epoch: 219\n",
      "Train loss: 0.6468904614448547\n",
      "Test loss: 0.6387829780578613\n",
      "Epoch: 220\n",
      "Train loss: 0.6468592882156372\n",
      "Test loss: 0.6387518048286438\n",
      "Epoch: 221\n",
      "Train loss: 0.6468280553817749\n",
      "Test loss: 0.6387205719947815\n",
      "Epoch: 222\n",
      "Train loss: 0.6467968821525574\n",
      "Test loss: 0.638689398765564\n",
      "Epoch: 223\n",
      "Train loss: 0.6467655897140503\n",
      "Test loss: 0.6386582255363464\n",
      "Epoch: 224\n",
      "Train loss: 0.6467344164848328\n",
      "Test loss: 0.6386269927024841\n",
      "Epoch: 225\n",
      "Train loss: 0.64670330286026\n",
      "Test loss: 0.6385958790779114\n",
      "Epoch: 226\n",
      "Train loss: 0.6466721296310425\n",
      "Test loss: 0.6385647058486938\n",
      "Epoch: 227\n",
      "Train loss: 0.646640956401825\n",
      "Test loss: 0.6385334730148315\n",
      "Epoch: 228\n",
      "Train loss: 0.6466097831726074\n",
      "Test loss: 0.6385023593902588\n",
      "Epoch: 229\n",
      "Train loss: 0.6465786099433899\n",
      "Test loss: 0.6384711861610413\n",
      "Epoch: 230\n",
      "Train loss: 0.6465474963188171\n",
      "Test loss: 0.6384400725364685\n",
      "Epoch: 231\n",
      "Train loss: 0.6465163230895996\n",
      "Test loss: 0.6384089589118958\n",
      "Epoch: 232\n",
      "Train loss: 0.6464852690696716\n",
      "Test loss: 0.6383777856826782\n",
      "Epoch: 233\n",
      "Train loss: 0.6464540958404541\n",
      "Test loss: 0.6383466720581055\n",
      "Epoch: 234\n",
      "Train loss: 0.6464229822158813\n",
      "Test loss: 0.6383155584335327\n",
      "Epoch: 235\n",
      "Train loss: 0.6463918685913086\n",
      "Test loss: 0.63828444480896\n",
      "Epoch: 236\n",
      "Train loss: 0.6463607549667358\n",
      "Test loss: 0.6382533311843872\n",
      "Epoch: 237\n",
      "Train loss: 0.6463296413421631\n",
      "Test loss: 0.6382222771644592\n",
      "Epoch: 238\n",
      "Train loss: 0.6462985873222351\n",
      "Test loss: 0.6381911635398865\n",
      "Epoch: 239\n",
      "Train loss: 0.6462674736976624\n",
      "Test loss: 0.6381601095199585\n",
      "Epoch: 240\n",
      "Train loss: 0.6462363600730896\n",
      "Test loss: 0.6381289958953857\n",
      "Epoch: 241\n",
      "Train loss: 0.6462053060531616\n",
      "Test loss: 0.638097882270813\n",
      "Epoch: 242\n",
      "Train loss: 0.6461743116378784\n",
      "Test loss: 0.638066828250885\n",
      "Epoch: 243\n",
      "Train loss: 0.6461432576179504\n",
      "Test loss: 0.638035774230957\n",
      "Epoch: 244\n",
      "Train loss: 0.6461121439933777\n",
      "Test loss: 0.638004720211029\n",
      "Epoch: 245\n",
      "Train loss: 0.6460810899734497\n",
      "Test loss: 0.6379736661911011\n",
      "Epoch: 246\n",
      "Train loss: 0.6460500359535217\n",
      "Test loss: 0.6379426121711731\n",
      "Epoch: 247\n",
      "Train loss: 0.6460189819335938\n",
      "Test loss: 0.6379115581512451\n",
      "Epoch: 248\n",
      "Train loss: 0.6459879279136658\n",
      "Test loss: 0.6378805637359619\n",
      "Epoch: 249\n",
      "Train loss: 0.6459568738937378\n",
      "Test loss: 0.6378495097160339\n",
      "Epoch: 250\n",
      "Train loss: 0.6459258794784546\n",
      "Test loss: 0.6378185153007507\n",
      "Epoch: 251\n",
      "Train loss: 0.6458948850631714\n",
      "Test loss: 0.6377875208854675\n",
      "Epoch: 252\n",
      "Train loss: 0.6458638906478882\n",
      "Test loss: 0.6377564668655396\n",
      "Epoch: 253\n",
      "Train loss: 0.645832896232605\n",
      "Test loss: 0.6377254724502563\n",
      "Epoch: 254\n",
      "Train loss: 0.6458019018173218\n",
      "Test loss: 0.6376944780349731\n",
      "Epoch: 255\n",
      "Train loss: 0.6457709074020386\n",
      "Test loss: 0.6376634836196899\n",
      "Epoch: 256\n",
      "Train loss: 0.6457399129867554\n",
      "Test loss: 0.6376324892044067\n",
      "Epoch: 257\n",
      "Train loss: 0.6457089185714722\n",
      "Test loss: 0.6376014947891235\n",
      "Epoch: 258\n",
      "Train loss: 0.6456779837608337\n",
      "Test loss: 0.6375706195831299\n",
      "Epoch: 259\n",
      "Train loss: 0.6456470489501953\n",
      "Test loss: 0.6375396251678467\n",
      "Epoch: 260\n",
      "Train loss: 0.6456160545349121\n",
      "Test loss: 0.6375086307525635\n",
      "Epoch: 261\n",
      "Train loss: 0.6455851197242737\n",
      "Test loss: 0.637477695941925\n",
      "Epoch: 262\n",
      "Train loss: 0.6455541849136353\n",
      "Test loss: 0.6374467611312866\n",
      "Epoch: 263\n",
      "Train loss: 0.645523190498352\n",
      "Test loss: 0.6374157667160034\n",
      "Epoch: 264\n",
      "Train loss: 0.6454922556877136\n",
      "Test loss: 0.6373848915100098\n",
      "Epoch: 265\n",
      "Train loss: 0.6454613208770752\n",
      "Test loss: 0.6373539566993713\n",
      "Epoch: 266\n",
      "Train loss: 0.6454304456710815\n",
      "Test loss: 0.6373230218887329\n",
      "Epoch: 267\n",
      "Train loss: 0.6453995108604431\n",
      "Test loss: 0.6372921466827393\n",
      "Epoch: 268\n",
      "Train loss: 0.6453685760498047\n",
      "Test loss: 0.6372612118721008\n",
      "Epoch: 269\n",
      "Train loss: 0.645337700843811\n",
      "Test loss: 0.6372302770614624\n",
      "Epoch: 270\n",
      "Train loss: 0.6453068256378174\n",
      "Test loss: 0.6371994018554688\n",
      "Epoch: 271\n",
      "Train loss: 0.645275890827179\n",
      "Test loss: 0.6371685266494751\n",
      "Epoch: 272\n",
      "Train loss: 0.6452450156211853\n",
      "Test loss: 0.6371376514434814\n",
      "Epoch: 273\n",
      "Train loss: 0.6452141404151917\n",
      "Test loss: 0.6371067762374878\n",
      "Epoch: 274\n",
      "Train loss: 0.6451833248138428\n",
      "Test loss: 0.6370759010314941\n",
      "Epoch: 275\n",
      "Train loss: 0.6451524496078491\n",
      "Test loss: 0.6370450258255005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 276\n",
      "Train loss: 0.6451215744018555\n",
      "Test loss: 0.6370142102241516\n",
      "Epoch: 277\n",
      "Train loss: 0.6450906991958618\n",
      "Test loss: 0.6369832754135132\n",
      "Epoch: 278\n",
      "Train loss: 0.6450598835945129\n",
      "Test loss: 0.6369524598121643\n",
      "Epoch: 279\n",
      "Train loss: 0.6450290083885193\n",
      "Test loss: 0.6369216442108154\n",
      "Epoch: 280\n",
      "Train loss: 0.6449981927871704\n",
      "Test loss: 0.6368907690048218\n",
      "Epoch: 281\n",
      "Train loss: 0.6449673771858215\n",
      "Test loss: 0.6368600130081177\n",
      "Epoch: 282\n",
      "Train loss: 0.6449365615844727\n",
      "Test loss: 0.6368291974067688\n",
      "Epoch: 283\n",
      "Train loss: 0.6449057459831238\n",
      "Test loss: 0.6367983222007751\n",
      "Epoch: 284\n",
      "Train loss: 0.6448749303817749\n",
      "Test loss: 0.636767566204071\n",
      "Epoch: 285\n",
      "Train loss: 0.644844114780426\n",
      "Test loss: 0.6367367506027222\n",
      "Epoch: 286\n",
      "Train loss: 0.6448132991790771\n",
      "Test loss: 0.6367059350013733\n",
      "Epoch: 287\n",
      "Train loss: 0.6447824835777283\n",
      "Test loss: 0.6366751194000244\n",
      "Epoch: 288\n",
      "Train loss: 0.6447517275810242\n",
      "Test loss: 0.6366443634033203\n",
      "Epoch: 289\n",
      "Train loss: 0.6447209715843201\n",
      "Test loss: 0.6366135478019714\n",
      "Epoch: 290\n",
      "Train loss: 0.6446901559829712\n",
      "Test loss: 0.6365827918052673\n",
      "Epoch: 291\n",
      "Train loss: 0.6446593999862671\n",
      "Test loss: 0.6365520358085632\n",
      "Epoch: 292\n",
      "Train loss: 0.644628643989563\n",
      "Test loss: 0.6365212798118591\n",
      "Epoch: 293\n",
      "Train loss: 0.6445978879928589\n",
      "Test loss: 0.636490523815155\n",
      "Epoch: 294\n",
      "Train loss: 0.6445671319961548\n",
      "Test loss: 0.6364597082138062\n",
      "Epoch: 295\n",
      "Train loss: 0.6445364356040955\n",
      "Test loss: 0.6364290118217468\n",
      "Epoch: 296\n",
      "Train loss: 0.6445056796073914\n",
      "Test loss: 0.6363983154296875\n",
      "Epoch: 297\n",
      "Train loss: 0.644474983215332\n",
      "Test loss: 0.6363675594329834\n",
      "Epoch: 298\n",
      "Train loss: 0.6444442272186279\n",
      "Test loss: 0.6363368630409241\n",
      "Epoch: 299\n",
      "Train loss: 0.6444135308265686\n",
      "Test loss: 0.6363061666488647\n",
      "Epoch: 300\n",
      "Train loss: 0.6443828344345093\n",
      "Test loss: 0.6362754106521606\n",
      "Epoch: 301\n",
      "Train loss: 0.6443520784378052\n",
      "Test loss: 0.6362447142601013\n",
      "Epoch: 302\n",
      "Train loss: 0.6443213820457458\n",
      "Test loss: 0.636214017868042\n",
      "Epoch: 303\n",
      "Train loss: 0.6442906856536865\n",
      "Test loss: 0.6361833214759827\n",
      "Epoch: 304\n",
      "Train loss: 0.6442599892616272\n",
      "Test loss: 0.6361526250839233\n",
      "Epoch: 305\n",
      "Train loss: 0.6442293524742126\n",
      "Test loss: 0.6361219882965088\n",
      "Epoch: 306\n",
      "Train loss: 0.6441986560821533\n",
      "Test loss: 0.6360912919044495\n",
      "Epoch: 307\n",
      "Train loss: 0.6441680192947388\n",
      "Test loss: 0.6360605955123901\n",
      "Epoch: 308\n",
      "Train loss: 0.6441373229026794\n",
      "Test loss: 0.6360299587249756\n",
      "Epoch: 309\n",
      "Train loss: 0.6441066265106201\n",
      "Test loss: 0.6359992623329163\n",
      "Epoch: 310\n",
      "Train loss: 0.6440760493278503\n",
      "Test loss: 0.6359686255455017\n",
      "Epoch: 311\n",
      "Train loss: 0.6440454721450806\n",
      "Test loss: 0.6359379887580872\n",
      "Epoch: 312\n",
      "Train loss: 0.644014835357666\n",
      "Test loss: 0.6359074115753174\n",
      "Epoch: 313\n",
      "Train loss: 0.6439841985702515\n",
      "Test loss: 0.6358767151832581\n",
      "Epoch: 314\n",
      "Train loss: 0.6439535617828369\n",
      "Test loss: 0.6358461380004883\n",
      "Epoch: 315\n",
      "Train loss: 0.6439229249954224\n",
      "Test loss: 0.6358155012130737\n",
      "Epoch: 316\n",
      "Train loss: 0.6438923478126526\n",
      "Test loss: 0.6357848644256592\n",
      "Epoch: 317\n",
      "Train loss: 0.643861711025238\n",
      "Test loss: 0.6357542872428894\n",
      "Epoch: 318\n",
      "Train loss: 0.6438311338424683\n",
      "Test loss: 0.6357236504554749\n",
      "Epoch: 319\n",
      "Train loss: 0.6438004970550537\n",
      "Test loss: 0.6356930732727051\n",
      "Epoch: 320\n",
      "Train loss: 0.6437699794769287\n",
      "Test loss: 0.6356624960899353\n",
      "Epoch: 321\n",
      "Train loss: 0.6437393426895142\n",
      "Test loss: 0.6356319189071655\n",
      "Epoch: 322\n",
      "Train loss: 0.6437087655067444\n",
      "Test loss: 0.6356013417243958\n",
      "Epoch: 323\n",
      "Train loss: 0.6436781883239746\n",
      "Test loss: 0.635570764541626\n",
      "Epoch: 324\n",
      "Train loss: 0.6436476707458496\n",
      "Test loss: 0.6355401873588562\n",
      "Epoch: 325\n",
      "Train loss: 0.6436170935630798\n",
      "Test loss: 0.6355096101760864\n",
      "Epoch: 326\n",
      "Train loss: 0.6435865163803101\n",
      "Test loss: 0.6354790925979614\n",
      "Epoch: 327\n",
      "Train loss: 0.6435559988021851\n",
      "Test loss: 0.6354485154151917\n",
      "Epoch: 328\n",
      "Train loss: 0.6435254812240601\n",
      "Test loss: 0.6354179978370667\n",
      "Epoch: 329\n",
      "Train loss: 0.6434949040412903\n",
      "Test loss: 0.6353874802589417\n",
      "Epoch: 330\n",
      "Train loss: 0.6434643864631653\n",
      "Test loss: 0.6353569030761719\n",
      "Epoch: 331\n",
      "Train loss: 0.6434338688850403\n",
      "Test loss: 0.6353263854980469\n",
      "Epoch: 332\n",
      "Train loss: 0.6434033513069153\n",
      "Test loss: 0.6352958679199219\n",
      "Epoch: 333\n",
      "Train loss: 0.6433728337287903\n",
      "Test loss: 0.6352654099464417\n",
      "Epoch: 334\n",
      "Train loss: 0.6433423161506653\n",
      "Test loss: 0.6352348923683167\n",
      "Epoch: 335\n",
      "Train loss: 0.6433118581771851\n",
      "Test loss: 0.6352043747901917\n",
      "Epoch: 336\n",
      "Train loss: 0.6432813405990601\n",
      "Test loss: 0.6351739168167114\n",
      "Epoch: 337\n",
      "Train loss: 0.6432508826255798\n",
      "Test loss: 0.6351433992385864\n",
      "Epoch: 338\n",
      "Train loss: 0.6432203650474548\n",
      "Test loss: 0.6351129412651062\n",
      "Epoch: 339\n",
      "Train loss: 0.6431899070739746\n",
      "Test loss: 0.635082483291626\n",
      "Epoch: 340\n",
      "Train loss: 0.6431594491004944\n",
      "Test loss: 0.635051965713501\n",
      "Epoch: 341\n",
      "Train loss: 0.6431289911270142\n",
      "Test loss: 0.6350215077400208\n",
      "Epoch: 342\n",
      "Train loss: 0.6430985331535339\n",
      "Test loss: 0.6349910497665405\n",
      "Epoch: 343\n",
      "Train loss: 0.6430680751800537\n",
      "Test loss: 0.6349606513977051\n",
      "Epoch: 344\n",
      "Train loss: 0.6430376768112183\n",
      "Test loss: 0.6349301338195801\n",
      "Epoch: 345\n",
      "Train loss: 0.643007218837738\n",
      "Test loss: 0.6348997354507446\n",
      "Epoch: 346\n",
      "Train loss: 0.6429767608642578\n",
      "Test loss: 0.6348692774772644\n",
      "Epoch: 347\n",
      "Train loss: 0.6429463624954224\n",
      "Test loss: 0.634838879108429\n",
      "Epoch: 348\n",
      "Train loss: 0.6429159045219421\n",
      "Test loss: 0.6348084807395935\n",
      "Epoch: 349\n",
      "Train loss: 0.6428855061531067\n",
      "Test loss: 0.6347780823707581\n",
      "Epoch: 350\n",
      "Train loss: 0.642855167388916\n",
      "Test loss: 0.6347476243972778\n",
      "Epoch: 351\n",
      "Train loss: 0.6428247094154358\n",
      "Test loss: 0.6347172260284424\n",
      "Epoch: 352\n",
      "Train loss: 0.6427943110466003\n",
      "Test loss: 0.6346868276596069\n",
      "Epoch: 353\n",
      "Train loss: 0.6427639126777649\n",
      "Test loss: 0.6346564292907715\n",
      "Epoch: 354\n",
      "Train loss: 0.6427335739135742\n",
      "Test loss: 0.634626030921936\n",
      "Epoch: 355\n",
      "Train loss: 0.6427031755447388\n",
      "Test loss: 0.6345956921577454\n",
      "Epoch: 356\n",
      "Train loss: 0.6426727771759033\n",
      "Test loss: 0.6345653533935547\n",
      "Epoch: 357\n",
      "Train loss: 0.6426424384117126\n",
      "Test loss: 0.6345349550247192\n",
      "Epoch: 358\n",
      "Train loss: 0.642612099647522\n",
      "Test loss: 0.6345046758651733\n",
      "Epoch: 359\n",
      "Train loss: 0.6425817608833313\n",
      "Test loss: 0.6344742774963379\n",
      "Epoch: 360\n",
      "Train loss: 0.6425514221191406\n",
      "Test loss: 0.6344438791275024\n",
      "Epoch: 361\n",
      "Train loss: 0.6425210237503052\n",
      "Test loss: 0.6344135999679565\n",
      "Epoch: 362\n",
      "Train loss: 0.6424907445907593\n",
      "Test loss: 0.6343832612037659\n",
      "Epoch: 363\n",
      "Train loss: 0.6424604058265686\n",
      "Test loss: 0.6343529224395752\n",
      "Epoch: 364\n",
      "Train loss: 0.6424301266670227\n",
      "Test loss: 0.6343225836753845\n",
      "Epoch: 365\n",
      "Train loss: 0.642399787902832\n",
      "Test loss: 0.6342922449111938\n",
      "Epoch: 366\n",
      "Train loss: 0.6423695087432861\n",
      "Test loss: 0.6342620253562927\n",
      "Epoch: 367\n",
      "Train loss: 0.6423391103744507\n",
      "Test loss: 0.634231686592102\n",
      "Epoch: 368\n",
      "Train loss: 0.6423088312149048\n",
      "Test loss: 0.6342014074325562\n",
      "Epoch: 369\n",
      "Train loss: 0.6422785520553589\n",
      "Test loss: 0.6341711282730103\n",
      "Epoch: 370\n",
      "Train loss: 0.642248272895813\n",
      "Test loss: 0.6341408491134644\n",
      "Epoch: 371\n",
      "Train loss: 0.6422179937362671\n",
      "Test loss: 0.6341105103492737\n",
      "Epoch: 372\n",
      "Train loss: 0.6421877145767212\n",
      "Test loss: 0.6340802907943726\n",
      "Epoch: 373\n",
      "Train loss: 0.6421574950218201\n",
      "Test loss: 0.6340500116348267\n",
      "Epoch: 374\n",
      "Train loss: 0.642127275466919\n",
      "Test loss: 0.6340197920799255\n",
      "Epoch: 375\n",
      "Train loss: 0.642096996307373\n",
      "Test loss: 0.6339895129203796\n",
      "Epoch: 376\n",
      "Train loss: 0.6420667171478271\n",
      "Test loss: 0.6339592933654785\n",
      "Epoch: 377\n",
      "Train loss: 0.6420364379882812\n",
      "Test loss: 0.6339290142059326\n",
      "Epoch: 378\n",
      "Train loss: 0.6420062184333801\n",
      "Test loss: 0.6338987350463867\n",
      "Epoch: 379\n",
      "Train loss: 0.641975998878479\n",
      "Test loss: 0.6338685750961304\n",
      "Epoch: 380\n",
      "Train loss: 0.6419457793235779\n",
      "Test loss: 0.6338383555412292\n",
      "Epoch: 381\n",
      "Train loss: 0.6419155597686768\n",
      "Test loss: 0.6338081359863281\n",
      "Epoch: 382\n",
      "Train loss: 0.6418853998184204\n",
      "Test loss: 0.633777916431427\n",
      "Epoch: 383\n",
      "Train loss: 0.6418551802635193\n",
      "Test loss: 0.6337476968765259\n",
      "Epoch: 384\n",
      "Train loss: 0.6418249607086182\n",
      "Test loss: 0.6337174773216248\n",
      "Epoch: 385\n",
      "Train loss: 0.641794741153717\n",
      "Test loss: 0.6336873173713684\n",
      "Epoch: 386\n",
      "Train loss: 0.6417645812034607\n",
      "Test loss: 0.6336570978164673\n",
      "Epoch: 387\n",
      "Train loss: 0.6417343616485596\n",
      "Test loss: 0.6336269378662109\n",
      "Epoch: 388\n",
      "Train loss: 0.6417042016983032\n",
      "Test loss: 0.6335967183113098\n",
      "Epoch: 389\n",
      "Train loss: 0.6416740417480469\n",
      "Test loss: 0.6335665583610535\n",
      "Epoch: 390\n",
      "Train loss: 0.6416439414024353\n",
      "Test loss: 0.6335363984107971\n",
      "Epoch: 391\n",
      "Train loss: 0.6416137218475342\n",
      "Test loss: 0.6335062980651855\n",
      "Epoch: 392\n",
      "Train loss: 0.6415835618972778\n",
      "Test loss: 0.6334761381149292\n",
      "Epoch: 393\n",
      "Train loss: 0.6415534019470215\n",
      "Test loss: 0.6334459781646729\n",
      "Epoch: 394\n",
      "Train loss: 0.6415233016014099\n",
      "Test loss: 0.6334158182144165\n",
      "Epoch: 395\n",
      "Train loss: 0.6414932012557983\n",
      "Test loss: 0.6333857178688049\n",
      "Epoch: 396\n",
      "Train loss: 0.641463041305542\n",
      "Test loss: 0.6333555579185486\n",
      "Epoch: 397\n",
      "Train loss: 0.6414329409599304\n",
      "Test loss: 0.633325457572937\n",
      "Epoch: 398\n",
      "Train loss: 0.6414027810096741\n",
      "Test loss: 0.6332952976226807\n",
      "Epoch: 399\n",
      "Train loss: 0.6413726806640625\n",
      "Test loss: 0.6332652568817139\n",
      "Epoch: 400\n",
      "Train loss: 0.6413425803184509\n",
      "Test loss: 0.6332350969314575\n",
      "Epoch: 401\n",
      "Train loss: 0.6413124799728394\n",
      "Test loss: 0.633204996585846\n",
      "Epoch: 402\n",
      "Train loss: 0.6412824392318726\n",
      "Test loss: 0.6331748962402344\n",
      "Epoch: 403\n",
      "Train loss: 0.641252338886261\n",
      "Test loss: 0.6331448554992676\n",
      "Epoch: 404\n",
      "Train loss: 0.6412222385406494\n",
      "Test loss: 0.633114755153656\n",
      "Epoch: 405\n",
      "Train loss: 0.6411921977996826\n",
      "Test loss: 0.6330847144126892\n",
      "Epoch: 406\n",
      "Train loss: 0.6411621570587158\n",
      "Test loss: 0.6330546140670776\n",
      "Epoch: 407\n",
      "Train loss: 0.6411320567131042\n",
      "Test loss: 0.6330245733261108\n",
      "Epoch: 408\n",
      "Train loss: 0.6411019563674927\n",
      "Test loss: 0.632994532585144\n",
      "Epoch: 409\n",
      "Train loss: 0.6410719156265259\n",
      "Test loss: 0.6329644322395325\n",
      "Epoch: 410\n",
      "Train loss: 0.6410418748855591\n",
      "Test loss: 0.6329343914985657\n",
      "Epoch: 411\n",
      "Train loss: 0.6410118341445923\n",
      "Test loss: 0.6329043507575989\n",
      "Epoch: 412\n",
      "Train loss: 0.6409818530082703\n",
      "Test loss: 0.6328743696212769\n",
      "Epoch: 413\n",
      "Train loss: 0.6409518122673035\n",
      "Test loss: 0.6328443288803101\n",
      "Epoch: 414\n",
      "Train loss: 0.6409218311309814\n",
      "Test loss: 0.6328142881393433\n",
      "Epoch: 415\n",
      "Train loss: 0.6408917903900146\n",
      "Test loss: 0.6327843070030212\n",
      "Epoch: 416\n",
      "Train loss: 0.6408617496490479\n",
      "Test loss: 0.6327542662620544\n",
      "Epoch: 417\n",
      "Train loss: 0.640831708908081\n",
      "Test loss: 0.6327242851257324\n",
      "Epoch: 418\n",
      "Train loss: 0.640801727771759\n",
      "Test loss: 0.6326942443847656\n",
      "Epoch: 419\n",
      "Train loss: 0.640771746635437\n",
      "Test loss: 0.6326643228530884\n",
      "Epoch: 420\n",
      "Train loss: 0.6407418251037598\n",
      "Test loss: 0.6326342821121216\n",
      "Epoch: 421\n",
      "Train loss: 0.640711784362793\n",
      "Test loss: 0.6326043605804443\n",
      "Epoch: 422\n",
      "Train loss: 0.6406818628311157\n",
      "Test loss: 0.6325743794441223\n",
      "Epoch: 423\n",
      "Train loss: 0.6406518816947937\n",
      "Test loss: 0.6325443983078003\n",
      "Epoch: 424\n",
      "Train loss: 0.6406219005584717\n",
      "Test loss: 0.6325144171714783\n",
      "Epoch: 425\n",
      "Train loss: 0.6405919790267944\n",
      "Test loss: 0.632484495639801\n",
      "Epoch: 426\n",
      "Train loss: 0.6405620574951172\n",
      "Test loss: 0.6324545741081238\n",
      "Epoch: 427\n",
      "Train loss: 0.6405320763587952\n",
      "Test loss: 0.6324245929718018\n",
      "Epoch: 428\n",
      "Train loss: 0.6405021548271179\n",
      "Test loss: 0.6323946714401245\n",
      "Epoch: 429\n",
      "Train loss: 0.6404722332954407\n",
      "Test loss: 0.6323647499084473\n",
      "Epoch: 430\n",
      "Train loss: 0.6404423117637634\n",
      "Test loss: 0.63233482837677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 431\n",
      "Train loss: 0.6404123902320862\n",
      "Test loss: 0.6323049068450928\n",
      "Epoch: 432\n",
      "Train loss: 0.6403824687004089\n",
      "Test loss: 0.6322749853134155\n",
      "Epoch: 433\n",
      "Train loss: 0.6403525471687317\n",
      "Test loss: 0.6322450637817383\n",
      "Epoch: 434\n",
      "Train loss: 0.6403226852416992\n",
      "Test loss: 0.6322152018547058\n",
      "Epoch: 435\n",
      "Train loss: 0.640292763710022\n",
      "Test loss: 0.6321852803230286\n",
      "Epoch: 436\n",
      "Train loss: 0.6402629017829895\n",
      "Test loss: 0.6321554183959961\n",
      "Epoch: 437\n",
      "Train loss: 0.6402329802513123\n",
      "Test loss: 0.6321254968643188\n",
      "Epoch: 438\n",
      "Train loss: 0.6402031183242798\n",
      "Test loss: 0.6320956349372864\n",
      "Epoch: 439\n",
      "Train loss: 0.6401732563972473\n",
      "Test loss: 0.6320657730102539\n",
      "Epoch: 440\n",
      "Train loss: 0.6401433944702148\n",
      "Test loss: 0.6320359110832214\n",
      "Epoch: 441\n",
      "Train loss: 0.6401135325431824\n",
      "Test loss: 0.632006049156189\n",
      "Epoch: 442\n",
      "Train loss: 0.6400837302207947\n",
      "Test loss: 0.6319762468338013\n",
      "Epoch: 443\n",
      "Train loss: 0.6400538682937622\n",
      "Test loss: 0.6319463849067688\n",
      "Epoch: 444\n",
      "Train loss: 0.6400240063667297\n",
      "Test loss: 0.6319165229797363\n",
      "Epoch: 445\n",
      "Train loss: 0.6399941444396973\n",
      "Test loss: 0.6318867206573486\n",
      "Epoch: 446\n",
      "Train loss: 0.6399643421173096\n",
      "Test loss: 0.6318568587303162\n",
      "Epoch: 447\n",
      "Train loss: 0.6399345397949219\n",
      "Test loss: 0.6318269968032837\n",
      "Epoch: 448\n",
      "Train loss: 0.6399046778678894\n",
      "Test loss: 0.631797194480896\n",
      "Epoch: 449\n",
      "Train loss: 0.6398749351501465\n",
      "Test loss: 0.6317673921585083\n",
      "Epoch: 450\n",
      "Train loss: 0.6398451328277588\n",
      "Test loss: 0.6317375898361206\n",
      "Epoch: 451\n",
      "Train loss: 0.6398153305053711\n",
      "Test loss: 0.6317078471183777\n",
      "Epoch: 452\n",
      "Train loss: 0.6397855281829834\n",
      "Test loss: 0.63167804479599\n",
      "Epoch: 453\n",
      "Train loss: 0.6397557258605957\n",
      "Test loss: 0.6316482424736023\n",
      "Epoch: 454\n",
      "Train loss: 0.639725923538208\n",
      "Test loss: 0.6316184997558594\n",
      "Epoch: 455\n",
      "Train loss: 0.6396961212158203\n",
      "Test loss: 0.6315886974334717\n",
      "Epoch: 456\n",
      "Train loss: 0.6396664381027222\n",
      "Test loss: 0.631558895111084\n",
      "Epoch: 457\n",
      "Train loss: 0.6396366357803345\n",
      "Test loss: 0.6315292119979858\n",
      "Epoch: 458\n",
      "Train loss: 0.6396068930625916\n",
      "Test loss: 0.6314994692802429\n",
      "Epoch: 459\n",
      "Train loss: 0.6395771503448486\n",
      "Test loss: 0.6314697265625\n",
      "Epoch: 460\n",
      "Train loss: 0.6395474076271057\n",
      "Test loss: 0.6314399838447571\n",
      "Epoch: 461\n",
      "Train loss: 0.6395176649093628\n",
      "Test loss: 0.6314102411270142\n",
      "Epoch: 462\n",
      "Train loss: 0.6394879221916199\n",
      "Test loss: 0.6313804388046265\n",
      "Epoch: 463\n",
      "Train loss: 0.639458179473877\n",
      "Test loss: 0.6313507556915283\n",
      "Epoch: 464\n",
      "Train loss: 0.6394284963607788\n",
      "Test loss: 0.6313210129737854\n",
      "Epoch: 465\n",
      "Train loss: 0.6393987536430359\n",
      "Test loss: 0.6312913298606873\n",
      "Epoch: 466\n",
      "Train loss: 0.6393690705299377\n",
      "Test loss: 0.6312615871429443\n",
      "Epoch: 467\n",
      "Train loss: 0.6393393874168396\n",
      "Test loss: 0.6312319040298462\n",
      "Epoch: 468\n",
      "Train loss: 0.6393096446990967\n",
      "Test loss: 0.631202220916748\n",
      "Epoch: 469\n",
      "Train loss: 0.6392799615859985\n",
      "Test loss: 0.6311725378036499\n",
      "Epoch: 470\n",
      "Train loss: 0.6392502784729004\n",
      "Test loss: 0.6311428546905518\n",
      "Epoch: 471\n",
      "Train loss: 0.6392205953598022\n",
      "Test loss: 0.6311131119728088\n",
      "Epoch: 472\n",
      "Train loss: 0.6391909122467041\n",
      "Test loss: 0.6310834884643555\n",
      "Epoch: 473\n",
      "Train loss: 0.6391612887382507\n",
      "Test loss: 0.6310538053512573\n",
      "Epoch: 474\n",
      "Train loss: 0.6391316056251526\n",
      "Test loss: 0.631024181842804\n",
      "Epoch: 475\n",
      "Train loss: 0.6391019821166992\n",
      "Test loss: 0.6309944987297058\n",
      "Epoch: 476\n",
      "Train loss: 0.6390722990036011\n",
      "Test loss: 0.6309648752212524\n",
      "Epoch: 477\n",
      "Train loss: 0.6390426158905029\n",
      "Test loss: 0.6309351921081543\n",
      "Epoch: 478\n",
      "Train loss: 0.6390129923820496\n",
      "Test loss: 0.6309055089950562\n",
      "Epoch: 479\n",
      "Train loss: 0.6389833688735962\n",
      "Test loss: 0.6308759450912476\n",
      "Epoch: 480\n",
      "Train loss: 0.6389537453651428\n",
      "Test loss: 0.6308462619781494\n",
      "Epoch: 481\n",
      "Train loss: 0.6389241218566895\n",
      "Test loss: 0.6308166980743408\n",
      "Epoch: 482\n",
      "Train loss: 0.6388945579528809\n",
      "Test loss: 0.6307870745658875\n",
      "Epoch: 483\n",
      "Train loss: 0.6388649344444275\n",
      "Test loss: 0.6307574510574341\n",
      "Epoch: 484\n",
      "Train loss: 0.6388353109359741\n",
      "Test loss: 0.6307278871536255\n",
      "Epoch: 485\n",
      "Train loss: 0.6388057470321655\n",
      "Test loss: 0.6306982636451721\n",
      "Epoch: 486\n",
      "Train loss: 0.6387761235237122\n",
      "Test loss: 0.6306686401367188\n",
      "Epoch: 487\n",
      "Train loss: 0.6387465596199036\n",
      "Test loss: 0.6306390762329102\n",
      "Epoch: 488\n",
      "Train loss: 0.638716995716095\n",
      "Test loss: 0.6306095123291016\n",
      "Epoch: 489\n",
      "Train loss: 0.6386874318122864\n",
      "Test loss: 0.630579948425293\n",
      "Epoch: 490\n",
      "Train loss: 0.6386578679084778\n",
      "Test loss: 0.6305503845214844\n",
      "Epoch: 491\n",
      "Train loss: 0.6386283040046692\n",
      "Test loss: 0.6305208206176758\n",
      "Epoch: 492\n",
      "Train loss: 0.6385986804962158\n",
      "Test loss: 0.6304912567138672\n",
      "Epoch: 493\n",
      "Train loss: 0.638569176197052\n",
      "Test loss: 0.6304616928100586\n",
      "Epoch: 494\n",
      "Train loss: 0.6385396122932434\n",
      "Test loss: 0.6304321885108948\n",
      "Epoch: 495\n",
      "Train loss: 0.6385101079940796\n",
      "Test loss: 0.6304026246070862\n",
      "Epoch: 496\n",
      "Train loss: 0.638480544090271\n",
      "Test loss: 0.6303731203079224\n",
      "Epoch: 497\n",
      "Train loss: 0.6384510397911072\n",
      "Test loss: 0.6303435564041138\n",
      "Epoch: 498\n",
      "Train loss: 0.6384215354919434\n",
      "Test loss: 0.63031405210495\n",
      "Epoch: 499\n",
      "Train loss: 0.6383920311927795\n",
      "Test loss: 0.6302845478057861\n",
      "====================================================================================================\n",
      "Trainer: BPTrainer (Adagrad_trainer)\n",
      "Settings: {'method': 'AdagradOptimizer', 'args': {'learning_rate': 0.1}}\n",
      "====================================================================================================\n",
      "Epoch: init\n",
      "Train loss: 76.24555969238281\n",
      "Test loss: 89.11460876464844\n",
      "Epoch: 0\n",
      "Train loss: 0.6111255884170532\n",
      "Test loss: 0.6443696618080139\n",
      "Epoch: 1\n",
      "Train loss: 0.4205777943134308\n",
      "Test loss: 0.4876353144645691\n",
      "Epoch: 2\n",
      "Train loss: 0.38778769969940186\n",
      "Test loss: 0.466798335313797\n",
      "Epoch: 3\n",
      "Train loss: 0.3792545199394226\n",
      "Test loss: 0.462158203125\n",
      "Epoch: 4\n",
      "Train loss: 0.37483614683151245\n",
      "Test loss: 0.4584695100784302\n",
      "Epoch: 5\n",
      "Train loss: 0.3712449073791504\n",
      "Test loss: 0.454361230134964\n",
      "Epoch: 6\n",
      "Train loss: 0.3679254353046417\n",
      "Test loss: 0.45004576444625854\n",
      "Epoch: 7\n",
      "Train loss: 0.3647759258747101\n",
      "Test loss: 0.44573190808296204\n",
      "Epoch: 8\n",
      "Train loss: 0.36177149415016174\n",
      "Test loss: 0.4415203034877777\n",
      "Epoch: 9\n",
      "Train loss: 0.35890108346939087\n",
      "Test loss: 0.43744999170303345\n",
      "Epoch: 10\n",
      "Train loss: 0.35615628957748413\n",
      "Test loss: 0.43353185057640076\n",
      "Epoch: 11\n",
      "Train loss: 0.35352981090545654\n",
      "Test loss: 0.42976564168930054\n",
      "Epoch: 12\n",
      "Train loss: 0.35101473331451416\n",
      "Test loss: 0.4261462986469269\n",
      "Epoch: 13\n",
      "Train loss: 0.34860485792160034\n",
      "Test loss: 0.4226672053337097\n",
      "Epoch: 14\n",
      "Train loss: 0.34629446268081665\n",
      "Test loss: 0.4193215072154999\n",
      "Epoch: 15\n",
      "Train loss: 0.3440779447555542\n",
      "Test loss: 0.41610220074653625\n",
      "Epoch: 16\n",
      "Train loss: 0.34195029735565186\n",
      "Test loss: 0.413003146648407\n",
      "Epoch: 17\n",
      "Train loss: 0.3399069309234619\n",
      "Test loss: 0.41001808643341064\n",
      "Epoch: 18\n",
      "Train loss: 0.3379431962966919\n",
      "Test loss: 0.4071412980556488\n",
      "Epoch: 19\n",
      "Train loss: 0.3360552191734314\n",
      "Test loss: 0.40436744689941406\n",
      "Epoch: 20\n",
      "Train loss: 0.33423906564712524\n",
      "Test loss: 0.40169134736061096\n",
      "Epoch: 21\n",
      "Train loss: 0.33249109983444214\n",
      "Test loss: 0.3991084098815918\n",
      "Epoch: 22\n",
      "Train loss: 0.3308080732822418\n",
      "Test loss: 0.3966141641139984\n",
      "Epoch: 23\n",
      "Train loss: 0.32918664813041687\n",
      "Test loss: 0.39420443773269653\n",
      "Epoch: 24\n",
      "Train loss: 0.32762396335601807\n",
      "Test loss: 0.391875296831131\n",
      "Epoch: 25\n",
      "Train loss: 0.3261171877384186\n",
      "Test loss: 0.38962313532829285\n",
      "Epoch: 26\n",
      "Train loss: 0.3246636688709259\n",
      "Test loss: 0.3874443471431732\n",
      "Epoch: 27\n",
      "Train loss: 0.32326099276542664\n",
      "Test loss: 0.3853358030319214\n",
      "Epoch: 28\n",
      "Train loss: 0.32190680503845215\n",
      "Test loss: 0.38329434394836426\n",
      "Epoch: 29\n",
      "Train loss: 0.32059887051582336\n",
      "Test loss: 0.38131698966026306\n",
      "Epoch: 30\n",
      "Train loss: 0.31933513283729553\n",
      "Test loss: 0.3794010579586029\n",
      "Epoch: 31\n",
      "Train loss: 0.31811365485191345\n",
      "Test loss: 0.3775438964366913\n",
      "Epoch: 32\n",
      "Train loss: 0.31693267822265625\n",
      "Test loss: 0.37574315071105957\n",
      "Epoch: 33\n",
      "Train loss: 0.3157902657985687\n",
      "Test loss: 0.373996376991272\n",
      "Epoch: 34\n",
      "Train loss: 0.3146848678588867\n",
      "Test loss: 0.3723014295101166\n",
      "Epoch: 35\n",
      "Train loss: 0.31361493468284607\n",
      "Test loss: 0.3706561326980591\n",
      "Epoch: 36\n",
      "Train loss: 0.3125789761543274\n",
      "Test loss: 0.36905863881111145\n",
      "Epoch: 37\n",
      "Train loss: 0.3115755617618561\n",
      "Test loss: 0.36750689148902893\n",
      "Epoch: 38\n",
      "Train loss: 0.31060323119163513\n",
      "Test loss: 0.36599916219711304\n",
      "Epoch: 39\n",
      "Train loss: 0.30966100096702576\n",
      "Test loss: 0.36453381180763245\n",
      "Epoch: 40\n",
      "Train loss: 0.30874744057655334\n",
      "Test loss: 0.36310914158821106\n",
      "Epoch: 41\n",
      "Train loss: 0.3078615069389343\n",
      "Test loss: 0.3617236912250519\n",
      "Epoch: 42\n",
      "Train loss: 0.30700206756591797\n",
      "Test loss: 0.36037594079971313\n",
      "Epoch: 43\n",
      "Train loss: 0.3061681389808655\n",
      "Test loss: 0.35906437039375305\n",
      "Epoch: 44\n",
      "Train loss: 0.3053587079048157\n",
      "Test loss: 0.35778796672821045\n",
      "Epoch: 45\n",
      "Train loss: 0.3045729100704193\n",
      "Test loss: 0.3565451502799988\n",
      "Epoch: 46\n",
      "Train loss: 0.30380964279174805\n",
      "Test loss: 0.3553348183631897\n",
      "Epoch: 47\n",
      "Train loss: 0.30306825041770935\n",
      "Test loss: 0.3541558086872101\n",
      "Epoch: 48\n",
      "Train loss: 0.302347868680954\n",
      "Test loss: 0.35300707817077637\n",
      "Epoch: 49\n",
      "Train loss: 0.30164772272109985\n",
      "Test loss: 0.3518875241279602\n",
      "Epoch: 50\n",
      "Train loss: 0.3009670376777649\n",
      "Test loss: 0.3507961332798004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51\n",
      "Train loss: 0.30030518770217896\n",
      "Test loss: 0.3497319519519806\n",
      "Epoch: 52\n",
      "Train loss: 0.29966142773628235\n",
      "Test loss: 0.34869396686553955\n",
      "Epoch: 53\n",
      "Train loss: 0.2990351915359497\n",
      "Test loss: 0.3476814329624176\n",
      "Epoch: 54\n",
      "Train loss: 0.2984257638454437\n",
      "Test loss: 0.34669339656829834\n",
      "Epoch: 55\n",
      "Train loss: 0.2978326082229614\n",
      "Test loss: 0.34572917222976685\n",
      "Epoch: 56\n",
      "Train loss: 0.29725512862205505\n",
      "Test loss: 0.3447878956794739\n",
      "Epoch: 57\n",
      "Train loss: 0.2966928482055664\n",
      "Test loss: 0.34386879205703735\n",
      "Epoch: 58\n",
      "Train loss: 0.2961452007293701\n",
      "Test loss: 0.3429712653160095\n",
      "Epoch: 59\n",
      "Train loss: 0.29561173915863037\n",
      "Test loss: 0.3420945107936859\n",
      "Epoch: 60\n",
      "Train loss: 0.2950919270515442\n",
      "Test loss: 0.3412379026412964\n",
      "Epoch: 61\n",
      "Train loss: 0.29458534717559814\n",
      "Test loss: 0.34040090441703796\n",
      "Epoch: 62\n",
      "Train loss: 0.29409152269363403\n",
      "Test loss: 0.3395827114582062\n",
      "Epoch: 63\n",
      "Train loss: 0.2936100661754608\n",
      "Test loss: 0.3387829065322876\n",
      "Epoch: 64\n",
      "Train loss: 0.29314056038856506\n",
      "Test loss: 0.3380008339881897\n",
      "Epoch: 65\n",
      "Train loss: 0.2926827073097229\n",
      "Test loss: 0.3372361361980438\n",
      "Epoch: 66\n",
      "Train loss: 0.29223597049713135\n",
      "Test loss: 0.33648809790611267\n",
      "Epoch: 67\n",
      "Train loss: 0.2918001115322113\n",
      "Test loss: 0.3357563018798828\n",
      "Epoch: 68\n",
      "Train loss: 0.29137474298477173\n",
      "Test loss: 0.3350401818752289\n",
      "Epoch: 69\n",
      "Train loss: 0.29095959663391113\n",
      "Test loss: 0.3343393802642822\n",
      "Epoch: 70\n",
      "Train loss: 0.2905542850494385\n",
      "Test loss: 0.33365342020988464\n",
      "Epoch: 71\n",
      "Train loss: 0.29015854001045227\n",
      "Test loss: 0.3329819142818451\n",
      "Epoch: 72\n",
      "Train loss: 0.28977203369140625\n",
      "Test loss: 0.332324355840683\n",
      "Epoch: 73\n",
      "Train loss: 0.2893945276737213\n",
      "Test loss: 0.3316805064678192\n",
      "Epoch: 74\n",
      "Train loss: 0.2890256941318512\n",
      "Test loss: 0.33104977011680603\n",
      "Epoch: 75\n",
      "Train loss: 0.2886653542518616\n",
      "Test loss: 0.33043190836906433\n",
      "Epoch: 76\n",
      "Train loss: 0.2883131504058838\n",
      "Test loss: 0.3298265039920807\n",
      "Epoch: 77\n",
      "Train loss: 0.2879689633846283\n",
      "Test loss: 0.3292332887649536\n",
      "Epoch: 78\n",
      "Train loss: 0.2876324951648712\n",
      "Test loss: 0.32865187525749207\n",
      "Epoch: 79\n",
      "Train loss: 0.28730347752571106\n",
      "Test loss: 0.3280819356441498\n",
      "Epoch: 80\n",
      "Train loss: 0.2869817912578583\n",
      "Test loss: 0.32752326130867004\n",
      "Epoch: 81\n",
      "Train loss: 0.2866670489311218\n",
      "Test loss: 0.32697537541389465\n",
      "Epoch: 82\n",
      "Train loss: 0.2863592505455017\n",
      "Test loss: 0.32643815875053406\n",
      "Epoch: 83\n",
      "Train loss: 0.28605809807777405\n",
      "Test loss: 0.32591119408607483\n",
      "Epoch: 84\n",
      "Train loss: 0.2857634127140045\n",
      "Test loss: 0.32539430260658264\n",
      "Epoch: 85\n",
      "Train loss: 0.2854750454425812\n",
      "Test loss: 0.32488715648651123\n",
      "Epoch: 86\n",
      "Train loss: 0.2851927876472473\n",
      "Test loss: 0.3243895471096039\n",
      "Epoch: 87\n",
      "Train loss: 0.284916490316391\n",
      "Test loss: 0.3239012658596039\n",
      "Epoch: 88\n",
      "Train loss: 0.2846459448337555\n",
      "Test loss: 0.32342204451560974\n",
      "Epoch: 89\n",
      "Train loss: 0.2843810021877289\n",
      "Test loss: 0.3229515850543976\n",
      "Epoch: 90\n",
      "Train loss: 0.2841215431690216\n",
      "Test loss: 0.32248973846435547\n",
      "Epoch: 91\n",
      "Train loss: 0.2838674485683441\n",
      "Test loss: 0.3220362663269043\n",
      "Epoch: 92\n",
      "Train loss: 0.2836184799671173\n",
      "Test loss: 0.32159096002578735\n",
      "Epoch: 93\n",
      "Train loss: 0.2833745777606964\n",
      "Test loss: 0.3211536407470703\n",
      "Epoch: 94\n",
      "Train loss: 0.2831355631351471\n",
      "Test loss: 0.3207240700721741\n",
      "Epoch: 95\n",
      "Train loss: 0.2829012870788574\n",
      "Test loss: 0.3203020691871643\n",
      "Epoch: 96\n",
      "Train loss: 0.2826717495918274\n",
      "Test loss: 0.31988751888275146\n",
      "Epoch: 97\n",
      "Train loss: 0.28244662284851074\n",
      "Test loss: 0.31948018074035645\n",
      "Epoch: 98\n",
      "Train loss: 0.28222599625587463\n",
      "Test loss: 0.31907984614372253\n",
      "Epoch: 99\n",
      "Train loss: 0.28200963139533997\n",
      "Test loss: 0.31868645548820496\n",
      "Epoch: 100\n",
      "Train loss: 0.2817974090576172\n",
      "Test loss: 0.3182997703552246\n",
      "Epoch: 101\n",
      "Train loss: 0.2815892696380615\n",
      "Test loss: 0.31791961193084717\n",
      "Epoch: 102\n",
      "Train loss: 0.2813851535320282\n",
      "Test loss: 0.31754589080810547\n",
      "Epoch: 103\n",
      "Train loss: 0.2811848223209381\n",
      "Test loss: 0.3171784281730652\n",
      "Epoch: 104\n",
      "Train loss: 0.2809883654117584\n",
      "Test loss: 0.316817045211792\n",
      "Epoch: 105\n",
      "Train loss: 0.28079551458358765\n",
      "Test loss: 0.3164616823196411\n",
      "Epoch: 106\n",
      "Train loss: 0.280606210231781\n",
      "Test loss: 0.31611213088035583\n",
      "Epoch: 107\n",
      "Train loss: 0.2804204225540161\n",
      "Test loss: 0.315768301486969\n",
      "Epoch: 108\n",
      "Train loss: 0.2802380621433258\n",
      "Test loss: 0.31543004512786865\n",
      "Epoch: 109\n",
      "Train loss: 0.2800590395927429\n",
      "Test loss: 0.31509724259376526\n",
      "Epoch: 110\n",
      "Train loss: 0.2798832356929779\n",
      "Test loss: 0.3147697448730469\n",
      "Epoch: 111\n",
      "Train loss: 0.279710590839386\n",
      "Test loss: 0.3144474923610687\n",
      "Epoch: 112\n",
      "Train loss: 0.2795410752296448\n",
      "Test loss: 0.31413033604621887\n",
      "Epoch: 113\n",
      "Train loss: 0.27937448024749756\n",
      "Test loss: 0.3138181269168854\n",
      "Epoch: 114\n",
      "Train loss: 0.2792109251022339\n",
      "Test loss: 0.31351083517074585\n",
      "Epoch: 115\n",
      "Train loss: 0.27905014157295227\n",
      "Test loss: 0.31320828199386597\n",
      "Epoch: 116\n",
      "Train loss: 0.2788921892642975\n",
      "Test loss: 0.31291040778160095\n",
      "Epoch: 117\n",
      "Train loss: 0.27873697876930237\n",
      "Test loss: 0.31261715292930603\n",
      "Epoch: 118\n",
      "Train loss: 0.27858445048332214\n",
      "Test loss: 0.3123283088207245\n",
      "Epoch: 119\n",
      "Train loss: 0.27843451499938965\n",
      "Test loss: 0.31204384565353394\n",
      "Epoch: 120\n",
      "Train loss: 0.27828705310821533\n",
      "Test loss: 0.3117637038230896\n",
      "Epoch: 121\n",
      "Train loss: 0.27814218401908875\n",
      "Test loss: 0.31148776412010193\n",
      "Epoch: 122\n",
      "Train loss: 0.2779996395111084\n",
      "Test loss: 0.311215877532959\n",
      "Epoch: 123\n",
      "Train loss: 0.27785950899124146\n",
      "Test loss: 0.3109480142593384\n",
      "Epoch: 124\n",
      "Train loss: 0.27772170305252075\n",
      "Test loss: 0.3106841444969177\n",
      "Epoch: 125\n",
      "Train loss: 0.2775862216949463\n",
      "Test loss: 0.31042414903640747\n",
      "Epoch: 126\n",
      "Train loss: 0.27745291590690613\n",
      "Test loss: 0.31016790866851807\n",
      "Epoch: 127\n",
      "Train loss: 0.2773216962814331\n",
      "Test loss: 0.30991536378860474\n",
      "Epoch: 128\n",
      "Train loss: 0.277192622423172\n",
      "Test loss: 0.30966639518737793\n",
      "Epoch: 129\n",
      "Train loss: 0.277065634727478\n",
      "Test loss: 0.30942103266716003\n",
      "Epoch: 130\n",
      "Train loss: 0.2769406735897064\n",
      "Test loss: 0.3091791570186615\n",
      "Epoch: 131\n",
      "Train loss: 0.27681764960289\n",
      "Test loss: 0.30894067883491516\n",
      "Epoch: 132\n",
      "Train loss: 0.2766965627670288\n",
      "Test loss: 0.30870556831359863\n",
      "Epoch: 133\n",
      "Train loss: 0.2765773832798004\n",
      "Test loss: 0.30847370624542236\n",
      "Epoch: 134\n",
      "Train loss: 0.27646002173423767\n",
      "Test loss: 0.30824506282806396\n",
      "Epoch: 135\n",
      "Train loss: 0.2763444483280182\n",
      "Test loss: 0.30801957845687866\n",
      "Epoch: 136\n",
      "Train loss: 0.27623072266578674\n",
      "Test loss: 0.3077971935272217\n",
      "Epoch: 137\n",
      "Train loss: 0.2761186361312866\n",
      "Test loss: 0.30757787823677063\n",
      "Epoch: 138\n",
      "Train loss: 0.276008278131485\n",
      "Test loss: 0.3073614835739136\n",
      "Epoch: 139\n",
      "Train loss: 0.27589958906173706\n",
      "Test loss: 0.3071480393409729\n",
      "Epoch: 140\n",
      "Train loss: 0.2757924795150757\n",
      "Test loss: 0.30693739652633667\n",
      "Epoch: 141\n",
      "Train loss: 0.27568697929382324\n",
      "Test loss: 0.30672967433929443\n",
      "Epoch: 142\n",
      "Train loss: 0.2755829989910126\n",
      "Test loss: 0.3065246343612671\n",
      "Epoch: 143\n",
      "Train loss: 0.27548056840896606\n",
      "Test loss: 0.3063223361968994\n",
      "Epoch: 144\n",
      "Train loss: 0.27537962794303894\n",
      "Test loss: 0.30612269043922424\n",
      "Epoch: 145\n",
      "Train loss: 0.27528005838394165\n",
      "Test loss: 0.305925577878952\n",
      "Epoch: 146\n",
      "Train loss: 0.27518197894096375\n",
      "Test loss: 0.3057311177253723\n",
      "Epoch: 147\n",
      "Train loss: 0.27508530020713806\n",
      "Test loss: 0.3055391013622284\n",
      "Epoch: 148\n",
      "Train loss: 0.27498993277549744\n",
      "Test loss: 0.30534955859184265\n",
      "Epoch: 149\n",
      "Train loss: 0.27489593625068665\n",
      "Test loss: 0.3051624000072479\n",
      "Epoch: 150\n",
      "Train loss: 0.2748032212257385\n",
      "Test loss: 0.3049776554107666\n",
      "Epoch: 151\n",
      "Train loss: 0.27471181750297546\n",
      "Test loss: 0.3047952950000763\n",
      "Epoch: 152\n",
      "Train loss: 0.2746216654777527\n",
      "Test loss: 0.3046150803565979\n",
      "Epoch: 153\n",
      "Train loss: 0.2745327353477478\n",
      "Test loss: 0.30443722009658813\n",
      "Epoch: 154\n",
      "Train loss: 0.27444496750831604\n",
      "Test loss: 0.30426156520843506\n",
      "Epoch: 155\n",
      "Train loss: 0.27435845136642456\n",
      "Test loss: 0.3040880560874939\n",
      "Epoch: 156\n",
      "Train loss: 0.2742730379104614\n",
      "Test loss: 0.30391669273376465\n",
      "Epoch: 157\n",
      "Train loss: 0.27418872714042664\n",
      "Test loss: 0.30374735593795776\n",
      "Epoch: 158\n",
      "Train loss: 0.27410557866096497\n",
      "Test loss: 0.3035801947116852\n",
      "Epoch: 159\n",
      "Train loss: 0.27402350306510925\n",
      "Test loss: 0.303414911031723\n",
      "Epoch: 160\n",
      "Train loss: 0.2739425003528595\n",
      "Test loss: 0.303251713514328\n",
      "Epoch: 161\n",
      "Train loss: 0.27386248111724854\n",
      "Test loss: 0.3030904233455658\n",
      "Epoch: 162\n",
      "Train loss: 0.2737835645675659\n",
      "Test loss: 0.3029310703277588\n",
      "Epoch: 163\n",
      "Train loss: 0.2737056016921997\n",
      "Test loss: 0.3027736246585846\n",
      "Epoch: 164\n",
      "Train loss: 0.2736286520957947\n",
      "Test loss: 0.30261799693107605\n",
      "Epoch: 165\n",
      "Train loss: 0.27355262637138367\n",
      "Test loss: 0.30246415734291077\n",
      "Epoch: 166\n",
      "Train loss: 0.27347758412361145\n",
      "Test loss: 0.3023121953010559\n",
      "Epoch: 167\n",
      "Train loss: 0.2734034061431885\n",
      "Test loss: 0.30216190218925476\n",
      "Epoch: 168\n",
      "Train loss: 0.2733302116394043\n",
      "Test loss: 0.3020133674144745\n",
      "Epoch: 169\n",
      "Train loss: 0.273257851600647\n",
      "Test loss: 0.3018665611743927\n",
      "Epoch: 170\n",
      "Train loss: 0.27318644523620605\n",
      "Test loss: 0.301721453666687\n",
      "Epoch: 171\n",
      "Train loss: 0.2731158137321472\n",
      "Test loss: 0.30157792568206787\n",
      "Epoch: 172\n",
      "Train loss: 0.27304607629776\n",
      "Test loss: 0.30143603682518005\n",
      "Epoch: 173\n",
      "Train loss: 0.2729770839214325\n",
      "Test loss: 0.30129575729370117\n",
      "Epoch: 174\n",
      "Train loss: 0.2729089856147766\n",
      "Test loss: 0.30115702748298645\n",
      "Epoch: 175\n",
      "Train loss: 0.2728416621685028\n",
      "Test loss: 0.3010198473930359\n",
      "Epoch: 176\n",
      "Train loss: 0.2727750837802887\n",
      "Test loss: 0.3008841574192047\n",
      "Epoch: 177\n",
      "Train loss: 0.2727092504501343\n",
      "Test loss: 0.3007499873638153\n",
      "Epoch: 178\n",
      "Train loss: 0.2726442217826843\n",
      "Test loss: 0.3006172478199005\n",
      "Epoch: 179\n",
      "Train loss: 0.2725798785686493\n",
      "Test loss: 0.3004859983921051\n",
      "Epoch: 180\n",
      "Train loss: 0.27251631021499634\n",
      "Test loss: 0.3003561496734619\n",
      "Epoch: 181\n",
      "Train loss: 0.2724533975124359\n",
      "Test loss: 0.30022767186164856\n",
      "Epoch: 182\n",
      "Train loss: 0.2723911702632904\n",
      "Test loss: 0.30010056495666504\n",
      "Epoch: 183\n",
      "Train loss: 0.2723296880722046\n",
      "Test loss: 0.29997485876083374\n",
      "Epoch: 184\n",
      "Train loss: 0.2722688317298889\n",
      "Test loss: 0.2998504638671875\n",
      "Epoch: 185\n",
      "Train loss: 0.27220866084098816\n",
      "Test loss: 0.2997273802757263\n",
      "Epoch: 186\n",
      "Train loss: 0.27214908599853516\n",
      "Test loss: 0.2996055483818054\n",
      "Epoch: 187\n",
      "Train loss: 0.2720901668071747\n",
      "Test loss: 0.2994850277900696\n",
      "Epoch: 188\n",
      "Train loss: 0.27203190326690674\n",
      "Test loss: 0.2993657886981964\n",
      "Epoch: 189\n",
      "Train loss: 0.27197426557540894\n",
      "Test loss: 0.299247682094574\n",
      "Epoch: 190\n",
      "Train loss: 0.2719171643257141\n",
      "Test loss: 0.2991308867931366\n",
      "Epoch: 191\n",
      "Train loss: 0.27186068892478943\n",
      "Test loss: 0.29901522397994995\n",
      "Epoch: 192\n",
      "Train loss: 0.2718047797679901\n",
      "Test loss: 0.2989007532596588\n",
      "Epoch: 193\n",
      "Train loss: 0.2717493772506714\n",
      "Test loss: 0.2987874746322632\n",
      "Epoch: 194\n",
      "Train loss: 0.2716946303844452\n",
      "Test loss: 0.2986752986907959\n",
      "Epoch: 195\n",
      "Train loss: 0.2716403901576996\n",
      "Test loss: 0.29856425523757935\n",
      "Epoch: 196\n",
      "Train loss: 0.27158665657043457\n",
      "Test loss: 0.2984543442726135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 197\n",
      "Train loss: 0.2715334892272949\n",
      "Test loss: 0.2983454763889313\n",
      "Epoch: 198\n",
      "Train loss: 0.27148085832595825\n",
      "Test loss: 0.29823774099349976\n",
      "Epoch: 199\n",
      "Train loss: 0.2714287042617798\n",
      "Test loss: 0.2981310188770294\n",
      "Epoch: 200\n",
      "Train loss: 0.27137699723243713\n",
      "Test loss: 0.29802533984184265\n",
      "Epoch: 201\n",
      "Train loss: 0.27132585644721985\n",
      "Test loss: 0.29792070388793945\n",
      "Epoch: 202\n",
      "Train loss: 0.27127519249916077\n",
      "Test loss: 0.29781705141067505\n",
      "Epoch: 203\n",
      "Train loss: 0.2712250053882599\n",
      "Test loss: 0.29771438241004944\n",
      "Epoch: 204\n",
      "Train loss: 0.27117523550987244\n",
      "Test loss: 0.297612726688385\n",
      "Epoch: 205\n",
      "Train loss: 0.2711259722709656\n",
      "Test loss: 0.2975120544433594\n",
      "Epoch: 206\n",
      "Train loss: 0.27107715606689453\n",
      "Test loss: 0.29741236567497253\n",
      "Epoch: 207\n",
      "Train loss: 0.2710288166999817\n",
      "Test loss: 0.2973135709762573\n",
      "Epoch: 208\n",
      "Train loss: 0.2709808647632599\n",
      "Test loss: 0.29721570014953613\n",
      "Epoch: 209\n",
      "Train loss: 0.2709333002567291\n",
      "Test loss: 0.29711875319480896\n",
      "Epoch: 210\n",
      "Train loss: 0.27088624238967896\n",
      "Test loss: 0.2970227003097534\n",
      "Epoch: 211\n",
      "Train loss: 0.27083954215049744\n",
      "Test loss: 0.2969275414943695\n",
      "Epoch: 212\n",
      "Train loss: 0.27079328894615173\n",
      "Test loss: 0.29683324694633484\n",
      "Epoch: 213\n",
      "Train loss: 0.2707473933696747\n",
      "Test loss: 0.296739786863327\n",
      "Epoch: 214\n",
      "Train loss: 0.27070191502571106\n",
      "Test loss: 0.29664725065231323\n",
      "Epoch: 215\n",
      "Train loss: 0.2706568241119385\n",
      "Test loss: 0.2965554893016815\n",
      "Epoch: 216\n",
      "Train loss: 0.27061209082603455\n",
      "Test loss: 0.29646462202072144\n",
      "Epoch: 217\n",
      "Train loss: 0.27056774497032166\n",
      "Test loss: 0.29637452960014343\n",
      "Epoch: 218\n",
      "Train loss: 0.2705238163471222\n",
      "Test loss: 0.2962852418422699\n",
      "Epoch: 219\n",
      "Train loss: 0.2704801559448242\n",
      "Test loss: 0.2961967885494232\n",
      "Epoch: 220\n",
      "Train loss: 0.2704369127750397\n",
      "Test loss: 0.29610905051231384\n",
      "Epoch: 221\n",
      "Train loss: 0.270393967628479\n",
      "Test loss: 0.29602211713790894\n",
      "Epoch: 222\n",
      "Train loss: 0.27035143971443176\n",
      "Test loss: 0.2959359288215637\n",
      "Epoch: 223\n",
      "Train loss: 0.2703092098236084\n",
      "Test loss: 0.2958505153656006\n",
      "Epoch: 224\n",
      "Train loss: 0.2702673375606537\n",
      "Test loss: 0.29576584696769714\n",
      "Epoch: 225\n",
      "Train loss: 0.27022576332092285\n",
      "Test loss: 0.2956818640232086\n",
      "Epoch: 226\n",
      "Train loss: 0.27018454670906067\n",
      "Test loss: 0.2955986559391022\n",
      "Epoch: 227\n",
      "Train loss: 0.27014362812042236\n",
      "Test loss: 0.29551613330841064\n",
      "Epoch: 228\n",
      "Train loss: 0.27010297775268555\n",
      "Test loss: 0.2954343259334564\n",
      "Epoch: 229\n",
      "Train loss: 0.270062655210495\n",
      "Test loss: 0.29535314440727234\n",
      "Epoch: 230\n",
      "Train loss: 0.2700226604938507\n",
      "Test loss: 0.2952727675437927\n",
      "Epoch: 231\n",
      "Train loss: 0.2699829638004303\n",
      "Test loss: 0.29519298672676086\n",
      "Epoch: 232\n",
      "Train loss: 0.2699435353279114\n",
      "Test loss: 0.2951138913631439\n",
      "Epoch: 233\n",
      "Train loss: 0.26990437507629395\n",
      "Test loss: 0.2950354218482971\n",
      "Epoch: 234\n",
      "Train loss: 0.2698655426502228\n",
      "Test loss: 0.29495760798454285\n",
      "Epoch: 235\n",
      "Train loss: 0.2698269784450531\n",
      "Test loss: 0.2948804795742035\n",
      "Epoch: 236\n",
      "Train loss: 0.2697886824607849\n",
      "Test loss: 0.2948039174079895\n",
      "Epoch: 237\n",
      "Train loss: 0.2697506546974182\n",
      "Test loss: 0.29472804069519043\n",
      "Epoch: 238\n",
      "Train loss: 0.269712895154953\n",
      "Test loss: 0.2946527600288391\n",
      "Epoch: 239\n",
      "Train loss: 0.2696754038333893\n",
      "Test loss: 0.29457804560661316\n",
      "Epoch: 240\n",
      "Train loss: 0.2696381211280823\n",
      "Test loss: 0.29450398683547974\n",
      "Epoch: 241\n",
      "Train loss: 0.26960116624832153\n",
      "Test loss: 0.29443055391311646\n",
      "Epoch: 242\n",
      "Train loss: 0.2695644497871399\n",
      "Test loss: 0.2943575978279114\n",
      "Epoch: 243\n",
      "Train loss: 0.26952797174453735\n",
      "Test loss: 0.29428526759147644\n",
      "Epoch: 244\n",
      "Train loss: 0.26949167251586914\n",
      "Test loss: 0.29421353340148926\n",
      "Epoch: 245\n",
      "Train loss: 0.2694556713104248\n",
      "Test loss: 0.29414230585098267\n",
      "Epoch: 246\n",
      "Train loss: 0.26941990852355957\n",
      "Test loss: 0.29407164454460144\n",
      "Epoch: 247\n",
      "Train loss: 0.26938438415527344\n",
      "Test loss: 0.29400157928466797\n",
      "Epoch: 248\n",
      "Train loss: 0.269349068403244\n",
      "Test loss: 0.2939320206642151\n",
      "Epoch: 249\n",
      "Train loss: 0.2693139612674713\n",
      "Test loss: 0.2938629984855652\n",
      "Epoch: 250\n",
      "Train loss: 0.2692790925502777\n",
      "Test loss: 0.29379454255104065\n",
      "Epoch: 251\n",
      "Train loss: 0.2692444324493408\n",
      "Test loss: 0.2937265932559967\n",
      "Epoch: 252\n",
      "Train loss: 0.26921001076698303\n",
      "Test loss: 0.29365915060043335\n",
      "Epoch: 253\n",
      "Train loss: 0.26917579770088196\n",
      "Test loss: 0.2935921847820282\n",
      "Epoch: 254\n",
      "Train loss: 0.2691417634487152\n",
      "Test loss: 0.2935257852077484\n",
      "Epoch: 255\n",
      "Train loss: 0.2691079378128052\n",
      "Test loss: 0.29345980286598206\n",
      "Epoch: 256\n",
      "Train loss: 0.26907435059547424\n",
      "Test loss: 0.2933943569660187\n",
      "Epoch: 257\n",
      "Train loss: 0.26904091238975525\n",
      "Test loss: 0.2933294475078583\n",
      "Epoch: 258\n",
      "Train loss: 0.26900768280029297\n",
      "Test loss: 0.29326489567756653\n",
      "Epoch: 259\n",
      "Train loss: 0.2689746618270874\n",
      "Test loss: 0.29320091009140015\n",
      "Epoch: 260\n",
      "Train loss: 0.26894184947013855\n",
      "Test loss: 0.29313740134239197\n",
      "Epoch: 261\n",
      "Train loss: 0.268909215927124\n",
      "Test loss: 0.29307425022125244\n",
      "Epoch: 262\n",
      "Train loss: 0.26887673139572144\n",
      "Test loss: 0.2930116355419159\n",
      "Epoch: 263\n",
      "Train loss: 0.26884445548057556\n",
      "Test loss: 0.29294946789741516\n",
      "Epoch: 264\n",
      "Train loss: 0.268812358379364\n",
      "Test loss: 0.29288774728775024\n",
      "Epoch: 265\n",
      "Train loss: 0.2687804400920868\n",
      "Test loss: 0.29282641410827637\n",
      "Epoch: 266\n",
      "Train loss: 0.2687487006187439\n",
      "Test loss: 0.2927655875682831\n",
      "Epoch: 267\n",
      "Train loss: 0.26871708035469055\n",
      "Test loss: 0.2927051782608032\n",
      "Epoch: 268\n",
      "Train loss: 0.2686856985092163\n",
      "Test loss: 0.2926451861858368\n",
      "Epoch: 269\n",
      "Train loss: 0.2686544358730316\n",
      "Test loss: 0.2925856113433838\n",
      "Epoch: 270\n",
      "Train loss: 0.26862338185310364\n",
      "Test loss: 0.2925264239311218\n",
      "Epoch: 271\n",
      "Train loss: 0.2685924768447876\n",
      "Test loss: 0.2924676537513733\n",
      "Epoch: 272\n",
      "Train loss: 0.2685617506504059\n",
      "Test loss: 0.29240933060646057\n",
      "Epoch: 273\n",
      "Train loss: 0.26853111386299133\n",
      "Test loss: 0.2923513650894165\n",
      "Epoch: 274\n",
      "Train loss: 0.2685006856918335\n",
      "Test loss: 0.29229381680488586\n",
      "Epoch: 275\n",
      "Train loss: 0.2684704065322876\n",
      "Test loss: 0.2922366261482239\n",
      "Epoch: 276\n",
      "Train loss: 0.26844024658203125\n",
      "Test loss: 0.2921799123287201\n",
      "Epoch: 277\n",
      "Train loss: 0.2684102952480316\n",
      "Test loss: 0.2921234965324402\n",
      "Epoch: 278\n",
      "Train loss: 0.26838046312332153\n",
      "Test loss: 0.2920674979686737\n",
      "Epoch: 279\n",
      "Train loss: 0.268350750207901\n",
      "Test loss: 0.29201188683509827\n",
      "Epoch: 280\n",
      "Train loss: 0.2683212161064148\n",
      "Test loss: 0.2919566035270691\n",
      "Epoch: 281\n",
      "Train loss: 0.26829180121421814\n",
      "Test loss: 0.29190167784690857\n",
      "Epoch: 282\n",
      "Train loss: 0.2682625651359558\n",
      "Test loss: 0.2918471395969391\n",
      "Epoch: 283\n",
      "Train loss: 0.26823341846466064\n",
      "Test loss: 0.29179298877716064\n",
      "Epoch: 284\n",
      "Train loss: 0.2682044208049774\n",
      "Test loss: 0.2917391359806061\n",
      "Epoch: 285\n",
      "Train loss: 0.26817557215690613\n",
      "Test loss: 0.29168564081192017\n",
      "Epoch: 286\n",
      "Train loss: 0.2681468725204468\n",
      "Test loss: 0.29163259267807007\n",
      "Epoch: 287\n",
      "Train loss: 0.2681182622909546\n",
      "Test loss: 0.29157981276512146\n",
      "Epoch: 288\n",
      "Train loss: 0.26808977127075195\n",
      "Test loss: 0.29152733087539673\n",
      "Epoch: 289\n",
      "Train loss: 0.26806148886680603\n",
      "Test loss: 0.29147517681121826\n",
      "Epoch: 290\n",
      "Train loss: 0.2680332660675049\n",
      "Test loss: 0.2914234399795532\n",
      "Epoch: 291\n",
      "Train loss: 0.2680051922798157\n",
      "Test loss: 0.29137200117111206\n",
      "Epoch: 292\n",
      "Train loss: 0.26797720789909363\n",
      "Test loss: 0.2913208603858948\n",
      "Epoch: 293\n",
      "Train loss: 0.2679493725299835\n",
      "Test loss: 0.29127004742622375\n",
      "Epoch: 294\n",
      "Train loss: 0.26792165637016296\n",
      "Test loss: 0.2912195324897766\n",
      "Epoch: 295\n",
      "Train loss: 0.26789408922195435\n",
      "Test loss: 0.2911694049835205\n",
      "Epoch: 296\n",
      "Train loss: 0.2678665518760681\n",
      "Test loss: 0.2911195158958435\n",
      "Epoch: 297\n",
      "Train loss: 0.2678391635417938\n",
      "Test loss: 0.2910699248313904\n",
      "Epoch: 298\n",
      "Train loss: 0.26781192421913147\n",
      "Test loss: 0.2910206913948059\n",
      "Epoch: 299\n",
      "Train loss: 0.2677847743034363\n",
      "Test loss: 0.2909717559814453\n",
      "Epoch: 300\n",
      "Train loss: 0.26775771379470825\n",
      "Test loss: 0.2909231185913086\n",
      "Epoch: 301\n",
      "Train loss: 0.26773080229759216\n",
      "Test loss: 0.290874719619751\n",
      "Epoch: 302\n",
      "Train loss: 0.26770398020744324\n",
      "Test loss: 0.290826678276062\n",
      "Epoch: 303\n",
      "Train loss: 0.26767727732658386\n",
      "Test loss: 0.29077890515327454\n",
      "Epoch: 304\n",
      "Train loss: 0.26765066385269165\n",
      "Test loss: 0.29073140025138855\n",
      "Epoch: 305\n",
      "Train loss: 0.2676241397857666\n",
      "Test loss: 0.29068416357040405\n",
      "Epoch: 306\n",
      "Train loss: 0.2675977051258087\n",
      "Test loss: 0.29063722491264343\n",
      "Epoch: 307\n",
      "Train loss: 0.26757141947746277\n",
      "Test loss: 0.2905905842781067\n",
      "Epoch: 308\n",
      "Train loss: 0.267545223236084\n",
      "Test loss: 0.29054421186447144\n",
      "Epoch: 309\n",
      "Train loss: 0.26751914620399475\n",
      "Test loss: 0.29049810767173767\n",
      "Epoch: 310\n",
      "Train loss: 0.2674930989742279\n",
      "Test loss: 0.2904522716999054\n",
      "Epoch: 311\n",
      "Train loss: 0.267467200756073\n",
      "Test loss: 0.29040661454200745\n",
      "Epoch: 312\n",
      "Train loss: 0.26744142174720764\n",
      "Test loss: 0.29036134481430054\n",
      "Epoch: 313\n",
      "Train loss: 0.2674156725406647\n",
      "Test loss: 0.29031631350517273\n",
      "Epoch: 314\n",
      "Train loss: 0.26739004254341125\n",
      "Test loss: 0.29027149081230164\n",
      "Epoch: 315\n",
      "Train loss: 0.2673645615577698\n",
      "Test loss: 0.2902269959449768\n",
      "Epoch: 316\n",
      "Train loss: 0.2673390507698059\n",
      "Test loss: 0.2901826500892639\n",
      "Epoch: 317\n",
      "Train loss: 0.26731371879577637\n",
      "Test loss: 0.2901386320590973\n",
      "Epoch: 318\n",
      "Train loss: 0.2672884464263916\n",
      "Test loss: 0.2900948226451874\n",
      "Epoch: 319\n",
      "Train loss: 0.2672632932662964\n",
      "Test loss: 0.29005131125450134\n",
      "Epoch: 320\n",
      "Train loss: 0.26723819971084595\n",
      "Test loss: 0.29000797867774963\n",
      "Epoch: 321\n",
      "Train loss: 0.2672131657600403\n",
      "Test loss: 0.289964884519577\n",
      "Epoch: 322\n",
      "Train loss: 0.26718825101852417\n",
      "Test loss: 0.2899220585823059\n",
      "Epoch: 323\n",
      "Train loss: 0.2671634256839752\n",
      "Test loss: 0.2898794710636139\n",
      "Epoch: 324\n",
      "Train loss: 0.2671387195587158\n",
      "Test loss: 0.28983718156814575\n",
      "Epoch: 325\n",
      "Train loss: 0.2671140134334564\n",
      "Test loss: 0.2897949814796448\n",
      "Epoch: 326\n",
      "Train loss: 0.26708945631980896\n",
      "Test loss: 0.28975310921669006\n",
      "Epoch: 327\n",
      "Train loss: 0.2670648992061615\n",
      "Test loss: 0.2897113859653473\n",
      "Epoch: 328\n",
      "Train loss: 0.2670404613018036\n",
      "Test loss: 0.2896699905395508\n",
      "Epoch: 329\n",
      "Train loss: 0.26701614260673523\n",
      "Test loss: 0.2896287143230438\n",
      "Epoch: 330\n",
      "Train loss: 0.26699188351631165\n",
      "Test loss: 0.28958776593208313\n",
      "Epoch: 331\n",
      "Train loss: 0.26696768403053284\n",
      "Test loss: 0.2895469665527344\n",
      "Epoch: 332\n",
      "Train loss: 0.2669435739517212\n",
      "Test loss: 0.28950634598731995\n",
      "Epoch: 333\n",
      "Train loss: 0.26691949367523193\n",
      "Test loss: 0.2894660234451294\n",
      "Epoch: 334\n",
      "Train loss: 0.2668955326080322\n",
      "Test loss: 0.2894258499145508\n",
      "Epoch: 335\n",
      "Train loss: 0.2668716311454773\n",
      "Test loss: 0.28938591480255127\n",
      "Epoch: 336\n",
      "Train loss: 0.2668478488922119\n",
      "Test loss: 0.28934621810913086\n",
      "Epoch: 337\n",
      "Train loss: 0.2668240964412689\n",
      "Test loss: 0.2893066704273224\n",
      "Epoch: 338\n",
      "Train loss: 0.2668004035949707\n",
      "Test loss: 0.289267361164093\n",
      "Epoch: 339\n",
      "Train loss: 0.26677680015563965\n",
      "Test loss: 0.28922826051712036\n",
      "Epoch: 340\n",
      "Train loss: 0.26675328612327576\n",
      "Test loss: 0.28918930888175964\n",
      "Epoch: 341\n",
      "Train loss: 0.26672980189323425\n",
      "Test loss: 0.289150595664978\n",
      "Epoch: 342\n",
      "Train loss: 0.2667064070701599\n",
      "Test loss: 0.2891120910644531\n",
      "Epoch: 343\n",
      "Train loss: 0.26668304204940796\n",
      "Test loss: 0.2890737056732178\n",
      "Epoch: 344\n",
      "Train loss: 0.26665979623794556\n",
      "Test loss: 0.2890355587005615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 345\n",
      "Train loss: 0.26663658022880554\n",
      "Test loss: 0.288997620344162\n",
      "Epoch: 346\n",
      "Train loss: 0.2666134834289551\n",
      "Test loss: 0.28895989060401917\n",
      "Epoch: 347\n",
      "Train loss: 0.266590416431427\n",
      "Test loss: 0.28892233967781067\n",
      "Epoch: 348\n",
      "Train loss: 0.2665673792362213\n",
      "Test loss: 0.28888487815856934\n",
      "Epoch: 349\n",
      "Train loss: 0.2665444612503052\n",
      "Test loss: 0.2888476550579071\n",
      "Epoch: 350\n",
      "Train loss: 0.2665216028690338\n",
      "Test loss: 0.288810670375824\n",
      "Epoch: 351\n",
      "Train loss: 0.26649874448776245\n",
      "Test loss: 0.2887738347053528\n",
      "Epoch: 352\n",
      "Train loss: 0.26647600531578064\n",
      "Test loss: 0.2887371778488159\n",
      "Epoch: 353\n",
      "Train loss: 0.2664533257484436\n",
      "Test loss: 0.2887006998062134\n",
      "Epoch: 354\n",
      "Train loss: 0.26643064618110657\n",
      "Test loss: 0.28866440057754517\n",
      "Epoch: 355\n",
      "Train loss: 0.26640811562538147\n",
      "Test loss: 0.2886282503604889\n",
      "Epoch: 356\n",
      "Train loss: 0.26638561487197876\n",
      "Test loss: 0.28859224915504456\n",
      "Epoch: 357\n",
      "Train loss: 0.2663631737232208\n",
      "Test loss: 0.2885564863681793\n",
      "Epoch: 358\n",
      "Train loss: 0.26634079217910767\n",
      "Test loss: 0.288520872592926\n",
      "Epoch: 359\n",
      "Train loss: 0.2663183808326721\n",
      "Test loss: 0.28848543763160706\n",
      "Epoch: 360\n",
      "Train loss: 0.2662961483001709\n",
      "Test loss: 0.2884501516819\n",
      "Epoch: 361\n",
      "Train loss: 0.2662739157676697\n",
      "Test loss: 0.28841501474380493\n",
      "Epoch: 362\n",
      "Train loss: 0.26625174283981323\n",
      "Test loss: 0.2883800268173218\n",
      "Epoch: 363\n",
      "Train loss: 0.26622965931892395\n",
      "Test loss: 0.28834524750709534\n",
      "Epoch: 364\n",
      "Train loss: 0.26620757579803467\n",
      "Test loss: 0.28831061720848083\n",
      "Epoch: 365\n",
      "Train loss: 0.26618561148643494\n",
      "Test loss: 0.2882761061191559\n",
      "Epoch: 366\n",
      "Train loss: 0.2661636769771576\n",
      "Test loss: 0.28824180364608765\n",
      "Epoch: 367\n",
      "Train loss: 0.266141802072525\n",
      "Test loss: 0.28820767998695374\n",
      "Epoch: 368\n",
      "Train loss: 0.26611998677253723\n",
      "Test loss: 0.2881736755371094\n",
      "Epoch: 369\n",
      "Train loss: 0.26609814167022705\n",
      "Test loss: 0.28813982009887695\n",
      "Epoch: 370\n",
      "Train loss: 0.2660764455795288\n",
      "Test loss: 0.28810614347457886\n",
      "Epoch: 371\n",
      "Train loss: 0.26605474948883057\n",
      "Test loss: 0.2880725562572479\n",
      "Epoch: 372\n",
      "Train loss: 0.2660331130027771\n",
      "Test loss: 0.2880391478538513\n",
      "Epoch: 373\n",
      "Train loss: 0.2660115659236908\n",
      "Test loss: 0.2880059778690338\n",
      "Epoch: 374\n",
      "Train loss: 0.2659900486469269\n",
      "Test loss: 0.2879728078842163\n",
      "Epoch: 375\n",
      "Train loss: 0.26596856117248535\n",
      "Test loss: 0.2879399061203003\n",
      "Epoch: 376\n",
      "Train loss: 0.2659471333026886\n",
      "Test loss: 0.28790712356567383\n",
      "Epoch: 377\n",
      "Train loss: 0.265925794839859\n",
      "Test loss: 0.2878744602203369\n",
      "Epoch: 378\n",
      "Train loss: 0.26590442657470703\n",
      "Test loss: 0.28784191608428955\n",
      "Epoch: 379\n",
      "Train loss: 0.2658831477165222\n",
      "Test loss: 0.2878095209598541\n",
      "Epoch: 380\n",
      "Train loss: 0.2658618986606598\n",
      "Test loss: 0.28777727484703064\n",
      "Epoch: 381\n",
      "Train loss: 0.2658407688140869\n",
      "Test loss: 0.2877452075481415\n",
      "Epoch: 382\n",
      "Train loss: 0.26581960916519165\n",
      "Test loss: 0.2877132296562195\n",
      "Epoch: 383\n",
      "Train loss: 0.26579856872558594\n",
      "Test loss: 0.2876814305782318\n",
      "Epoch: 384\n",
      "Train loss: 0.26577749848365784\n",
      "Test loss: 0.2876497209072113\n",
      "Epoch: 385\n",
      "Train loss: 0.2657565176486969\n",
      "Test loss: 0.28761816024780273\n",
      "Epoch: 386\n",
      "Train loss: 0.26573559641838074\n",
      "Test loss: 0.2875867486000061\n",
      "Epoch: 387\n",
      "Train loss: 0.2657146453857422\n",
      "Test loss: 0.287555456161499\n",
      "Epoch: 388\n",
      "Train loss: 0.2656938135623932\n",
      "Test loss: 0.2875242829322815\n",
      "Epoch: 389\n",
      "Train loss: 0.2656730115413666\n",
      "Test loss: 0.2874932587146759\n",
      "Epoch: 390\n",
      "Train loss: 0.26565226912498474\n",
      "Test loss: 0.28746238350868225\n",
      "Epoch: 391\n",
      "Train loss: 0.2656315565109253\n",
      "Test loss: 0.28743162751197815\n",
      "Epoch: 392\n",
      "Train loss: 0.26561087369918823\n",
      "Test loss: 0.2874009907245636\n",
      "Epoch: 393\n",
      "Train loss: 0.26559025049209595\n",
      "Test loss: 0.2873704731464386\n",
      "Epoch: 394\n",
      "Train loss: 0.26556965708732605\n",
      "Test loss: 0.28734007477760315\n",
      "Epoch: 395\n",
      "Train loss: 0.2655491232872009\n",
      "Test loss: 0.28730982542037964\n",
      "Epoch: 396\n",
      "Train loss: 0.2655285894870758\n",
      "Test loss: 0.2872796952724457\n",
      "Epoch: 397\n",
      "Train loss: 0.26550817489624023\n",
      "Test loss: 0.28724968433380127\n",
      "Epoch: 398\n",
      "Train loss: 0.2654877305030823\n",
      "Test loss: 0.28721973299980164\n",
      "Epoch: 399\n",
      "Train loss: 0.2654673457145691\n",
      "Test loss: 0.28718996047973633\n",
      "Epoch: 400\n",
      "Train loss: 0.2654470205307007\n",
      "Test loss: 0.28716033697128296\n",
      "Epoch: 401\n",
      "Train loss: 0.26542675495147705\n",
      "Test loss: 0.28713077306747437\n",
      "Epoch: 402\n",
      "Train loss: 0.26540645956993103\n",
      "Test loss: 0.2871013581752777\n",
      "Epoch: 403\n",
      "Train loss: 0.26538628339767456\n",
      "Test loss: 0.287072092294693\n",
      "Epoch: 404\n",
      "Train loss: 0.2653661072254181\n",
      "Test loss: 0.28704291582107544\n",
      "Epoch: 405\n",
      "Train loss: 0.2653459906578064\n",
      "Test loss: 0.28701382875442505\n",
      "Epoch: 406\n",
      "Train loss: 0.2653258740901947\n",
      "Test loss: 0.2869848906993866\n",
      "Epoch: 407\n",
      "Train loss: 0.26530584692955017\n",
      "Test loss: 0.2869560122489929\n",
      "Epoch: 408\n",
      "Train loss: 0.26528581976890564\n",
      "Test loss: 0.28692731261253357\n",
      "Epoch: 409\n",
      "Train loss: 0.2652658522129059\n",
      "Test loss: 0.286898672580719\n",
      "Epoch: 410\n",
      "Train loss: 0.2652459442615509\n",
      "Test loss: 0.28687015175819397\n",
      "Epoch: 411\n",
      "Train loss: 0.2652260363101959\n",
      "Test loss: 0.2868417501449585\n",
      "Epoch: 412\n",
      "Train loss: 0.2652061879634857\n",
      "Test loss: 0.2868134379386902\n",
      "Epoch: 413\n",
      "Train loss: 0.2651863992214203\n",
      "Test loss: 0.2867853045463562\n",
      "Epoch: 414\n",
      "Train loss: 0.26516658067703247\n",
      "Test loss: 0.2867571711540222\n",
      "Epoch: 415\n",
      "Train loss: 0.2651468813419342\n",
      "Test loss: 0.28672921657562256\n",
      "Epoch: 416\n",
      "Train loss: 0.26512712240219116\n",
      "Test loss: 0.28670135140419006\n",
      "Epoch: 417\n",
      "Train loss: 0.26510751247406006\n",
      "Test loss: 0.2866736352443695\n",
      "Epoch: 418\n",
      "Train loss: 0.26508787274360657\n",
      "Test loss: 0.28664594888687134\n",
      "Epoch: 419\n",
      "Train loss: 0.26506826281547546\n",
      "Test loss: 0.2866183817386627\n",
      "Epoch: 420\n",
      "Train loss: 0.26504868268966675\n",
      "Test loss: 0.28659093379974365\n",
      "Epoch: 421\n",
      "Train loss: 0.2650292217731476\n",
      "Test loss: 0.28656360507011414\n",
      "Epoch: 422\n",
      "Train loss: 0.26500970125198364\n",
      "Test loss: 0.2865363657474518\n",
      "Epoch: 423\n",
      "Train loss: 0.26499029994010925\n",
      "Test loss: 0.286509245634079\n",
      "Epoch: 424\n",
      "Train loss: 0.2649708688259125\n",
      "Test loss: 0.28648218512535095\n",
      "Epoch: 425\n",
      "Train loss: 0.2649514675140381\n",
      "Test loss: 0.2864552140235901\n",
      "Epoch: 426\n",
      "Train loss: 0.26493215560913086\n",
      "Test loss: 0.28642839193344116\n",
      "Epoch: 427\n",
      "Train loss: 0.26491284370422363\n",
      "Test loss: 0.286401629447937\n",
      "Epoch: 428\n",
      "Train loss: 0.2648935914039612\n",
      "Test loss: 0.2863749861717224\n",
      "Epoch: 429\n",
      "Train loss: 0.26487433910369873\n",
      "Test loss: 0.286348432302475\n",
      "Epoch: 430\n",
      "Train loss: 0.26485514640808105\n",
      "Test loss: 0.2863219678401947\n",
      "Epoch: 431\n",
      "Train loss: 0.2648359537124634\n",
      "Test loss: 0.2862955927848816\n",
      "Epoch: 432\n",
      "Train loss: 0.26481685042381287\n",
      "Test loss: 0.28626933693885803\n",
      "Epoch: 433\n",
      "Train loss: 0.26479774713516235\n",
      "Test loss: 0.28624311089515686\n",
      "Epoch: 434\n",
      "Train loss: 0.26477867364883423\n",
      "Test loss: 0.2862170338630676\n",
      "Epoch: 435\n",
      "Train loss: 0.2647596001625061\n",
      "Test loss: 0.28619104623794556\n",
      "Epoch: 436\n",
      "Train loss: 0.2647406756877899\n",
      "Test loss: 0.28616514801979065\n",
      "Epoch: 437\n",
      "Train loss: 0.26472169160842896\n",
      "Test loss: 0.2861393094062805\n",
      "Epoch: 438\n",
      "Train loss: 0.26470276713371277\n",
      "Test loss: 0.2861136198043823\n",
      "Epoch: 439\n",
      "Train loss: 0.26468387246131897\n",
      "Test loss: 0.2860879898071289\n",
      "Epoch: 440\n",
      "Train loss: 0.26466503739356995\n",
      "Test loss: 0.28606247901916504\n",
      "Epoch: 441\n",
      "Train loss: 0.26464617252349854\n",
      "Test loss: 0.28603696823120117\n",
      "Epoch: 442\n",
      "Train loss: 0.2646273672580719\n",
      "Test loss: 0.28601160645484924\n",
      "Epoch: 443\n",
      "Train loss: 0.26460862159729004\n",
      "Test loss: 0.2859863042831421\n",
      "Epoch: 444\n",
      "Train loss: 0.2645898759365082\n",
      "Test loss: 0.2859611511230469\n",
      "Epoch: 445\n",
      "Train loss: 0.2645711898803711\n",
      "Test loss: 0.28593602776527405\n",
      "Epoch: 446\n",
      "Train loss: 0.264552503824234\n",
      "Test loss: 0.2859109938144684\n",
      "Epoch: 447\n",
      "Train loss: 0.2645338773727417\n",
      "Test loss: 0.2858860492706299\n",
      "Epoch: 448\n",
      "Train loss: 0.264515221118927\n",
      "Test loss: 0.28586122393608093\n",
      "Epoch: 449\n",
      "Train loss: 0.26449671387672424\n",
      "Test loss: 0.285836398601532\n",
      "Epoch: 450\n",
      "Train loss: 0.2644781172275543\n",
      "Test loss: 0.28581175208091736\n",
      "Epoch: 451\n",
      "Train loss: 0.26445960998535156\n",
      "Test loss: 0.2857871353626251\n",
      "Epoch: 452\n",
      "Train loss: 0.2644411325454712\n",
      "Test loss: 0.28576257824897766\n",
      "Epoch: 453\n",
      "Train loss: 0.2644227147102356\n",
      "Test loss: 0.28573817014694214\n",
      "Epoch: 454\n",
      "Train loss: 0.2644042670726776\n",
      "Test loss: 0.2857138216495514\n",
      "Epoch: 455\n",
      "Train loss: 0.2643858790397644\n",
      "Test loss: 0.2856895327568054\n",
      "Epoch: 456\n",
      "Train loss: 0.2643675208091736\n",
      "Test loss: 0.2856653034687042\n",
      "Epoch: 457\n",
      "Train loss: 0.26434919238090515\n",
      "Test loss: 0.2856411933898926\n",
      "Epoch: 458\n",
      "Train loss: 0.2643308937549591\n",
      "Test loss: 0.2856171429157257\n",
      "Epoch: 459\n",
      "Train loss: 0.26431262493133545\n",
      "Test loss: 0.2855932116508484\n",
      "Epoch: 460\n",
      "Train loss: 0.2642943859100342\n",
      "Test loss: 0.28556931018829346\n",
      "Epoch: 461\n",
      "Train loss: 0.2642762064933777\n",
      "Test loss: 0.2855454981327057\n",
      "Epoch: 462\n",
      "Train loss: 0.2642580270767212\n",
      "Test loss: 0.2855217754840851\n",
      "Epoch: 463\n",
      "Train loss: 0.2642398774623871\n",
      "Test loss: 0.28549808263778687\n",
      "Epoch: 464\n",
      "Train loss: 0.264221727848053\n",
      "Test loss: 0.2854745090007782\n",
      "Epoch: 465\n",
      "Train loss: 0.26420363783836365\n",
      "Test loss: 0.2854510247707367\n",
      "Epoch: 466\n",
      "Train loss: 0.2641855776309967\n",
      "Test loss: 0.28542760014533997\n",
      "Epoch: 467\n",
      "Train loss: 0.26416754722595215\n",
      "Test loss: 0.2854042053222656\n",
      "Epoch: 468\n",
      "Train loss: 0.26414954662323\n",
      "Test loss: 0.28538089990615845\n",
      "Epoch: 469\n",
      "Train loss: 0.2641315758228302\n",
      "Test loss: 0.28535768389701843\n",
      "Epoch: 470\n",
      "Train loss: 0.2641136050224304\n",
      "Test loss: 0.2853345572948456\n",
      "Epoch: 471\n",
      "Train loss: 0.264095664024353\n",
      "Test loss: 0.2853114902973175\n",
      "Epoch: 472\n",
      "Train loss: 0.2640778422355652\n",
      "Test loss: 0.2852885127067566\n",
      "Epoch: 473\n",
      "Train loss: 0.2640599310398102\n",
      "Test loss: 0.2852655351161957\n",
      "Epoch: 474\n",
      "Train loss: 0.2640421390533447\n",
      "Test loss: 0.2852427363395691\n",
      "Epoch: 475\n",
      "Train loss: 0.2640243470668793\n",
      "Test loss: 0.2852199077606201\n",
      "Epoch: 476\n",
      "Train loss: 0.2640065550804138\n",
      "Test loss: 0.2851971983909607\n",
      "Epoch: 477\n",
      "Train loss: 0.26398879289627075\n",
      "Test loss: 0.28517457842826843\n",
      "Epoch: 478\n",
      "Train loss: 0.26397109031677246\n",
      "Test loss: 0.28515198826789856\n",
      "Epoch: 479\n",
      "Train loss: 0.26395338773727417\n",
      "Test loss: 0.28512948751449585\n",
      "Epoch: 480\n",
      "Train loss: 0.26393574476242065\n",
      "Test loss: 0.2851070761680603\n",
      "Epoch: 481\n",
      "Train loss: 0.26391810178756714\n",
      "Test loss: 0.28508469462394714\n",
      "Epoch: 482\n",
      "Train loss: 0.263900488615036\n",
      "Test loss: 0.28506237268447876\n",
      "Epoch: 483\n",
      "Train loss: 0.26388290524482727\n",
      "Test loss: 0.28504014015197754\n",
      "Epoch: 484\n",
      "Train loss: 0.26386532187461853\n",
      "Test loss: 0.2850179672241211\n",
      "Epoch: 485\n",
      "Train loss: 0.26384779810905457\n",
      "Test loss: 0.2849958539009094\n",
      "Epoch: 486\n",
      "Train loss: 0.263830304145813\n",
      "Test loss: 0.28497380018234253\n",
      "Epoch: 487\n",
      "Train loss: 0.2638128399848938\n",
      "Test loss: 0.2849518358707428\n",
      "Epoch: 488\n",
      "Train loss: 0.263795405626297\n",
      "Test loss: 0.2849299907684326\n",
      "Epoch: 489\n",
      "Train loss: 0.2637779712677002\n",
      "Test loss: 0.28490808606147766\n",
      "Epoch: 490\n",
      "Train loss: 0.26376059651374817\n",
      "Test loss: 0.28488633036613464\n",
      "Epoch: 491\n",
      "Train loss: 0.26374322175979614\n",
      "Test loss: 0.284864604473114\n",
      "Epoch: 492\n",
      "Train loss: 0.2637258470058441\n",
      "Test loss: 0.28484293818473816\n",
      "Epoch: 493\n",
      "Train loss: 0.26370853185653687\n",
      "Test loss: 0.28482136130332947\n",
      "Epoch: 494\n",
      "Train loss: 0.2636912763118744\n",
      "Test loss: 0.28479984402656555\n",
      "Epoch: 495\n",
      "Train loss: 0.2636739909648895\n",
      "Test loss: 0.284778356552124\n",
      "Epoch: 496\n",
      "Train loss: 0.26365676522254944\n",
      "Test loss: 0.28475698828697205\n",
      "Epoch: 497\n",
      "Train loss: 0.26363953948020935\n",
      "Test loss: 0.2847355902194977\n",
      "Epoch: 498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.26362234354019165\n",
      "Test loss: 0.28471434116363525\n",
      "Epoch: 499\n",
      "Train loss: 0.2636052370071411\n",
      "Test loss: 0.2846930921077728\n",
      "====================================================================================================\n",
      "Trainer: BPTrainer (Adam_trainer)\n",
      "Settings: {'method': 'AdamOptimizer', 'args': {'learning_rate': 0.005}}\n",
      "====================================================================================================\n",
      "Epoch: init\n",
      "Train loss: 76.24555969238281\n",
      "Test loss: 89.11460876464844\n",
      "Epoch: 0\n",
      "Train loss: 58.344974517822266\n",
      "Test loss: 68.31175231933594\n",
      "Epoch: 1\n",
      "Train loss: 44.74446487426758\n",
      "Test loss: 52.48972702026367\n",
      "Epoch: 2\n",
      "Train loss: 34.507015228271484\n",
      "Test loss: 40.5676383972168\n",
      "Epoch: 3\n",
      "Train loss: 26.82599449157715\n",
      "Test loss: 31.612756729125977\n",
      "Epoch: 4\n",
      "Train loss: 21.054109573364258\n",
      "Test loss: 24.87596321105957\n",
      "Epoch: 5\n",
      "Train loss: 16.695707321166992\n",
      "Test loss: 19.783010482788086\n",
      "Epoch: 6\n",
      "Train loss: 13.382044792175293\n",
      "Test loss: 15.906180381774902\n",
      "Epoch: 7\n",
      "Train loss: 10.842714309692383\n",
      "Test loss: 12.93144416809082\n",
      "Epoch: 8\n",
      "Train loss: 8.88032341003418\n",
      "Test loss: 10.629358291625977\n",
      "Epoch: 9\n",
      "Train loss: 7.350700378417969\n",
      "Test loss: 8.832228660583496\n",
      "Epoch: 10\n",
      "Train loss: 6.148179054260254\n",
      "Test loss: 7.4170660972595215\n",
      "Epoch: 11\n",
      "Train loss: 5.194887638092041\n",
      "Test loss: 6.293179988861084\n",
      "Epoch: 12\n",
      "Train loss: 4.433049201965332\n",
      "Test loss: 5.393237113952637\n",
      "Epoch: 13\n",
      "Train loss: 3.819472312927246\n",
      "Test loss: 4.666869640350342\n",
      "Epoch: 14\n",
      "Train loss: 3.321603536605835\n",
      "Test loss: 4.076088905334473\n",
      "Epoch: 15\n",
      "Train loss: 2.914703369140625\n",
      "Test loss: 3.592007637023926\n",
      "Epoch: 16\n",
      "Train loss: 2.5798089504241943\n",
      "Test loss: 3.192472219467163\n",
      "Epoch: 17\n",
      "Train loss: 2.3022778034210205\n",
      "Test loss: 2.8603620529174805\n",
      "Epoch: 18\n",
      "Train loss: 2.070712089538574\n",
      "Test loss: 2.582348585128784\n",
      "Epoch: 19\n",
      "Train loss: 1.8761886358261108\n",
      "Test loss: 2.347989320755005\n",
      "Epoch: 20\n",
      "Train loss: 1.7116822004318237\n",
      "Test loss: 2.1490566730499268\n",
      "Epoch: 21\n",
      "Train loss: 1.571636438369751\n",
      "Test loss: 1.9790414571762085\n",
      "Epoch: 22\n",
      "Train loss: 1.4516412019729614\n",
      "Test loss: 1.8327703475952148\n",
      "Epoch: 23\n",
      "Train loss: 1.348184585571289\n",
      "Test loss: 1.7061219215393066\n",
      "Epoch: 24\n",
      "Train loss: 1.25846266746521\n",
      "Test loss: 1.5957999229431152\n",
      "Epoch: 25\n",
      "Train loss: 1.180227518081665\n",
      "Test loss: 1.4991596937179565\n",
      "Epoch: 26\n",
      "Train loss: 1.1116710901260376\n",
      "Test loss: 1.4140714406967163\n",
      "Epoch: 27\n",
      "Train loss: 1.0513309240341187\n",
      "Test loss: 1.3388100862503052\n",
      "Epoch: 28\n",
      "Train loss: 0.9980153441429138\n",
      "Test loss: 1.2719709873199463\n",
      "Epoch: 29\n",
      "Train loss: 0.9507466554641724\n",
      "Test loss: 1.2124011516571045\n",
      "Epoch: 30\n",
      "Train loss: 0.9087138175964355\n",
      "Test loss: 1.159145474433899\n",
      "Epoch: 31\n",
      "Train loss: 0.8712365031242371\n",
      "Test loss: 1.1114048957824707\n",
      "Epoch: 32\n",
      "Train loss: 0.8377379775047302\n",
      "Test loss: 1.068503737449646\n",
      "Epoch: 33\n",
      "Train loss: 0.8077226281166077\n",
      "Test loss: 1.0298640727996826\n",
      "Epoch: 34\n",
      "Train loss: 0.7807613611221313\n",
      "Test loss: 0.9949867725372314\n",
      "Epoch: 35\n",
      "Train loss: 0.7564787268638611\n",
      "Test loss: 0.9634369611740112\n",
      "Epoch: 36\n",
      "Train loss: 0.7345443367958069\n",
      "Test loss: 0.9348326325416565\n",
      "Epoch: 37\n",
      "Train loss: 0.7146674990653992\n",
      "Test loss: 0.9088375568389893\n",
      "Epoch: 38\n",
      "Train loss: 0.6965916752815247\n",
      "Test loss: 0.8851546049118042\n",
      "Epoch: 39\n",
      "Train loss: 0.6800919771194458\n",
      "Test loss: 0.8635215163230896\n",
      "Epoch: 40\n",
      "Train loss: 0.6649719476699829\n",
      "Test loss: 0.8437072038650513\n",
      "Epoch: 41\n",
      "Train loss: 0.6510618925094604\n",
      "Test loss: 0.8255088329315186\n",
      "Epoch: 42\n",
      "Train loss: 0.6382154226303101\n",
      "Test loss: 0.8087488412857056\n",
      "Epoch: 43\n",
      "Train loss: 0.6263086795806885\n",
      "Test loss: 0.7932726144790649\n",
      "Epoch: 44\n",
      "Train loss: 0.6152364611625671\n",
      "Test loss: 0.778945803642273\n",
      "Epoch: 45\n",
      "Train loss: 0.6049107313156128\n",
      "Test loss: 0.7656511068344116\n",
      "Epoch: 46\n",
      "Train loss: 0.5952566862106323\n",
      "Test loss: 0.7532860040664673\n",
      "Epoch: 47\n",
      "Train loss: 0.586212158203125\n",
      "Test loss: 0.741760790348053\n",
      "Epoch: 48\n",
      "Train loss: 0.5777235627174377\n",
      "Test loss: 0.730995774269104\n",
      "Epoch: 49\n",
      "Train loss: 0.569744348526001\n",
      "Test loss: 0.7209192514419556\n",
      "Epoch: 50\n",
      "Train loss: 0.562233567237854\n",
      "Test loss: 0.7114662528038025\n",
      "Epoch: 51\n",
      "Train loss: 0.5551537871360779\n",
      "Test loss: 0.7025768160820007\n",
      "Epoch: 52\n",
      "Train loss: 0.5484707951545715\n",
      "Test loss: 0.6941956281661987\n",
      "Epoch: 53\n",
      "Train loss: 0.5421515703201294\n",
      "Test loss: 0.6862709522247314\n",
      "Epoch: 54\n",
      "Train loss: 0.5361658930778503\n",
      "Test loss: 0.678754985332489\n",
      "Epoch: 55\n",
      "Train loss: 0.5304844975471497\n",
      "Test loss: 0.6716027855873108\n",
      "Epoch: 56\n",
      "Train loss: 0.5250799059867859\n",
      "Test loss: 0.6647736430168152\n",
      "Epoch: 57\n",
      "Train loss: 0.5199265480041504\n",
      "Test loss: 0.6582304239273071\n",
      "Epoch: 58\n",
      "Train loss: 0.5150007009506226\n",
      "Test loss: 0.6519400477409363\n",
      "Epoch: 59\n",
      "Train loss: 0.5102810859680176\n",
      "Test loss: 0.645873486995697\n",
      "Epoch: 60\n",
      "Train loss: 0.5057485103607178\n",
      "Test loss: 0.6400060653686523\n",
      "Epoch: 61\n",
      "Train loss: 0.5013865232467651\n",
      "Test loss: 0.6343173384666443\n",
      "Epoch: 62\n",
      "Train loss: 0.4971805214881897\n",
      "Test loss: 0.6287904381752014\n",
      "Epoch: 63\n",
      "Train loss: 0.4931184947490692\n",
      "Test loss: 0.6234123110771179\n",
      "Epoch: 64\n",
      "Train loss: 0.48919016122817993\n",
      "Test loss: 0.6181729435920715\n",
      "Epoch: 65\n",
      "Train loss: 0.4853866994380951\n",
      "Test loss: 0.6130649447441101\n",
      "Epoch: 66\n",
      "Train loss: 0.4817006289958954\n",
      "Test loss: 0.6080829501152039\n",
      "Epoch: 67\n",
      "Train loss: 0.4781254827976227\n",
      "Test loss: 0.6032233834266663\n",
      "Epoch: 68\n",
      "Train loss: 0.474655419588089\n",
      "Test loss: 0.5984833240509033\n",
      "Epoch: 69\n",
      "Train loss: 0.4712848961353302\n",
      "Test loss: 0.5938608050346375\n",
      "Epoch: 70\n",
      "Train loss: 0.46800896525382996\n",
      "Test loss: 0.5893540382385254\n",
      "Epoch: 71\n",
      "Train loss: 0.4648224711418152\n",
      "Test loss: 0.5849613547325134\n",
      "Epoch: 72\n",
      "Train loss: 0.46172043681144714\n",
      "Test loss: 0.5806808471679688\n",
      "Epoch: 73\n",
      "Train loss: 0.4586982727050781\n",
      "Test loss: 0.5765100121498108\n",
      "Epoch: 74\n",
      "Train loss: 0.45575109124183655\n",
      "Test loss: 0.5724465847015381\n",
      "Epoch: 75\n",
      "Train loss: 0.45287466049194336\n",
      "Test loss: 0.5684875249862671\n",
      "Epoch: 76\n",
      "Train loss: 0.45006468892097473\n",
      "Test loss: 0.5646296739578247\n",
      "Epoch: 77\n",
      "Train loss: 0.44731730222702026\n",
      "Test loss: 0.5608693361282349\n",
      "Epoch: 78\n",
      "Train loss: 0.4446292519569397\n",
      "Test loss: 0.5572031140327454\n",
      "Epoch: 79\n",
      "Train loss: 0.44199734926223755\n",
      "Test loss: 0.5536267161369324\n",
      "Epoch: 80\n",
      "Train loss: 0.4394189119338989\n",
      "Test loss: 0.5501363277435303\n",
      "Epoch: 81\n",
      "Train loss: 0.43689167499542236\n",
      "Test loss: 0.5467278361320496\n",
      "Epoch: 82\n",
      "Train loss: 0.43441373109817505\n",
      "Test loss: 0.5433968901634216\n",
      "Epoch: 83\n",
      "Train loss: 0.4319833815097809\n",
      "Test loss: 0.5401392579078674\n",
      "Epoch: 84\n",
      "Train loss: 0.42959898710250854\n",
      "Test loss: 0.5369510054588318\n",
      "Epoch: 85\n",
      "Train loss: 0.42725902795791626\n",
      "Test loss: 0.5338274240493774\n",
      "Epoch: 86\n",
      "Train loss: 0.4249625504016876\n",
      "Test loss: 0.5307645797729492\n",
      "Epoch: 87\n",
      "Train loss: 0.4227079749107361\n",
      "Test loss: 0.5277585387229919\n",
      "Epoch: 88\n",
      "Train loss: 0.42049431800842285\n",
      "Test loss: 0.5248051881790161\n",
      "Epoch: 89\n",
      "Train loss: 0.4183202385902405\n",
      "Test loss: 0.5219009518623352\n",
      "Epoch: 90\n",
      "Train loss: 0.4161844551563263\n",
      "Test loss: 0.5190423130989075\n",
      "Epoch: 91\n",
      "Train loss: 0.41408589482307434\n",
      "Test loss: 0.5162264108657837\n",
      "Epoch: 92\n",
      "Train loss: 0.4120233952999115\n",
      "Test loss: 0.5134504437446594\n",
      "Epoch: 93\n",
      "Train loss: 0.40999582409858704\n",
      "Test loss: 0.5107119083404541\n",
      "Epoch: 94\n",
      "Train loss: 0.40800216794013977\n",
      "Test loss: 0.5080092549324036\n",
      "Epoch: 95\n",
      "Train loss: 0.40604153275489807\n",
      "Test loss: 0.5053406953811646\n",
      "Epoch: 96\n",
      "Train loss: 0.40411269664764404\n",
      "Test loss: 0.5027053952217102\n",
      "Epoch: 97\n",
      "Train loss: 0.4022153615951538\n",
      "Test loss: 0.5001024603843689\n",
      "Epoch: 98\n",
      "Train loss: 0.400348424911499\n",
      "Test loss: 0.49753156304359436\n",
      "Epoch: 99\n",
      "Train loss: 0.3985114097595215\n",
      "Test loss: 0.49499282240867615\n",
      "Epoch: 100\n",
      "Train loss: 0.3967033922672272\n",
      "Test loss: 0.4924861788749695\n",
      "Epoch: 101\n",
      "Train loss: 0.3949242830276489\n",
      "Test loss: 0.49001187086105347\n",
      "Epoch: 102\n",
      "Train loss: 0.3931730389595032\n",
      "Test loss: 0.48757028579711914\n",
      "Epoch: 103\n",
      "Train loss: 0.39144930243492126\n",
      "Test loss: 0.48516198992729187\n",
      "Epoch: 104\n",
      "Train loss: 0.3897526264190674\n",
      "Test loss: 0.4827873110771179\n",
      "Epoch: 105\n",
      "Train loss: 0.38808244466781616\n",
      "Test loss: 0.48044663667678833\n",
      "Epoch: 106\n",
      "Train loss: 0.38643819093704224\n",
      "Test loss: 0.47814038395881653\n",
      "Epoch: 107\n",
      "Train loss: 0.3848193883895874\n",
      "Test loss: 0.4758686125278473\n",
      "Epoch: 108\n",
      "Train loss: 0.3832255005836487\n",
      "Test loss: 0.4736313819885254\n",
      "Epoch: 109\n",
      "Train loss: 0.38165608048439026\n",
      "Test loss: 0.4714285731315613\n",
      "Epoch: 110\n",
      "Train loss: 0.380110502243042\n",
      "Test loss: 0.4692600667476654\n",
      "Epoch: 111\n",
      "Train loss: 0.37858858704566956\n",
      "Test loss: 0.4671255052089691\n",
      "Epoch: 112\n",
      "Train loss: 0.37708964943885803\n",
      "Test loss: 0.46502429246902466\n",
      "Epoch: 113\n",
      "Train loss: 0.37561333179473877\n",
      "Test loss: 0.4629557430744171\n",
      "Epoch: 114\n",
      "Train loss: 0.37415921688079834\n",
      "Test loss: 0.4609193205833435\n",
      "Epoch: 115\n",
      "Train loss: 0.3727269768714905\n",
      "Test loss: 0.4589141309261322\n",
      "Epoch: 116\n",
      "Train loss: 0.37131616473197937\n",
      "Test loss: 0.4569391906261444\n",
      "Epoch: 117\n",
      "Train loss: 0.3699265122413635\n",
      "Test loss: 0.45499387383461\n",
      "Epoch: 118\n",
      "Train loss: 0.3685576319694519\n",
      "Test loss: 0.45307692885398865\n",
      "Epoch: 119\n",
      "Train loss: 0.3672090172767639\n",
      "Test loss: 0.45118752121925354\n",
      "Epoch: 120\n",
      "Train loss: 0.3658805787563324\n",
      "Test loss: 0.44932472705841064\n",
      "Epoch: 121\n",
      "Train loss: 0.3645718991756439\n",
      "Test loss: 0.44748762249946594\n",
      "Epoch: 122\n",
      "Train loss: 0.3632825016975403\n",
      "Test loss: 0.4456752836704254\n",
      "Epoch: 123\n",
      "Train loss: 0.3620123267173767\n",
      "Test loss: 0.44388702511787415\n",
      "Epoch: 124\n",
      "Train loss: 0.36076077818870544\n",
      "Test loss: 0.44212210178375244\n",
      "Epoch: 125\n",
      "Train loss: 0.3595277667045593\n",
      "Test loss: 0.4403797388076782\n",
      "Epoch: 126\n",
      "Train loss: 0.35831281542778015\n",
      "Test loss: 0.4386594891548157\n",
      "Epoch: 127\n",
      "Train loss: 0.3571157455444336\n",
      "Test loss: 0.43696093559265137\n",
      "Epoch: 128\n",
      "Train loss: 0.35593611001968384\n",
      "Test loss: 0.43528351187705994\n",
      "Epoch: 129\n",
      "Train loss: 0.3547738492488861\n",
      "Test loss: 0.4336271584033966\n",
      "Epoch: 130\n",
      "Train loss: 0.3536285161972046\n",
      "Test loss: 0.4319912791252136\n",
      "Epoch: 131\n",
      "Train loss: 0.35249975323677063\n",
      "Test loss: 0.4303760826587677\n",
      "Epoch: 132\n",
      "Train loss: 0.35138750076293945\n",
      "Test loss: 0.4287811517715454\n",
      "Epoch: 133\n",
      "Train loss: 0.3502914011478424\n",
      "Test loss: 0.42720651626586914\n",
      "Epoch: 134\n",
      "Train loss: 0.3492112159729004\n",
      "Test loss: 0.42565199732780457\n",
      "Epoch: 135\n",
      "Train loss: 0.3481466472148895\n",
      "Test loss: 0.4241175055503845\n",
      "Epoch: 136\n",
      "Train loss: 0.3470974862575531\n",
      "Test loss: 0.42260321974754333\n",
      "Epoch: 137\n",
      "Train loss: 0.34606340527534485\n",
      "Test loss: 0.42110878229141235\n",
      "Epoch: 138\n",
      "Train loss: 0.3450443744659424\n",
      "Test loss: 0.41963422298431396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139\n",
      "Train loss: 0.34403982758522034\n",
      "Test loss: 0.41817936301231384\n",
      "Epoch: 140\n",
      "Train loss: 0.3430497944355011\n",
      "Test loss: 0.4167441129684448\n",
      "Epoch: 141\n",
      "Train loss: 0.3420740067958832\n",
      "Test loss: 0.4153282046318054\n",
      "Epoch: 142\n",
      "Train loss: 0.3411121666431427\n",
      "Test loss: 0.4139314889907837\n",
      "Epoch: 143\n",
      "Train loss: 0.34016403555870056\n",
      "Test loss: 0.41255369782447815\n",
      "Epoch: 144\n",
      "Train loss: 0.33922940492630005\n",
      "Test loss: 0.41119450330734253\n",
      "Epoch: 145\n",
      "Train loss: 0.3383081555366516\n",
      "Test loss: 0.4098537266254425\n",
      "Epoch: 146\n",
      "Train loss: 0.33740007877349854\n",
      "Test loss: 0.40853098034858704\n",
      "Epoch: 147\n",
      "Train loss: 0.3365047872066498\n",
      "Test loss: 0.4072258472442627\n",
      "Epoch: 148\n",
      "Train loss: 0.3356221318244934\n",
      "Test loss: 0.4059380292892456\n",
      "Epoch: 149\n",
      "Train loss: 0.3347521126270294\n",
      "Test loss: 0.4046672582626343\n",
      "Epoch: 150\n",
      "Train loss: 0.3338943421840668\n",
      "Test loss: 0.40341290831565857\n",
      "Epoch: 151\n",
      "Train loss: 0.33304861187934875\n",
      "Test loss: 0.4021748900413513\n",
      "Epoch: 152\n",
      "Train loss: 0.332214891910553\n",
      "Test loss: 0.4009527564048767\n",
      "Epoch: 153\n",
      "Train loss: 0.33139288425445557\n",
      "Test loss: 0.3997461199760437\n",
      "Epoch: 154\n",
      "Train loss: 0.3305823504924774\n",
      "Test loss: 0.3985546827316284\n",
      "Epoch: 155\n",
      "Train loss: 0.32978326082229614\n",
      "Test loss: 0.39737823605537415\n",
      "Epoch: 156\n",
      "Train loss: 0.32899534702301025\n",
      "Test loss: 0.39621639251708984\n",
      "Epoch: 157\n",
      "Train loss: 0.3282184302806854\n",
      "Test loss: 0.3950689435005188\n",
      "Epoch: 158\n",
      "Train loss: 0.32745233178138733\n",
      "Test loss: 0.39393559098243713\n",
      "Epoch: 159\n",
      "Train loss: 0.3266969621181488\n",
      "Test loss: 0.39281612634658813\n",
      "Epoch: 160\n",
      "Train loss: 0.32595202326774597\n",
      "Test loss: 0.3917105197906494\n",
      "Epoch: 161\n",
      "Train loss: 0.3252173662185669\n",
      "Test loss: 0.39061832427978516\n",
      "Epoch: 162\n",
      "Train loss: 0.32449302077293396\n",
      "Test loss: 0.38953959941864014\n",
      "Epoch: 163\n",
      "Train loss: 0.3237786591053009\n",
      "Test loss: 0.3884740173816681\n",
      "Epoch: 164\n",
      "Train loss: 0.3230741620063782\n",
      "Test loss: 0.38742169737815857\n",
      "Epoch: 165\n",
      "Train loss: 0.32237938046455383\n",
      "Test loss: 0.38638216257095337\n",
      "Epoch: 166\n",
      "Train loss: 0.32169413566589355\n",
      "Test loss: 0.38535550236701965\n",
      "Epoch: 167\n",
      "Train loss: 0.3210183084011078\n",
      "Test loss: 0.3843415081501007\n",
      "Epoch: 168\n",
      "Train loss: 0.32035180926322937\n",
      "Test loss: 0.38334009051322937\n",
      "Epoch: 169\n",
      "Train loss: 0.3196943700313568\n",
      "Test loss: 0.3823510706424713\n",
      "Epoch: 170\n",
      "Train loss: 0.3190459907054901\n",
      "Test loss: 0.38137441873550415\n",
      "Epoch: 171\n",
      "Train loss: 0.31840643286705017\n",
      "Test loss: 0.38040971755981445\n",
      "Epoch: 172\n",
      "Train loss: 0.31777554750442505\n",
      "Test loss: 0.3794570565223694\n",
      "Epoch: 173\n",
      "Train loss: 0.31715336441993713\n",
      "Test loss: 0.37851616740226746\n",
      "Epoch: 174\n",
      "Train loss: 0.3165395259857178\n",
      "Test loss: 0.3775869309902191\n",
      "Epoch: 175\n",
      "Train loss: 0.31593403220176697\n",
      "Test loss: 0.3766691982746124\n",
      "Epoch: 176\n",
      "Train loss: 0.31533676385879517\n",
      "Test loss: 0.3757627010345459\n",
      "Epoch: 177\n",
      "Train loss: 0.31474757194519043\n",
      "Test loss: 0.3748672604560852\n",
      "Epoch: 178\n",
      "Train loss: 0.31416624784469604\n",
      "Test loss: 0.37398284673690796\n",
      "Epoch: 179\n",
      "Train loss: 0.313592791557312\n",
      "Test loss: 0.3731091320514679\n",
      "Epoch: 180\n",
      "Train loss: 0.3130269944667816\n",
      "Test loss: 0.3722459673881531\n",
      "Epoch: 181\n",
      "Train loss: 0.312468945980072\n",
      "Test loss: 0.37139323353767395\n",
      "Epoch: 182\n",
      "Train loss: 0.3119182884693146\n",
      "Test loss: 0.37055063247680664\n",
      "Epoch: 183\n",
      "Train loss: 0.3113749921321869\n",
      "Test loss: 0.36971816420555115\n",
      "Epoch: 184\n",
      "Train loss: 0.3108389675617218\n",
      "Test loss: 0.36889562010765076\n",
      "Epoch: 185\n",
      "Train loss: 0.3103100061416626\n",
      "Test loss: 0.368082731962204\n",
      "Epoch: 186\n",
      "Train loss: 0.30978813767433167\n",
      "Test loss: 0.36727938055992126\n",
      "Epoch: 187\n",
      "Train loss: 0.30927321314811707\n",
      "Test loss: 0.3664856255054474\n",
      "Epoch: 188\n",
      "Train loss: 0.30876514315605164\n",
      "Test loss: 0.3657010793685913\n",
      "Epoch: 189\n",
      "Train loss: 0.30826377868652344\n",
      "Test loss: 0.36492565274238586\n",
      "Epoch: 190\n",
      "Train loss: 0.3077690601348877\n",
      "Test loss: 0.3641592860221863\n",
      "Epoch: 191\n",
      "Train loss: 0.3072808086872101\n",
      "Test loss: 0.3634018898010254\n",
      "Epoch: 192\n",
      "Train loss: 0.3067989647388458\n",
      "Test loss: 0.36265328526496887\n",
      "Epoch: 193\n",
      "Train loss: 0.3063235282897949\n",
      "Test loss: 0.36191341280937195\n",
      "Epoch: 194\n",
      "Train loss: 0.30585429072380066\n",
      "Test loss: 0.3611820340156555\n",
      "Epoch: 195\n",
      "Train loss: 0.3053913116455078\n",
      "Test loss: 0.36045923829078674\n",
      "Epoch: 196\n",
      "Train loss: 0.3049342930316925\n",
      "Test loss: 0.3597448170185089\n",
      "Epoch: 197\n",
      "Train loss: 0.30448323488235474\n",
      "Test loss: 0.35903868079185486\n",
      "Epoch: 198\n",
      "Train loss: 0.30403807759284973\n",
      "Test loss: 0.35834065079689026\n",
      "Epoch: 199\n",
      "Train loss: 0.30359870195388794\n",
      "Test loss: 0.35765060782432556\n",
      "Epoch: 200\n",
      "Train loss: 0.3031649887561798\n",
      "Test loss: 0.35696864128112793\n",
      "Epoch: 201\n",
      "Train loss: 0.30273696780204773\n",
      "Test loss: 0.35629457235336304\n",
      "Epoch: 202\n",
      "Train loss: 0.302314430475235\n",
      "Test loss: 0.35562822222709656\n",
      "Epoch: 203\n",
      "Train loss: 0.30189740657806396\n",
      "Test loss: 0.35496947169303894\n",
      "Epoch: 204\n",
      "Train loss: 0.30148574709892273\n",
      "Test loss: 0.3543183207511902\n",
      "Epoch: 205\n",
      "Train loss: 0.3010794520378113\n",
      "Test loss: 0.35367462038993835\n",
      "Epoch: 206\n",
      "Train loss: 0.30067819356918335\n",
      "Test loss: 0.3530381917953491\n",
      "Epoch: 207\n",
      "Train loss: 0.30028223991394043\n",
      "Test loss: 0.3524089753627777\n",
      "Epoch: 208\n",
      "Train loss: 0.2998913824558258\n",
      "Test loss: 0.3517870008945465\n",
      "Epoch: 209\n",
      "Train loss: 0.29950541257858276\n",
      "Test loss: 0.35117194056510925\n",
      "Epoch: 210\n",
      "Train loss: 0.2991243898868561\n",
      "Test loss: 0.3505638539791107\n",
      "Epoch: 211\n",
      "Train loss: 0.298748254776001\n",
      "Test loss: 0.34996262192726135\n",
      "Epoch: 212\n",
      "Train loss: 0.2983769476413727\n",
      "Test loss: 0.3493681252002716\n",
      "Epoch: 213\n",
      "Train loss: 0.29801031947135925\n",
      "Test loss: 0.34878021478652954\n",
      "Epoch: 214\n",
      "Train loss: 0.2976483404636383\n",
      "Test loss: 0.3481988310813904\n",
      "Epoch: 215\n",
      "Train loss: 0.2972909212112427\n",
      "Test loss: 0.34762388467788696\n",
      "Epoch: 216\n",
      "Train loss: 0.29693803191185\n",
      "Test loss: 0.34705525636672974\n",
      "Epoch: 217\n",
      "Train loss: 0.29658961296081543\n",
      "Test loss: 0.3464930057525635\n",
      "Epoch: 218\n",
      "Train loss: 0.2962455451488495\n",
      "Test loss: 0.3459368646144867\n",
      "Epoch: 219\n",
      "Train loss: 0.29590582847595215\n",
      "Test loss: 0.34538689255714417\n",
      "Epoch: 220\n",
      "Train loss: 0.29557037353515625\n",
      "Test loss: 0.34484297037124634\n",
      "Epoch: 221\n",
      "Train loss: 0.29523909091949463\n",
      "Test loss: 0.3443049192428589\n",
      "Epoch: 222\n",
      "Train loss: 0.2949119806289673\n",
      "Test loss: 0.343772828578949\n",
      "Epoch: 223\n",
      "Train loss: 0.29458895325660706\n",
      "Test loss: 0.3432464599609375\n",
      "Epoch: 224\n",
      "Train loss: 0.294269859790802\n",
      "Test loss: 0.342725932598114\n",
      "Epoch: 225\n",
      "Train loss: 0.29395487904548645\n",
      "Test loss: 0.342210978269577\n",
      "Epoch: 226\n",
      "Train loss: 0.2936438024044037\n",
      "Test loss: 0.34170159697532654\n",
      "Epoch: 227\n",
      "Train loss: 0.2933364808559418\n",
      "Test loss: 0.3411978483200073\n",
      "Epoch: 228\n",
      "Train loss: 0.29303303360939026\n",
      "Test loss: 0.3406994044780731\n",
      "Epoch: 229\n",
      "Train loss: 0.29273325204849243\n",
      "Test loss: 0.34020644426345825\n",
      "Epoch: 230\n",
      "Train loss: 0.29243728518486023\n",
      "Test loss: 0.3397187292575836\n",
      "Epoch: 231\n",
      "Train loss: 0.29214492440223694\n",
      "Test loss: 0.339236319065094\n",
      "Epoch: 232\n",
      "Train loss: 0.2918560802936554\n",
      "Test loss: 0.33875903487205505\n",
      "Epoch: 233\n",
      "Train loss: 0.29157087206840515\n",
      "Test loss: 0.3382868766784668\n",
      "Epoch: 234\n",
      "Train loss: 0.29128918051719666\n",
      "Test loss: 0.33781975507736206\n",
      "Epoch: 235\n",
      "Train loss: 0.2910107970237732\n",
      "Test loss: 0.33735761046409607\n",
      "Epoch: 236\n",
      "Train loss: 0.2907359004020691\n",
      "Test loss: 0.33690038323402405\n",
      "Epoch: 237\n",
      "Train loss: 0.2904643714427948\n",
      "Test loss: 0.3364480137825012\n",
      "Epoch: 238\n",
      "Train loss: 0.29019615054130554\n",
      "Test loss: 0.33600038290023804\n",
      "Epoch: 239\n",
      "Train loss: 0.28993117809295654\n",
      "Test loss: 0.3355575203895569\n",
      "Epoch: 240\n",
      "Train loss: 0.28966936469078064\n",
      "Test loss: 0.33511924743652344\n",
      "Epoch: 241\n",
      "Train loss: 0.2894107401371002\n",
      "Test loss: 0.3346855640411377\n",
      "Epoch: 242\n",
      "Train loss: 0.2891552746295929\n",
      "Test loss: 0.3342565596103668\n",
      "Epoch: 243\n",
      "Train loss: 0.2889028787612915\n",
      "Test loss: 0.33383187651634216\n",
      "Epoch: 244\n",
      "Train loss: 0.2886534631252289\n",
      "Test loss: 0.33341169357299805\n",
      "Epoch: 245\n",
      "Train loss: 0.2884071171283722\n",
      "Test loss: 0.33299580216407776\n",
      "Epoch: 246\n",
      "Train loss: 0.2881637215614319\n",
      "Test loss: 0.33258429169654846\n",
      "Epoch: 247\n",
      "Train loss: 0.2879231870174408\n",
      "Test loss: 0.3321770131587982\n",
      "Epoch: 248\n",
      "Train loss: 0.2876855731010437\n",
      "Test loss: 0.33177393674850464\n",
      "Epoch: 249\n",
      "Train loss: 0.28745076060295105\n",
      "Test loss: 0.3313750922679901\n",
      "Epoch: 250\n",
      "Train loss: 0.28721877932548523\n",
      "Test loss: 0.3309802711009979\n",
      "Epoch: 251\n",
      "Train loss: 0.2869895100593567\n",
      "Test loss: 0.3305894434452057\n",
      "Epoch: 252\n",
      "Train loss: 0.2867630422115326\n",
      "Test loss: 0.33020269870758057\n",
      "Epoch: 253\n",
      "Train loss: 0.2865392565727234\n",
      "Test loss: 0.329819917678833\n",
      "Epoch: 254\n",
      "Train loss: 0.28631800413131714\n",
      "Test loss: 0.3294410705566406\n",
      "Epoch: 255\n",
      "Train loss: 0.2860994338989258\n",
      "Test loss: 0.3290660083293915\n",
      "Epoch: 256\n",
      "Train loss: 0.28588345646858215\n",
      "Test loss: 0.32869473099708557\n",
      "Epoch: 257\n",
      "Train loss: 0.2856700122356415\n",
      "Test loss: 0.3283272385597229\n",
      "Epoch: 258\n",
      "Train loss: 0.28545907139778137\n",
      "Test loss: 0.32796353101730347\n",
      "Epoch: 259\n",
      "Train loss: 0.28525060415267944\n",
      "Test loss: 0.32760339975357056\n",
      "Epoch: 260\n",
      "Train loss: 0.28504452109336853\n",
      "Test loss: 0.32724690437316895\n",
      "Epoch: 261\n",
      "Train loss: 0.2848409414291382\n",
      "Test loss: 0.3268941044807434\n",
      "Epoch: 262\n",
      "Train loss: 0.2846396863460541\n",
      "Test loss: 0.32654473185539246\n",
      "Epoch: 263\n",
      "Train loss: 0.2844407856464386\n",
      "Test loss: 0.32619884610176086\n",
      "Epoch: 264\n",
      "Train loss: 0.284244179725647\n",
      "Test loss: 0.3258565366268158\n",
      "Epoch: 265\n",
      "Train loss: 0.2840498983860016\n",
      "Test loss: 0.32551753520965576\n",
      "Epoch: 266\n",
      "Train loss: 0.2838577926158905\n",
      "Test loss: 0.32518184185028076\n",
      "Epoch: 267\n",
      "Train loss: 0.28366801142692566\n",
      "Test loss: 0.3248496651649475\n",
      "Epoch: 268\n",
      "Train loss: 0.28348034620285034\n",
      "Test loss: 0.3245205879211426\n",
      "Epoch: 269\n",
      "Train loss: 0.28329482674598694\n",
      "Test loss: 0.3241948187351227\n",
      "Epoch: 270\n",
      "Train loss: 0.28311148285865784\n",
      "Test loss: 0.32387226819992065\n",
      "Epoch: 271\n",
      "Train loss: 0.2829301953315735\n",
      "Test loss: 0.32355284690856934\n",
      "Epoch: 272\n",
      "Train loss: 0.28275102376937866\n",
      "Test loss: 0.3232366144657135\n",
      "Epoch: 273\n",
      "Train loss: 0.2825739085674286\n",
      "Test loss: 0.322923481464386\n",
      "Epoch: 274\n",
      "Train loss: 0.2823987603187561\n",
      "Test loss: 0.322613388299942\n",
      "Epoch: 275\n",
      "Train loss: 0.28222566843032837\n",
      "Test loss: 0.3223062753677368\n",
      "Epoch: 276\n",
      "Train loss: 0.2820545434951782\n",
      "Test loss: 0.3220021724700928\n",
      "Epoch: 277\n",
      "Train loss: 0.2818852961063385\n",
      "Test loss: 0.3217010200023651\n",
      "Epoch: 278\n",
      "Train loss: 0.28171804547309875\n",
      "Test loss: 0.32140278816223145\n",
      "Epoch: 279\n",
      "Train loss: 0.28155267238616943\n",
      "Test loss: 0.3211074471473694\n",
      "Epoch: 280\n",
      "Train loss: 0.28138914704322815\n",
      "Test loss: 0.3208148777484894\n",
      "Epoch: 281\n",
      "Train loss: 0.2812274396419525\n",
      "Test loss: 0.320525199174881\n",
      "Epoch: 282\n",
      "Train loss: 0.2810675799846649\n",
      "Test loss: 0.32023829221725464\n",
      "Epoch: 283\n",
      "Train loss: 0.2809094786643982\n",
      "Test loss: 0.3199540674686432\n",
      "Epoch: 284\n",
      "Train loss: 0.2807531952857971\n",
      "Test loss: 0.3196725845336914\n",
      "Epoch: 285\n",
      "Train loss: 0.28059864044189453\n",
      "Test loss: 0.3193938136100769\n",
      "Epoch: 286\n",
      "Train loss: 0.28044581413269043\n",
      "Test loss: 0.3191177248954773\n",
      "Epoch: 287\n",
      "Train loss: 0.2802946865558624\n",
      "Test loss: 0.3188442289829254\n",
      "Epoch: 288\n",
      "Train loss: 0.28014522790908813\n",
      "Test loss: 0.3185732662677765\n",
      "Epoch: 289\n",
      "Train loss: 0.27999746799468994\n",
      "Test loss: 0.31830495595932007\n",
      "Epoch: 290\n",
      "Train loss: 0.2798512578010559\n",
      "Test loss: 0.3180391192436218\n",
      "Epoch: 291\n",
      "Train loss: 0.27970677614212036\n",
      "Test loss: 0.31777575612068176\n",
      "Epoch: 292\n",
      "Train loss: 0.2795637845993042\n",
      "Test loss: 0.31751492619514465\n",
      "Epoch: 293\n",
      "Train loss: 0.27942243218421936\n",
      "Test loss: 0.31725648045539856\n",
      "Epoch: 294\n",
      "Train loss: 0.2792826294898987\n",
      "Test loss: 0.31700044870376587\n",
      "Epoch: 295\n",
      "Train loss: 0.2791443467140198\n",
      "Test loss: 0.31674692034721375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 296\n",
      "Train loss: 0.27900761365890503\n",
      "Test loss: 0.3164956569671631\n",
      "Epoch: 297\n",
      "Train loss: 0.2788723409175873\n",
      "Test loss: 0.31624671816825867\n",
      "Epoch: 298\n",
      "Train loss: 0.2787385582923889\n",
      "Test loss: 0.3160001039505005\n",
      "Epoch: 299\n",
      "Train loss: 0.2786061763763428\n",
      "Test loss: 0.31575578451156616\n",
      "Epoch: 300\n",
      "Train loss: 0.27847525477409363\n",
      "Test loss: 0.3155137002468109\n",
      "Epoch: 301\n",
      "Train loss: 0.27834582328796387\n",
      "Test loss: 0.31527388095855713\n",
      "Epoch: 302\n",
      "Train loss: 0.27821770310401917\n",
      "Test loss: 0.31503620743751526\n",
      "Epoch: 303\n",
      "Train loss: 0.27809104323387146\n",
      "Test loss: 0.3148007392883301\n",
      "Epoch: 304\n",
      "Train loss: 0.2779656946659088\n",
      "Test loss: 0.3145674467086792\n",
      "Epoch: 305\n",
      "Train loss: 0.277841717004776\n",
      "Test loss: 0.3143361806869507\n",
      "Epoch: 306\n",
      "Train loss: 0.27771902084350586\n",
      "Test loss: 0.31410712003707886\n",
      "Epoch: 307\n",
      "Train loss: 0.27759769558906555\n",
      "Test loss: 0.31388014554977417\n",
      "Epoch: 308\n",
      "Train loss: 0.2774776518344879\n",
      "Test loss: 0.31365519762039185\n",
      "Epoch: 309\n",
      "Train loss: 0.27735891938209534\n",
      "Test loss: 0.3134322762489319\n",
      "Epoch: 310\n",
      "Train loss: 0.2772413492202759\n",
      "Test loss: 0.3132113814353943\n",
      "Epoch: 311\n",
      "Train loss: 0.27712512016296387\n",
      "Test loss: 0.3129924535751343\n",
      "Epoch: 312\n",
      "Train loss: 0.27701011300086975\n",
      "Test loss: 0.31277555227279663\n",
      "Epoch: 313\n",
      "Train loss: 0.27689623832702637\n",
      "Test loss: 0.3125605285167694\n",
      "Epoch: 314\n",
      "Train loss: 0.27678367495536804\n",
      "Test loss: 0.3123474419116974\n",
      "Epoch: 315\n",
      "Train loss: 0.27667224407196045\n",
      "Test loss: 0.31213629245758057\n",
      "Epoch: 316\n",
      "Train loss: 0.276561975479126\n",
      "Test loss: 0.3119269907474518\n",
      "Epoch: 317\n",
      "Train loss: 0.2764528691768646\n",
      "Test loss: 0.31171953678131104\n",
      "Epoch: 318\n",
      "Train loss: 0.2763449549674988\n",
      "Test loss: 0.3115139901638031\n",
      "Epoch: 319\n",
      "Train loss: 0.2762380838394165\n",
      "Test loss: 0.31131020188331604\n",
      "Epoch: 320\n",
      "Train loss: 0.2761324346065521\n",
      "Test loss: 0.311108261346817\n",
      "Epoch: 321\n",
      "Train loss: 0.27602773904800415\n",
      "Test loss: 0.3109080195426941\n",
      "Epoch: 322\n",
      "Train loss: 0.2759242653846741\n",
      "Test loss: 0.3107096254825592\n",
      "Epoch: 323\n",
      "Train loss: 0.27582183480262756\n",
      "Test loss: 0.3105129301548004\n",
      "Epoch: 324\n",
      "Train loss: 0.27572038769721985\n",
      "Test loss: 0.3103179335594177\n",
      "Epoch: 325\n",
      "Train loss: 0.2756200134754181\n",
      "Test loss: 0.3101247251033783\n",
      "Epoch: 326\n",
      "Train loss: 0.2755207121372223\n",
      "Test loss: 0.3099330961704254\n",
      "Epoch: 327\n",
      "Train loss: 0.2754223644733429\n",
      "Test loss: 0.309743195772171\n",
      "Epoch: 328\n",
      "Train loss: 0.27532508969306946\n",
      "Test loss: 0.30955490469932556\n",
      "Epoch: 329\n",
      "Train loss: 0.2752287685871124\n",
      "Test loss: 0.3093683421611786\n",
      "Epoch: 330\n",
      "Train loss: 0.2751334309577942\n",
      "Test loss: 0.3091832399368286\n",
      "Epoch: 331\n",
      "Train loss: 0.27503907680511475\n",
      "Test loss: 0.30899983644485474\n",
      "Epoch: 332\n",
      "Train loss: 0.27494561672210693\n",
      "Test loss: 0.30881795287132263\n",
      "Epoch: 333\n",
      "Train loss: 0.2748531699180603\n",
      "Test loss: 0.3086376488208771\n",
      "Epoch: 334\n",
      "Train loss: 0.2747616469860077\n",
      "Test loss: 0.3084588646888733\n",
      "Epoch: 335\n",
      "Train loss: 0.2746710181236267\n",
      "Test loss: 0.30828168988227844\n",
      "Epoch: 336\n",
      "Train loss: 0.27458134293556213\n",
      "Test loss: 0.308106005191803\n",
      "Epoch: 337\n",
      "Train loss: 0.2744925320148468\n",
      "Test loss: 0.3079317808151245\n",
      "Epoch: 338\n",
      "Train loss: 0.2744046151638031\n",
      "Test loss: 0.30775901675224304\n",
      "Epoch: 339\n",
      "Train loss: 0.27431756258010864\n",
      "Test loss: 0.30758777260780334\n",
      "Epoch: 340\n",
      "Train loss: 0.2742313742637634\n",
      "Test loss: 0.30741792917251587\n",
      "Epoch: 341\n",
      "Train loss: 0.27414605021476746\n",
      "Test loss: 0.307249516248703\n",
      "Epoch: 342\n",
      "Train loss: 0.2740616202354431\n",
      "Test loss: 0.30708253383636475\n",
      "Epoch: 343\n",
      "Train loss: 0.27397796511650085\n",
      "Test loss: 0.3069169521331787\n",
      "Epoch: 344\n",
      "Train loss: 0.27389511466026306\n",
      "Test loss: 0.3067527711391449\n",
      "Epoch: 345\n",
      "Train loss: 0.2738131284713745\n",
      "Test loss: 0.3065899610519409\n",
      "Epoch: 346\n",
      "Train loss: 0.27373185753822327\n",
      "Test loss: 0.3064284920692444\n",
      "Epoch: 347\n",
      "Train loss: 0.27365148067474365\n",
      "Test loss: 0.3062683939933777\n",
      "Epoch: 348\n",
      "Train loss: 0.2735718786716461\n",
      "Test loss: 0.30610960721969604\n",
      "Epoch: 349\n",
      "Train loss: 0.2734929919242859\n",
      "Test loss: 0.30595213174819946\n",
      "Epoch: 350\n",
      "Train loss: 0.2734149396419525\n",
      "Test loss: 0.30579593777656555\n",
      "Epoch: 351\n",
      "Train loss: 0.27333757281303406\n",
      "Test loss: 0.3056410551071167\n",
      "Epoch: 352\n",
      "Train loss: 0.2732609510421753\n",
      "Test loss: 0.30548742413520813\n",
      "Epoch: 353\n",
      "Train loss: 0.2731851041316986\n",
      "Test loss: 0.30533507466316223\n",
      "Epoch: 354\n",
      "Train loss: 0.27310997247695923\n",
      "Test loss: 0.3051839768886566\n",
      "Epoch: 355\n",
      "Train loss: 0.27303555607795715\n",
      "Test loss: 0.3050341308116913\n",
      "Epoch: 356\n",
      "Train loss: 0.27296182513237\n",
      "Test loss: 0.30488547682762146\n",
      "Epoch: 357\n",
      "Train loss: 0.2728888690471649\n",
      "Test loss: 0.30473798513412476\n",
      "Epoch: 358\n",
      "Train loss: 0.27281653881073\n",
      "Test loss: 0.3045918047428131\n",
      "Epoch: 359\n",
      "Train loss: 0.27274492383003235\n",
      "Test loss: 0.3044467568397522\n",
      "Epoch: 360\n",
      "Train loss: 0.27267393469810486\n",
      "Test loss: 0.3043028712272644\n",
      "Epoch: 361\n",
      "Train loss: 0.2726036608219147\n",
      "Test loss: 0.30416011810302734\n",
      "Epoch: 362\n",
      "Train loss: 0.27253401279449463\n",
      "Test loss: 0.3040185272693634\n",
      "Epoch: 363\n",
      "Train loss: 0.2724649906158447\n",
      "Test loss: 0.3038780689239502\n",
      "Epoch: 364\n",
      "Train loss: 0.27239668369293213\n",
      "Test loss: 0.3037388026714325\n",
      "Epoch: 365\n",
      "Train loss: 0.2723290026187897\n",
      "Test loss: 0.30360057950019836\n",
      "Epoch: 366\n",
      "Train loss: 0.2722618579864502\n",
      "Test loss: 0.3034634292125702\n",
      "Epoch: 367\n",
      "Train loss: 0.272195428609848\n",
      "Test loss: 0.3033275008201599\n",
      "Epoch: 368\n",
      "Train loss: 0.2721295952796936\n",
      "Test loss: 0.30319246649742126\n",
      "Epoch: 369\n",
      "Train loss: 0.27206435799598694\n",
      "Test loss: 0.3030586540699005\n",
      "Epoch: 370\n",
      "Train loss: 0.271999716758728\n",
      "Test loss: 0.30292582511901855\n",
      "Epoch: 371\n",
      "Train loss: 0.2719356417655945\n",
      "Test loss: 0.30279403924942017\n",
      "Epoch: 372\n",
      "Train loss: 0.2718721628189087\n",
      "Test loss: 0.3026633560657501\n",
      "Epoch: 373\n",
      "Train loss: 0.27180930972099304\n",
      "Test loss: 0.3025336265563965\n",
      "Epoch: 374\n",
      "Train loss: 0.2717469334602356\n",
      "Test loss: 0.3024049401283264\n",
      "Epoch: 375\n",
      "Train loss: 0.2716851532459259\n",
      "Test loss: 0.30227726697921753\n",
      "Epoch: 376\n",
      "Train loss: 0.27162396907806396\n",
      "Test loss: 0.30215057730674744\n",
      "Epoch: 377\n",
      "Train loss: 0.2715632915496826\n",
      "Test loss: 0.30202487111091614\n",
      "Epoch: 378\n",
      "Train loss: 0.27150315046310425\n",
      "Test loss: 0.301900178194046\n",
      "Epoch: 379\n",
      "Train loss: 0.27144357562065125\n",
      "Test loss: 0.30177637934684753\n",
      "Epoch: 380\n",
      "Train loss: 0.27138447761535645\n",
      "Test loss: 0.30165359377861023\n",
      "Epoch: 381\n",
      "Train loss: 0.2713259756565094\n",
      "Test loss: 0.30153173208236694\n",
      "Epoch: 382\n",
      "Train loss: 0.27126798033714294\n",
      "Test loss: 0.3014107644557953\n",
      "Epoch: 383\n",
      "Train loss: 0.2712104320526123\n",
      "Test loss: 0.3012908101081848\n",
      "Epoch: 384\n",
      "Train loss: 0.27115345001220703\n",
      "Test loss: 0.3011717200279236\n",
      "Epoch: 385\n",
      "Train loss: 0.2710968852043152\n",
      "Test loss: 0.30105355381965637\n",
      "Epoch: 386\n",
      "Train loss: 0.2710408568382263\n",
      "Test loss: 0.3009362518787384\n",
      "Epoch: 387\n",
      "Train loss: 0.27098533511161804\n",
      "Test loss: 0.30081984400749207\n",
      "Epoch: 388\n",
      "Train loss: 0.2709302306175232\n",
      "Test loss: 0.30070436000823975\n",
      "Epoch: 389\n",
      "Train loss: 0.2708756625652313\n",
      "Test loss: 0.3005897104740143\n",
      "Epoch: 390\n",
      "Train loss: 0.27082154154777527\n",
      "Test loss: 0.30047595500946045\n",
      "Epoch: 391\n",
      "Train loss: 0.2707678973674774\n",
      "Test loss: 0.30036303400993347\n",
      "Epoch: 392\n",
      "Train loss: 0.270714670419693\n",
      "Test loss: 0.3002510070800781\n",
      "Epoch: 393\n",
      "Train loss: 0.2706619203090668\n",
      "Test loss: 0.30013975501060486\n",
      "Epoch: 394\n",
      "Train loss: 0.270609587430954\n",
      "Test loss: 0.30002930760383606\n",
      "Epoch: 395\n",
      "Train loss: 0.2705577611923218\n",
      "Test loss: 0.2999197840690613\n",
      "Epoch: 396\n",
      "Train loss: 0.2705063223838806\n",
      "Test loss: 0.29981106519699097\n",
      "Epoch: 397\n",
      "Train loss: 0.2704553008079529\n",
      "Test loss: 0.29970309138298035\n",
      "Epoch: 398\n",
      "Train loss: 0.2704046964645386\n",
      "Test loss: 0.2995959222316742\n",
      "Epoch: 399\n",
      "Train loss: 0.2703545391559601\n",
      "Test loss: 0.2994895577430725\n",
      "Epoch: 400\n",
      "Train loss: 0.2703048288822174\n",
      "Test loss: 0.2993839681148529\n",
      "Epoch: 401\n",
      "Train loss: 0.27025550603866577\n",
      "Test loss: 0.29927918314933777\n",
      "Epoch: 402\n",
      "Train loss: 0.2702065408229828\n",
      "Test loss: 0.29917511343955994\n",
      "Epoch: 403\n",
      "Train loss: 0.2701580226421356\n",
      "Test loss: 0.29907187819480896\n",
      "Epoch: 404\n",
      "Train loss: 0.2701098620891571\n",
      "Test loss: 0.2989693582057953\n",
      "Epoch: 405\n",
      "Train loss: 0.270062118768692\n",
      "Test loss: 0.29886752367019653\n",
      "Epoch: 406\n",
      "Train loss: 0.27001479268074036\n",
      "Test loss: 0.298766553401947\n",
      "Epoch: 407\n",
      "Train loss: 0.2699677646160126\n",
      "Test loss: 0.29866620898246765\n",
      "Epoch: 408\n",
      "Train loss: 0.2699211537837982\n",
      "Test loss: 0.29856663942337036\n",
      "Epoch: 409\n",
      "Train loss: 0.2698749303817749\n",
      "Test loss: 0.298467755317688\n",
      "Epoch: 410\n",
      "Train loss: 0.26982906460762024\n",
      "Test loss: 0.2983695864677429\n",
      "Epoch: 411\n",
      "Train loss: 0.26978355646133423\n",
      "Test loss: 0.29827216267585754\n",
      "Epoch: 412\n",
      "Train loss: 0.26973840594291687\n",
      "Test loss: 0.2981753647327423\n",
      "Epoch: 413\n",
      "Train loss: 0.2696935832500458\n",
      "Test loss: 0.29807931184768677\n",
      "Epoch: 414\n",
      "Train loss: 0.2696491479873657\n",
      "Test loss: 0.29798397421836853\n",
      "Epoch: 415\n",
      "Train loss: 0.26960504055023193\n",
      "Test loss: 0.29788926243782043\n",
      "Epoch: 416\n",
      "Train loss: 0.2695612907409668\n",
      "Test loss: 0.29779520630836487\n",
      "Epoch: 417\n",
      "Train loss: 0.2695178687572479\n",
      "Test loss: 0.2977018356323242\n",
      "Epoch: 418\n",
      "Train loss: 0.26947474479675293\n",
      "Test loss: 0.2976091504096985\n",
      "Epoch: 419\n",
      "Train loss: 0.269432008266449\n",
      "Test loss: 0.2975170612335205\n",
      "Epoch: 420\n",
      "Train loss: 0.2693895101547241\n",
      "Test loss: 0.29742562770843506\n",
      "Epoch: 421\n",
      "Train loss: 0.2693474292755127\n",
      "Test loss: 0.2973348796367645\n",
      "Epoch: 422\n",
      "Train loss: 0.26930561661720276\n",
      "Test loss: 0.29724472761154175\n",
      "Epoch: 423\n",
      "Train loss: 0.2692641019821167\n",
      "Test loss: 0.2971552014350891\n",
      "Epoch: 424\n",
      "Train loss: 0.2692229449748993\n",
      "Test loss: 0.29706627130508423\n",
      "Epoch: 425\n",
      "Train loss: 0.2691820561885834\n",
      "Test loss: 0.29697802662849426\n",
      "Epoch: 426\n",
      "Train loss: 0.2691415250301361\n",
      "Test loss: 0.29689034819602966\n",
      "Epoch: 427\n",
      "Train loss: 0.26910120248794556\n",
      "Test loss: 0.2968032658100128\n",
      "Epoch: 428\n",
      "Train loss: 0.26906126737594604\n",
      "Test loss: 0.2967168092727661\n",
      "Epoch: 429\n",
      "Train loss: 0.269021600484848\n",
      "Test loss: 0.29663094878196716\n",
      "Epoch: 430\n",
      "Train loss: 0.2689821720123291\n",
      "Test loss: 0.2965456545352936\n",
      "Epoch: 431\n",
      "Train loss: 0.26894304156303406\n",
      "Test loss: 0.29646095633506775\n",
      "Epoch: 432\n",
      "Train loss: 0.2689042389392853\n",
      "Test loss: 0.2963768243789673\n",
      "Epoch: 433\n",
      "Train loss: 0.2688656747341156\n",
      "Test loss: 0.2962932586669922\n",
      "Epoch: 434\n",
      "Train loss: 0.2688274085521698\n",
      "Test loss: 0.29621025919914246\n",
      "Epoch: 435\n",
      "Train loss: 0.2687894105911255\n",
      "Test loss: 0.2961278259754181\n",
      "Epoch: 436\n",
      "Train loss: 0.2687516510486603\n",
      "Test loss: 0.2960458993911743\n",
      "Epoch: 437\n",
      "Train loss: 0.26871415972709656\n",
      "Test loss: 0.2959645688533783\n",
      "Epoch: 438\n",
      "Train loss: 0.2686769962310791\n",
      "Test loss: 0.29588377475738525\n",
      "Epoch: 439\n",
      "Train loss: 0.26864001154899597\n",
      "Test loss: 0.29580357670783997\n",
      "Epoch: 440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.26860329508781433\n",
      "Test loss: 0.2957238256931305\n",
      "Epoch: 441\n",
      "Train loss: 0.26856690645217896\n",
      "Test loss: 0.2956446409225464\n",
      "Epoch: 442\n",
      "Train loss: 0.2685306966304779\n",
      "Test loss: 0.29556605219841003\n",
      "Epoch: 443\n",
      "Train loss: 0.26849472522735596\n",
      "Test loss: 0.2954878807067871\n",
      "Epoch: 444\n",
      "Train loss: 0.2684589922428131\n",
      "Test loss: 0.29541027545928955\n",
      "Epoch: 445\n",
      "Train loss: 0.26842355728149414\n",
      "Test loss: 0.2953331768512726\n",
      "Epoch: 446\n",
      "Train loss: 0.2683883309364319\n",
      "Test loss: 0.2952565550804138\n",
      "Epoch: 447\n",
      "Train loss: 0.26835331320762634\n",
      "Test loss: 0.29518043994903564\n",
      "Epoch: 448\n",
      "Train loss: 0.2683185636997223\n",
      "Test loss: 0.29510483145713806\n",
      "Epoch: 449\n",
      "Train loss: 0.26828405261039734\n",
      "Test loss: 0.29502972960472107\n",
      "Epoch: 450\n",
      "Train loss: 0.2682497203350067\n",
      "Test loss: 0.2949550747871399\n",
      "Epoch: 451\n",
      "Train loss: 0.2682156264781952\n",
      "Test loss: 0.2948809564113617\n",
      "Epoch: 452\n",
      "Train loss: 0.2681817412376404\n",
      "Test loss: 0.2948072850704193\n",
      "Epoch: 453\n",
      "Train loss: 0.26814812421798706\n",
      "Test loss: 0.29473409056663513\n",
      "Epoch: 454\n",
      "Train loss: 0.26811468601226807\n",
      "Test loss: 0.29466140270233154\n",
      "Epoch: 455\n",
      "Train loss: 0.26808154582977295\n",
      "Test loss: 0.2945891320705414\n",
      "Epoch: 456\n",
      "Train loss: 0.2680485248565674\n",
      "Test loss: 0.2945173382759094\n",
      "Epoch: 457\n",
      "Train loss: 0.26801571249961853\n",
      "Test loss: 0.29444602131843567\n",
      "Epoch: 458\n",
      "Train loss: 0.2679831087589264\n",
      "Test loss: 0.29437509179115295\n",
      "Epoch: 459\n",
      "Train loss: 0.26795071363449097\n",
      "Test loss: 0.2943046987056732\n",
      "Epoch: 460\n",
      "Train loss: 0.26791852712631226\n",
      "Test loss: 0.2942346930503845\n",
      "Epoch: 461\n",
      "Train loss: 0.26788651943206787\n",
      "Test loss: 0.2941651940345764\n",
      "Epoch: 462\n",
      "Train loss: 0.2678547501564026\n",
      "Test loss: 0.2940960228443146\n",
      "Epoch: 463\n",
      "Train loss: 0.26782315969467163\n",
      "Test loss: 0.2940273880958557\n",
      "Epoch: 464\n",
      "Train loss: 0.2677917778491974\n",
      "Test loss: 0.2939591705799103\n",
      "Epoch: 465\n",
      "Train loss: 0.2677605450153351\n",
      "Test loss: 0.29389137029647827\n",
      "Epoch: 466\n",
      "Train loss: 0.2677294909954071\n",
      "Test loss: 0.2938239574432373\n",
      "Epoch: 467\n",
      "Train loss: 0.26769864559173584\n",
      "Test loss: 0.29375696182250977\n",
      "Epoch: 468\n",
      "Train loss: 0.2676680088043213\n",
      "Test loss: 0.29369041323661804\n",
      "Epoch: 469\n",
      "Train loss: 0.2676374614238739\n",
      "Test loss: 0.29362425208091736\n",
      "Epoch: 470\n",
      "Train loss: 0.267607182264328\n",
      "Test loss: 0.2935585081577301\n",
      "Epoch: 471\n",
      "Train loss: 0.26757705211639404\n",
      "Test loss: 0.2934931814670563\n",
      "Epoch: 472\n",
      "Train loss: 0.2675471007823944\n",
      "Test loss: 0.2934282422065735\n",
      "Epoch: 473\n",
      "Train loss: 0.2675172686576843\n",
      "Test loss: 0.29336366057395935\n",
      "Epoch: 474\n",
      "Train loss: 0.26748770475387573\n",
      "Test loss: 0.2932995557785034\n",
      "Epoch: 475\n",
      "Train loss: 0.2674582302570343\n",
      "Test loss: 0.29323577880859375\n",
      "Epoch: 476\n",
      "Train loss: 0.2674289345741272\n",
      "Test loss: 0.29317235946655273\n",
      "Epoch: 477\n",
      "Train loss: 0.2673998773097992\n",
      "Test loss: 0.2931094169616699\n",
      "Epoch: 478\n",
      "Train loss: 0.26737087965011597\n",
      "Test loss: 0.293046772480011\n",
      "Epoch: 479\n",
      "Train loss: 0.26734206080436707\n",
      "Test loss: 0.2929845452308655\n",
      "Epoch: 480\n",
      "Train loss: 0.26731348037719727\n",
      "Test loss: 0.2929226756095886\n",
      "Epoch: 481\n",
      "Train loss: 0.26728498935699463\n",
      "Test loss: 0.2928611934185028\n",
      "Epoch: 482\n",
      "Train loss: 0.26725664734840393\n",
      "Test loss: 0.29280003905296326\n",
      "Epoch: 483\n",
      "Train loss: 0.26722845435142517\n",
      "Test loss: 0.29273930191993713\n",
      "Epoch: 484\n",
      "Train loss: 0.26720041036605835\n",
      "Test loss: 0.2926788628101349\n",
      "Epoch: 485\n",
      "Train loss: 0.26717257499694824\n",
      "Test loss: 0.29261887073516846\n",
      "Epoch: 486\n",
      "Train loss: 0.2671448290348053\n",
      "Test loss: 0.2925591468811035\n",
      "Epoch: 487\n",
      "Train loss: 0.2671172618865967\n",
      "Test loss: 0.292499840259552\n",
      "Epoch: 488\n",
      "Train loss: 0.2670897841453552\n",
      "Test loss: 0.292440801858902\n",
      "Epoch: 489\n",
      "Train loss: 0.2670624554157257\n",
      "Test loss: 0.2923821806907654\n",
      "Epoch: 490\n",
      "Train loss: 0.2670353353023529\n",
      "Test loss: 0.2923238277435303\n",
      "Epoch: 491\n",
      "Train loss: 0.26700830459594727\n",
      "Test loss: 0.2922658622264862\n",
      "Epoch: 492\n",
      "Train loss: 0.2669813930988312\n",
      "Test loss: 0.2922082543373108\n",
      "Epoch: 493\n",
      "Train loss: 0.2669546604156494\n",
      "Test loss: 0.29215094447135925\n",
      "Epoch: 494\n",
      "Train loss: 0.2669280469417572\n",
      "Test loss: 0.29209399223327637\n",
      "Epoch: 495\n",
      "Train loss: 0.26690152287483215\n",
      "Test loss: 0.29203730821609497\n",
      "Epoch: 496\n",
      "Train loss: 0.26687517762184143\n",
      "Test loss: 0.2919810116291046\n",
      "Epoch: 497\n",
      "Train loss: 0.26684895157814026\n",
      "Test loss: 0.29192495346069336\n",
      "Epoch: 498\n",
      "Train loss: 0.26682284474372864\n",
      "Test loss: 0.2918693423271179\n",
      "Epoch: 499\n",
      "Train loss: 0.26679685711860657\n",
      "Test loss: 0.2918139696121216\n"
     ]
    }
   ],
   "source": [
    " if __name__ == \"__main__\":\n",
    "    config = yaml.safe_load(open(\"config.yml\"))\n",
    "    run_toy(config['toy'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
