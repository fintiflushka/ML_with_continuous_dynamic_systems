{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from msalib import layers\n",
    "from msalib import network\n",
    "from msalib import train\n",
    "from msalib.utils import load_dataset\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(output, label):\n",
    "    \"\"\"Loss function (softmax cross entropy)\n",
    "\n",
    "    Arguments:\n",
    "        output {tf tensor} -- output from network\n",
    "        label {tf tensor} -- labels\n",
    "\n",
    "    Returns:\n",
    "        tf tensor -- loss\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.losses.softmax_cross_entropy(\n",
    "        logits=output, onehot_labels=label)\n",
    "\n",
    "\n",
    "def run_mnist(config):\n",
    "    \"\"\"Run mnist/fashion_mnist classification\n",
    "\n",
    "    Arguments:\n",
    "        config {dict} -- read from config.yml\n",
    "    \"\"\"\n",
    "\n",
    "    print('='*100)\n",
    "    print('Run CNN on MNIST')\n",
    "    print('='*100)\n",
    "    print('Config:')\n",
    "    for key, value in config.items():\n",
    "        print('{:20} ({})'.format(key, value))\n",
    "    print('='*100)\n",
    "\n",
    "    tf.logging.set_verbosity(\n",
    "        getattr(tf.logging, config['verbosity']))\n",
    "    tf.set_random_seed(config['seed'])\n",
    "\n",
    "    # Generate data\n",
    "    trainset, testset = load_dataset(\n",
    "        name=config['dataset_name'],\n",
    "        num_train=config['num_train'],\n",
    "        num_test=config['num_test'],\n",
    "        lift_dim=config['lift_dimension']\n",
    "    )\n",
    "\n",
    "    # Build network\n",
    "    input = tf.placeholder(\n",
    "        tf.float32, [None, 28, 28, config['lift_dimension']], name='input')\n",
    "    output = tf.placeholder(tf.float32, [None, 10], name='output')\n",
    "    net = network.Network(name='msa_net')\n",
    "\n",
    "    net.add(layers.Conv2D(\n",
    "        input_shape=input.shape[1:],\n",
    "        filters=config['num_channels'],\n",
    "        kernel_size=config['filter_size'],\n",
    "        padding=config['padding'],\n",
    "        activation=config['activation'],\n",
    "        msa_rho=config['rho'],\n",
    "        msa_reg=config['reg'],\n",
    "        name='conv2d_0')\n",
    "    )\n",
    "\n",
    "    net.add(layers.AveragePooling2D(\n",
    "        pool_size=2, strides=2, padding=config['padding'], name='avg_pool_0')\n",
    "    )\n",
    "\n",
    "    net.add(layers.Conv2D(\n",
    "        input_shape=input.shape[1:],\n",
    "        filters=config['num_channels'],\n",
    "        kernel_size=config['filter_size'],\n",
    "        padding=config['padding'],\n",
    "        activation=config['activation'],\n",
    "        msa_rho=config['rho'],\n",
    "        msa_reg=config['reg'],\n",
    "        name='conv2d_init')\n",
    "    )\n",
    "\n",
    "    net.add(layers.AveragePooling2D(\n",
    "        pool_size=2, strides=2, padding=config['padding'], name='avg_pool_1')\n",
    "    )\n",
    "\n",
    "    for n in range(config['num_layers']):\n",
    "        net.add(layers.ResidualConv2D(\n",
    "            filters=config['num_channels'],\n",
    "            kernel_size=config['filter_size'],\n",
    "            padding=config['padding'],\n",
    "            activation=config['activation'],\n",
    "            msa_rho=config['rho'],\n",
    "            msa_reg=config['reg'],\n",
    "            delta=config['delta'],\n",
    "            name='residualconv2d_{}'.format(n)))\n",
    "\n",
    "    net.add(layers.Lower(name='lower'))\n",
    "    net.add(layers.Flatten(name='flatten'))\n",
    "    net.add(layers.Dense(\n",
    "        units=10, msa_rho=config['rho'], msa_reg=config['reg'], name='dense'))\n",
    "\n",
    "    net.msa_compute_x(input)\n",
    "    net.msa_compute_p(output, loss_func)\n",
    "    net.summary()\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # MSA trainer\n",
    "    msa_trainer = train.MSATrainer(\n",
    "        network=net,\n",
    "        name='MSA_trainer',\n",
    "        maxiter=config['msa_maxiter'],\n",
    "        perturb_init=config['msa_perturb_init'])\n",
    "    msa_trainer.initialize(sess)\n",
    "    msa_trainer.train(\n",
    "        session=sess,\n",
    "        trainset=trainset,\n",
    "        testset=testset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_epochs=config['num_epochs'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        print_step=config['print_step'])\n",
    "    '''\n",
    "    # SGD trainer\n",
    "    sgd_trainer = train.BPTrainer(\n",
    "        network=net, name='SGD_trainer',\n",
    "        method='GradientDescentOptimizer',\n",
    "        args={'learning_rate': config['sgd_lr']})\n",
    "    sgd_trainer.initialize(sess)\n",
    "    sgd_trainer.train(\n",
    "        session=sess,\n",
    "        trainset=trainset,\n",
    "        testset=testset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_epochs=config['num_epochs'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        print_step=config['print_step'])\n",
    "\n",
    "    # Adagrad trainer\n",
    "    adagrad_trainer = train.BPTrainer(\n",
    "        network=net, name='Adagrad_trainer',\n",
    "        method='AdagradOptimizer',\n",
    "        args={'learning_rate': config['adagrad_lr']})\n",
    "    adagrad_trainer.initialize(sess)\n",
    "    adagrad_trainer.train(\n",
    "        session=sess,\n",
    "        trainset=trainset,\n",
    "        testset=testset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_epochs=config['num_epochs'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        print_step=config['print_step'])\n",
    "\n",
    "    # Adam trainer\n",
    "    Adam_trainer = train.BPTrainer(\n",
    "        network=net, name='Adam_trainer',\n",
    "        method='AdamOptimizer',\n",
    "        args={'learning_rate': config['adam_lr']})\n",
    "    Adam_trainer.initialize(sess)\n",
    "    Adam_trainer.train(\n",
    "        session=sess,\n",
    "        trainset=trainset,\n",
    "        testset=testset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_epochs=config['num_epochs'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        print_step=config['print_step'])\n",
    "      '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "====================================================================================================\n",
      "Run CNN on MNIST\n",
      "====================================================================================================\n",
      "Config:\n",
      "dataset_name         (mnist)\n",
      "seed                 (1)\n",
      "verbosity            (WARN)\n",
      "print_step           (100)\n",
      "lift_dimension       (1)\n",
      "num_train            (60000)\n",
      "num_test             (10000)\n",
      "num_channels         (5)\n",
      "filter_size          (3)\n",
      "padding              (same)\n",
      "num_layers           (2)\n",
      "rho                  (0.03)\n",
      "reg                  (0.001)\n",
      "activation           (tanh)\n",
      "delta                (0.5)\n",
      "batch_size           (128)\n",
      "buffer_size          (512)\n",
      "num_epochs           (1)\n",
      "sgd_lr               (0.01)\n",
      "adagrad_lr           (0.01)\n",
      "adam_lr              (0.001)\n",
      "msa_maxiter          (10)\n",
      "msa_perturb_init     (0.0)\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_0 (Conv2D)            (None, 28, 28, 5)         50        \n",
      "_________________________________________________________________\n",
      "avg_pool_0 (AveragePooling2D (None, 14, 14, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_init (Conv2D)         (None, 14, 14, 5)         230       \n",
      "_________________________________________________________________\n",
      "avg_pool_1 (AveragePooling2D (None, 7, 7, 5)           0         \n",
      "_________________________________________________________________\n",
      "residualconv2d_0 (ResidualCo (None, 7, 7, 5)           230       \n",
      "_________________________________________________________________\n",
      "residualconv2d_1 (ResidualCo (None, 7, 7, 5)           230       \n",
      "_________________________________________________________________\n",
      "lower (Lower)                (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                500       \n",
      "=================================================================\n",
      "Total params: 1,240\n",
      "Trainable params: 1,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2019-05-29 14:51:15.984219: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-05-29 14:51:16.638001: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2401000000 Hz\n",
      "2019-05-29 14:51:16.714261: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3dbf6d0 executing computations on platform Host. Devices:\n",
      "2019-05-29 14:51:16.714414: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "====================================================================================================\n",
      "Trainer: MSATrainer (MSA_trainer)\n",
      "Settings: {'maxiter': 10, 'perturb_init': 0.0}\n",
      "====================================================================================================\n",
      "Epoch: init\n",
      "Train loss: 2.3245041087522345\n",
      "Test loss: 2.3243458390235903\n",
      "Step     0 of   469: \n",
      "Train loss: 2.251601083803985\n",
      "Test loss: 2.251721012390266\n",
      "Step     5 of   469: \n",
      "Train loss: 1.538198397321216\n",
      "Test loss: 1.5384017582667076\n",
      "Step    10 of   469: \n",
      "Train loss: 1.1045078256372678\n",
      "Test loss: 1.105470321946225\n",
      "Step    15 of   469: \n",
      "Train loss: 0.9116388513880261\n",
      "Test loss: 0.9115235062979036\n",
      "Step    20 of   469: \n",
      "Train loss: 0.8169570423788943\n",
      "Test loss: 0.8171625869759058\n",
      "Step    25 of   469: \n",
      "Train loss: 0.7602604875120066\n",
      "Test loss: 0.7607012547678866\n",
      "Step    30 of   469: \n",
      "Train loss: 0.7229184611874112\n",
      "Test loss: 0.7237023216182903\n",
      "Step    35 of   469: \n",
      "Train loss: 0.6881335943432177\n",
      "Test loss: 0.6881960241471307\n",
      "Step    40 of   469: \n",
      "Train loss: 0.6703700312113358\n",
      "Test loss: 0.6701718917337515\n",
      "Step    45 of   469: \n",
      "Train loss: 0.6550580712698274\n",
      "Test loss: 0.655356424592309\n",
      "Step    50 of   469: \n",
      "Train loss: 0.6388610050839892\n",
      "Test loss: 0.6396449862395303\n",
      "Step    55 of   469: \n",
      "Train loss: 0.6270352091829655\n",
      "Test loss: 0.626168093944\n",
      "Step    60 of   469: \n",
      "Train loss: 0.6185165355771275\n",
      "Test loss: 0.6175281046810797\n",
      "Step    65 of   469: \n",
      "Train loss: 0.6108651140988883\n",
      "Test loss: 0.6108085874278667\n",
      "Step    70 of   469: \n",
      "Train loss: 0.6012003164170152\n",
      "Test loss: 0.5996275445667364\n",
      "Step    75 of   469: \n",
      "Train loss: 0.5895808399733851\n",
      "Test loss: 0.5888501579983759\n",
      "Step    80 of   469: \n",
      "Train loss: 0.5822208718223086\n",
      "Test loss: 0.5819709429801521\n",
      "Step    85 of   469: \n",
      "Train loss: 0.57622456777904\n",
      "Test loss: 0.5751379854598287\n",
      "Step    90 of   469: \n",
      "Train loss: 0.5683031243793035\n",
      "Test loss: 0.5689358696088953\n",
      "Step    95 of   469: \n",
      "Train loss: 0.5656674807354555\n",
      "Test loss: 0.566170269907531\n",
      "Step   100 of   469: \n",
      "Train loss: 0.5614590576644671\n",
      "Test loss: 0.5604840918617734\n",
      "Step   105 of   469: \n",
      "Train loss: 0.5524259277824628\n",
      "Test loss: 0.5533440001940323\n",
      "Step   110 of   469: \n",
      "Train loss: 0.549295807793989\n",
      "Test loss: 0.5498651670197309\n",
      "Step   115 of   469: \n",
      "Train loss: 0.5453842188847267\n",
      "Test loss: 0.5452207978499137\n",
      "Step   120 of   469: \n",
      "Train loss: 0.5416017548512604\n",
      "Test loss: 0.5415284158819813\n",
      "Step   125 of   469: \n",
      "Train loss: 0.5395527194112034\n",
      "Test loss: 0.5401004906428062\n",
      "Step   130 of   469: \n",
      "Train loss: 0.536581898139695\n",
      "Test loss: 0.5371723740787829\n",
      "Step   135 of   469: \n",
      "Train loss: 0.5358356399051214\n",
      "Test loss: 0.5367558280290183\n",
      "Step   140 of   469: \n",
      "Train loss: 0.53350989672087\n",
      "Test loss: 0.5341045811014661\n",
      "Step   145 of   469: \n",
      "Train loss: 0.5312683400461229\n",
      "Test loss: 0.5307194545107373\n",
      "Step   150 of   469: \n",
      "Train loss: 0.5293321927725259\n",
      "Test loss: 0.5296888924756292\n",
      "Step   155 of   469: \n",
      "Train loss: 0.5258042132955486\n",
      "Test loss: 0.5259975553569147\n",
      "Step   160 of   469: \n",
      "Train loss: 0.5205366586224508\n",
      "Test loss: 0.5212191078117339\n",
      "Step   165 of   469: \n",
      "Train loss: 0.5179164149498535\n",
      "Test loss: 0.5191225563570604\n",
      "Step   170 of   469: \n",
      "Train loss: 0.5170056228920564\n",
      "Test loss: 0.5177573299003859\n",
      "Step   175 of   469: \n",
      "Train loss: 0.5123911772744131\n",
      "Test loss: 0.5131419664722378\n",
      "Step   180 of   469: \n",
      "Train loss: 0.5122841960292751\n",
      "Test loss: 0.5117960627806388\n",
      "Step   185 of   469: \n",
      "Train loss: 0.50749823552067\n",
      "Test loss: 0.5078324243678884\n",
      "Step   190 of   469: \n",
      "Train loss: 0.5075498392521325\n",
      "Test loss: 0.5078902840614319\n",
      "Step   195 of   469: \n",
      "Train loss: 0.5041583672923556\n",
      "Test loss: 0.5046973377466202\n",
      "Step   200 of   469: \n",
      "Train loss: 0.5033667852817956\n",
      "Test loss: 0.5030495468842782\n",
      "Step   205 of   469: \n",
      "Train loss: 0.5029358262732878\n",
      "Test loss: 0.5018799355474569\n",
      "Step   210 of   469: \n",
      "Train loss: 0.499953619504379\n",
      "Test loss: 0.5008525562993551\n",
      "Step   215 of   469: \n",
      "Train loss: 0.49840809404850006\n",
      "Test loss: 0.49848284686015826\n",
      "Step   220 of   469: \n",
      "Train loss: 0.49659523974030706\n",
      "Test loss: 0.49688278669017855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   225 of   469: \n",
      "Train loss: 0.4957673425896693\n",
      "Test loss: 0.4958732517088874\n",
      "Step   230 of   469: \n",
      "Train loss: 0.49451214807518457\n",
      "Test loss: 0.49457962204844264\n",
      "Step   235 of   469: \n",
      "Train loss: 0.4921166934704376\n",
      "Test loss: 0.4927618523775521\n",
      "Step   240 of   469: \n",
      "Train loss: 0.4926631066758754\n",
      "Test loss: 0.4924122989177704\n",
      "Step   245 of   469: \n",
      "Train loss: 0.49086415515107623\n",
      "Test loss: 0.4904660663362277\n",
      "Step   250 of   469: \n",
      "Train loss: 0.4899554462251017\n",
      "Test loss: 0.4909431474693751\n",
      "Step   255 of   469: \n",
      "Train loss: 0.4888014793395996\n",
      "Test loss: 0.4894995843454943\n",
      "Step   260 of   469: \n",
      "Train loss: 0.4867824760533996\n",
      "Test loss: 0.48697429668095155\n",
      "Step   265 of   469: \n",
      "Train loss: 0.48523149753020983\n",
      "Test loss: 0.485996803489782\n",
      "Step   270 of   469: \n",
      "Train loss: 0.48493501392461485\n",
      "Test loss: 0.485340859677832\n",
      "Step   275 of   469: \n",
      "Train loss: 0.48269837690612016\n",
      "Test loss: 0.4830962687225665\n",
      "Step   280 of   469: \n",
      "Train loss: 0.48278380273762395\n",
      "Test loss: 0.4830165624113406\n",
      "Step   285 of   469: \n",
      "Train loss: 0.47917611720198294\n",
      "Test loss: 0.48061419972928904\n",
      "Step   290 of   469: \n",
      "Train loss: 0.4789215942560616\n",
      "Test loss: 0.4798034980135449\n",
      "Step   295 of   469: \n",
      "Train loss: 0.47815631809881176\n",
      "Test loss: 0.47815631809881176\n",
      "Step   300 of   469: \n",
      "Train loss: 0.4785119479490539\n",
      "Test loss: 0.478602801591663\n",
      "Step   305 of   469: \n",
      "Train loss: 0.47610038215831174\n",
      "Test loss: 0.47677513275106076\n",
      "Step   310 of   469: \n",
      "Train loss: 0.475517964716685\n",
      "Test loss: 0.47593226776284686\n",
      "Step   315 of   469: \n",
      "Train loss: 0.4739085259073872\n",
      "Test loss: 0.474296631701922\n",
      "Step   320 of   469: \n",
      "Train loss: 0.47122089337494416\n",
      "Test loss: 0.4714904500771377\n",
      "Step   325 of   469: \n",
      "Train loss: 0.47025581386129733\n",
      "Test loss: 0.47092799463514556\n",
      "Step   330 of   469: \n",
      "Train loss: 0.46943571001796397\n",
      "Test loss: 0.4699709097207603\n",
      "Step   335 of   469: \n",
      "Train loss: 0.46925336046744204\n",
      "Test loss: 0.4693315130169109\n",
      "Step   340 of   469: \n",
      "Train loss: 0.4684502017700066\n",
      "Test loss: 0.4687381235219665\n",
      "Step   345 of   469: \n",
      "Train loss: 0.46619904091802694\n",
      "Test loss: 0.46709389959351494\n",
      "Step   350 of   469: \n",
      "Train loss: 0.4669332271915371\n",
      "Test loss: 0.46598587475590786\n",
      "Step   355 of   469: \n",
      "Train loss: 0.4662953832391965\n",
      "Test loss: 0.46567575052633126\n",
      "Step   360 of   469: \n",
      "Train loss: 0.46498251112840944\n",
      "Test loss: 0.4652663575390638\n",
      "Step   365 of   469: \n",
      "Train loss: 0.4652283600831436\n",
      "Test loss: 0.4641418451980009\n",
      "Step   370 of   469: \n",
      "Train loss: 0.46274267591662327\n",
      "Test loss: 0.4622818289671914\n",
      "Step   375 of   469: \n",
      "Train loss: 0.46155323012400484\n",
      "Test loss: 0.46165141968403833\n",
      "Step   380 of   469: \n",
      "Train loss: 0.46112854551460786\n",
      "Test loss: 0.4606031853768785\n",
      "Step   385 of   469: \n",
      "Train loss: 0.4595914260815766\n",
      "Test loss: 0.4602083389536809\n",
      "Step   390 of   469: \n",
      "Train loss: 0.4593777335801367\n",
      "Test loss: 0.4600374501640514\n",
      "Step   395 of   469: \n",
      "Train loss: 0.4602017832004418\n",
      "Test loss: 0.4594078437756684\n",
      "Step   400 of   469: \n",
      "Train loss: 0.45780804121898394\n",
      "Test loss: 0.45871366459434315\n",
      "Step   405 of   469: \n",
      "Train loss: 0.4587490232819218\n",
      "Test loss: 0.45935172473980207\n",
      "Step   410 of   469: \n",
      "Train loss: 0.45828415099847114\n",
      "Test loss: 0.45837040321301603\n",
      "Step   415 of   469: \n",
      "Train loss: 0.456847216113139\n",
      "Test loss: 0.4569267792216802\n",
      "Step   420 of   469: \n",
      "Train loss: 0.45699987143783244\n",
      "Test loss: 0.45767841500751044\n",
      "Step   425 of   469: \n",
      "Train loss: 0.4565364892705012\n",
      "Test loss: 0.45726083320076183\n",
      "Step   430 of   469: \n",
      "Train loss: 0.45677901400347887\n",
      "Test loss: 0.45778694622597454\n",
      "Step   435 of   469: \n",
      "Train loss: 0.45601762603905244\n",
      "Test loss: 0.45676275658405435\n",
      "Step   440 of   469: \n",
      "Train loss: 0.4568202649637804\n",
      "Test loss: 0.4563003331422806\n",
      "Step   445 of   469: \n",
      "Train loss: 0.45637368448710036\n",
      "Test loss: 0.4567328658649477\n",
      "Step   450 of   469: \n",
      "Train loss: 0.4552969255689847\n",
      "Test loss: 0.4554555650989888\n",
      "Step   455 of   469: \n",
      "Train loss: 0.4527509270583169\n",
      "Test loss: 0.4534348673739676\n",
      "Step   460 of   469: \n",
      "Train loss: 0.451768976399454\n",
      "Test loss: 0.45282557101572973\n",
      "Step   465 of   469: \n",
      "Train loss: 0.45181818634776744\n",
      "Test loss: 0.4525002675541377\n",
      "Epoch: 0\n",
      "Train loss: 0.45064935138670065\n",
      "Test loss: 0.4378460422158241\n",
      "====================================================================================================\n",
      "Trainer: BPTrainer (SGD_trainer)\n",
      "Settings: {'method': 'GradientDescentOptimizer', 'args': {'learning_rate': 0.01}}\n",
      "====================================================================================================\n",
      "Epoch: init\n",
      "Train loss: 2.3143587758985618\n",
      "Test loss: 2.314602828025818\n",
      "Step     0 of   469: \n",
      "Train loss: 2.313755956746764\n",
      "Test loss: 2.31391594773632\n",
      "Step     5 of   469: \n",
      "Train loss: 2.311933262873504\n",
      "Test loss: 2.3120124218827587\n",
      "Step    10 of   469: \n",
      "Train loss: 2.3102579258256037\n",
      "Test loss: 2.3104115179029563\n",
      "Step    15 of   469: \n",
      "Train loss: 2.3080580719446733\n",
      "Test loss: 2.3081699771396185\n",
      "Step    20 of   469: \n",
      "Train loss: 2.306690579753811\n",
      "Test loss: 2.3068675792823403\n",
      "Step    25 of   469: \n",
      "Train loss: 2.3052960755461354\n",
      "Test loss: 2.305397690352747\n",
      "Step    30 of   469: \n",
      "Train loss: 2.3036481364298673\n",
      "Test loss: 2.303734991510036\n",
      "Step    35 of   469: \n",
      "Train loss: 2.302221350750681\n",
      "Test loss: 2.3023872011798923\n",
      "Step    40 of   469: \n",
      "Train loss: 2.3011280197208213\n",
      "Test loss: 2.3012316812903193\n",
      "Step    45 of   469: \n",
      "Train loss: 2.300315531633668\n",
      "Test loss: 2.3004771268973916\n",
      "Step    50 of   469: \n",
      "Train loss: 2.299068784309646\n",
      "Test loss: 2.2991938813258024\n",
      "Step    55 of   469: \n",
      "Train loss: 2.2979861942388244\n",
      "Test loss: 2.298112303523694\n",
      "Step    60 of   469: \n",
      "Train loss: 2.296471605866642\n",
      "Test loss: 2.296649520680056\n",
      "Step    65 of   469: \n",
      "Train loss: 2.2954655481597124\n",
      "Test loss: 2.295597642155017\n",
      "Step    70 of   469: \n",
      "Train loss: 2.294330291828867\n",
      "Test loss: 2.294506945852506\n",
      "Step    75 of   469: \n",
      "Train loss: 2.293169146877224\n",
      "Test loss: 2.293405191373017\n",
      "Step    80 of   469: \n",
      "Train loss: 2.2922173091920754\n",
      "Test loss: 2.2924105292659696\n",
      "Step    85 of   469: \n",
      "Train loss: 2.2910110324116078\n",
      "Test loss: 2.291183904065924\n",
      "Step    90 of   469: \n",
      "Train loss: 2.2899290262642555\n",
      "Test loss: 2.2901546470189498\n",
      "Step    95 of   469: \n",
      "Train loss: 2.2886731685218162\n",
      "Test loss: 2.2888800229056407\n",
      "Step   100 of   469: \n",
      "Train loss: 2.2875979835704223\n",
      "Test loss: 2.287806781671815\n",
      "Step   105 of   469: \n",
      "Train loss: 2.2864041085970603\n",
      "Test loss: 2.286589709378905\n",
      "Step   110 of   469: \n",
      "Train loss: 2.284984170380285\n",
      "Test loss: 2.2852716890432068\n",
      "Step   115 of   469: \n",
      "Train loss: 2.2835264933311334\n",
      "Test loss: 2.283830276990341\n",
      "Step   120 of   469: \n",
      "Train loss: 2.282597859027022\n",
      "Test loss: 2.282597859027022\n",
      "Step   125 of   469: \n",
      "Train loss: 2.2812721082719705\n",
      "Test loss: 2.281470822075666\n",
      "Step   130 of   469: \n",
      "Train loss: 2.279813542204388\n",
      "Test loss: 2.2800938941664617\n",
      "Step   135 of   469: \n",
      "Train loss: 2.278559678691929\n",
      "Test loss: 2.278755337505017\n",
      "Step   140 of   469: \n",
      "Train loss: 2.2771851026405723\n",
      "Test loss: 2.2774049730624184\n",
      "Step   145 of   469: \n",
      "Train loss: 2.2758373123104287\n",
      "Test loss: 2.27607085947263\n",
      "Step   150 of   469: \n",
      "Train loss: 2.2744067826513517\n",
      "Test loss: 2.274635122994245\n",
      "Step   155 of   469: \n",
      "Train loss: 2.2732731487791416\n",
      "Test loss: 2.2735280970395624\n",
      "Step   160 of   469: \n",
      "Train loss: 2.2720004926293584\n",
      "Test loss: 2.272222941204653\n",
      "Step   165 of   469: \n",
      "Train loss: 2.2706597558522628\n",
      "Test loss: 2.2709154375528886\n",
      "Step   170 of   469: \n",
      "Train loss: 2.2691988540908037\n",
      "Test loss: 2.269481378086543\n",
      "Step   175 of   469: \n",
      "Train loss: 2.2676992921505943\n",
      "Test loss: 2.2679651652352284\n",
      "Step   180 of   469: \n",
      "Train loss: 2.266328468161114\n",
      "Test loss: 2.266563878221027\n",
      "Step   185 of   469: \n",
      "Train loss: 2.2650302143420205\n",
      "Test loss: 2.2653523259243724\n",
      "Step   190 of   469: \n",
      "Train loss: 2.2639244956485296\n",
      "Test loss: 2.26411448898962\n",
      "Step   195 of   469: \n",
      "Train loss: 2.2624231516304665\n",
      "Test loss: 2.2626688197507696\n",
      "Step   200 of   469: \n",
      "Train loss: 2.26091944161108\n",
      "Test loss: 2.261167875791\n",
      "Step   205 of   469: \n",
      "Train loss: 2.2593671810829035\n",
      "Test loss: 2.2596068948002186\n",
      "Step   210 of   469: \n",
      "Train loss: 2.2577103154133944\n",
      "Test loss: 2.2579838744664595\n",
      "Step   215 of   469: \n",
      "Train loss: 2.256099781747592\n",
      "Test loss: 2.256438142162258\n",
      "Step   220 of   469: \n",
      "Train loss: 2.2546715615159374\n",
      "Test loss: 2.25492908590931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   225 of   469: \n",
      "Train loss: 2.2524853197194763\n",
      "Test loss: 2.2529329283762785\n",
      "Step   230 of   469: \n",
      "Train loss: 2.2511534711061896\n",
      "Test loss: 2.251514822749768\n",
      "Step   235 of   469: \n",
      "Train loss: 2.249378614506479\n",
      "Test loss: 2.2496854733612577\n",
      "Step   240 of   469: \n",
      "Train loss: 2.2476421796669395\n",
      "Test loss: 2.2479893777330044\n",
      "Step   245 of   469: \n",
      "Train loss: 2.2456171775268294\n",
      "Test loss: 2.2459436412584983\n",
      "Step   250 of   469: \n",
      "Train loss: 2.24367604417316\n",
      "Test loss: 2.2440550327301025\n",
      "Step   255 of   469: \n",
      "Train loss: 2.241596500752336\n",
      "Test loss: 2.2421015985941484\n",
      "Step   260 of   469: \n",
      "Train loss: 2.239576121508065\n",
      "Test loss: 2.2398870819706027\n",
      "Step   265 of   469: \n",
      "Train loss: 2.2373437881469727\n",
      "Test loss: 2.237594143819001\n",
      "Step   270 of   469: \n",
      "Train loss: 2.2353461617130344\n",
      "Test loss: 2.2357260291859253\n",
      "Step   275 of   469: \n",
      "Train loss: 2.2334859290365445\n",
      "Test loss: 2.2338335473658675\n",
      "Step   280 of   469: \n",
      "Train loss: 2.231290724317906\n",
      "Test loss: 2.2316262418940918\n",
      "Step   285 of   469: \n",
      "Train loss: 2.228906091997179\n",
      "Test loss: 2.229163190065804\n",
      "Step   290 of   469: \n",
      "Train loss: 2.2267276594194314\n",
      "Test loss: 2.2269920050087624\n",
      "Step   295 of   469: \n",
      "Train loss: 2.224477422439446\n",
      "Test loss: 2.2248920040615534\n",
      "Step   300 of   469: \n",
      "Train loss: 2.2220454640307667\n",
      "Test loss: 2.22240914126574\n",
      "Step   305 of   469: \n",
      "Train loss: 2.2197126049106406\n",
      "Test loss: 2.2201503010119423\n",
      "Step   310 of   469: \n",
      "Train loss: 2.2168324155322576\n",
      "Test loss: 2.217293826200194\n",
      "Step   315 of   469: \n",
      "Train loss: 2.2140969098624534\n",
      "Test loss: 2.2146344588974776\n",
      "Step   320 of   469: \n",
      "Train loss: 2.2118934190879433\n",
      "Test loss: 2.2122027389073775\n",
      "Step   325 of   469: \n",
      "Train loss: 2.209325279219676\n",
      "Test loss: 2.2096243733066623\n",
      "Step   330 of   469: \n",
      "Train loss: 2.2062639826435153\n",
      "Test loss: 2.206745087090185\n",
      "Step   335 of   469: \n",
      "Train loss: 2.203619108361713\n",
      "Test loss: 2.2039303092633262\n",
      "Step   340 of   469: \n",
      "Train loss: 2.2007272142474936\n",
      "Test loss: 2.201141876689458\n",
      "Step   345 of   469: \n",
      "Train loss: 2.197915517677695\n",
      "Test loss: 2.1983249248084373\n",
      "Step   350 of   469: \n",
      "Train loss: 2.1949032423859935\n",
      "Test loss: 2.195281529830674\n",
      "Step   355 of   469: \n",
      "Train loss: 2.1914172253366244\n",
      "Test loss: 2.191914606902559\n",
      "Step   360 of   469: \n",
      "Train loss: 2.1887569568924983\n",
      "Test loss: 2.189172009290275\n",
      "Step   365 of   469: \n",
      "Train loss: 2.18556829832368\n",
      "Test loss: 2.1860293695482156\n",
      "Step   370 of   469: \n",
      "Train loss: 2.181979631973525\n",
      "Test loss: 2.182570255408853\n",
      "Step   375 of   469: \n",
      "Train loss: 2.1788906667192105\n",
      "Test loss: 2.1794865757732067\n",
      "Step   380 of   469: \n",
      "Train loss: 2.1754692590842812\n",
      "Test loss: 2.1758971820443365\n",
      "Step   385 of   469: \n",
      "Train loss: 2.1717776080309332\n",
      "Test loss: 2.1723671060497476\n",
      "Step   390 of   469: \n",
      "Train loss: 2.167933264021146\n",
      "Test loss: 2.168519450446307\n",
      "Step   395 of   469: \n",
      "Train loss: 2.164017877336276\n",
      "Test loss: 2.1645497629197976\n",
      "Step   400 of   469: \n",
      "Train loss: 2.1600679522853787\n",
      "Test loss: 2.16055550211567\n",
      "Step   405 of   469: \n",
      "Train loss: 2.155442615686837\n",
      "Test loss: 2.156178048101522\n",
      "Step   410 of   469: \n",
      "Train loss: 2.150409492395692\n",
      "Test loss: 2.151278542259992\n",
      "Step   415 of   469: \n",
      "Train loss: 2.1470755217439037\n",
      "Test loss: 2.1470755217439037\n",
      "Step   420 of   469: \n",
      "Train loss: 2.1427307613825395\n",
      "Test loss: 2.143235776383998\n",
      "Step   425 of   469: \n",
      "Train loss: 2.1377546100293174\n",
      "Test loss: 2.138436996330649\n",
      "Step   430 of   469: \n",
      "Train loss: 2.133193490868908\n",
      "Test loss: 2.1337194079059665\n",
      "Step   435 of   469: \n",
      "Train loss: 2.128155409279516\n",
      "Test loss: 2.1287945064447693\n",
      "Step   440 of   469: \n",
      "Train loss: 2.1232377129085993\n",
      "Test loss: 2.1239276740510586\n",
      "Step   445 of   469: \n",
      "Train loss: 2.118397528842344\n",
      "Test loss: 2.119013687311593\n",
      "Step   450 of   469: \n",
      "Train loss: 2.113790758585526\n",
      "Test loss: 2.114461335085206\n",
      "Step   455 of   469: \n",
      "Train loss: 2.1087417905613526\n",
      "Test loss: 2.109373440176754\n",
      "Step   460 of   469: \n",
      "Train loss: 2.1035856392423984\n",
      "Test loss: 2.10426476850348\n",
      "Step   465 of   469: \n",
      "Train loss: 2.0979882882813277\n",
      "Test loss: 2.0987647024251648\n",
      "Epoch: 0\n",
      "Train loss: 2.094737004425566\n",
      "Test loss: 2.089696264266968\n",
      "====================================================================================================\n",
      "Trainer: BPTrainer (Adagrad_trainer)\n",
      "Settings: {'method': 'AdagradOptimizer', 'args': {'learning_rate': 0.01}}\n",
      "====================================================================================================\n",
      "Epoch: init\n",
      "Train loss: 2.3287912102068886\n",
      "Test loss: 2.328141474723816\n",
      "Step     0 of   469: \n",
      "Train loss: 2.328255916045884\n",
      "Test loss: 2.3283319493471564\n",
      "Step     5 of   469: \n",
      "Train loss: 2.3266726813073886\n",
      "Test loss: 2.3267209711721386\n",
      "Step    10 of   469: \n",
      "Train loss: 2.3249758744643905\n",
      "Test loss: 2.3250478869777615\n",
      "Step    15 of   469: \n",
      "Train loss: 2.323637493586136\n",
      "Test loss: 2.3237714403766696\n",
      "Step    20 of   469: \n",
      "Train loss: 2.32241370112209\n",
      "Test loss: 2.3224913205130626\n",
      "Step    25 of   469: \n",
      "Train loss: 2.320797811120243\n",
      "Test loss: 2.3208922087135964\n",
      "Step    30 of   469: \n",
      "Train loss: 2.319374638088679\n",
      "Test loss: 2.3194546093374995\n",
      "Step    35 of   469: \n",
      "Train loss: 2.3181185742556036\n",
      "Test loss: 2.3181966361352955\n",
      "Step    40 of   469: \n",
      "Train loss: 2.3166200609530434\n",
      "Test loss: 2.3167465682757102\n",
      "Step    45 of   469: \n",
      "Train loss: 2.3152667546676375\n",
      "Test loss: 2.315361705877013\n",
      "Step    50 of   469: \n",
      "Train loss: 2.31402640019433\n",
      "Test loss: 2.3141675844030867\n",
      "Step    55 of   469: \n",
      "Train loss: 2.3130829960612926\n",
      "Test loss: 2.3131754337731056\n",
      "Step    60 of   469: \n",
      "Train loss: 2.3119134680699496\n",
      "Test loss: 2.3120003009246566\n",
      "Step    65 of   469: \n",
      "Train loss: 2.310658337706226\n",
      "Test loss: 2.3107912095926575\n",
      "Step    70 of   469: \n",
      "Train loss: 2.309151912139634\n",
      "Test loss: 2.309280771320149\n",
      "Step    75 of   469: \n",
      "Train loss: 2.307906532691697\n",
      "Test loss: 2.308051731626866\n",
      "Step    80 of   469: \n",
      "Train loss: 2.3065466011984874\n",
      "Test loss: 2.306733190003088\n",
      "Step    85 of   469: \n",
      "Train loss: 2.305186049412873\n",
      "Test loss: 2.305316672486774\n",
      "Step    90 of   469: \n",
      "Train loss: 2.3037594859882935\n",
      "Test loss: 2.3038626788026195\n",
      "Step    95 of   469: \n",
      "Train loss: 2.3023545641010092\n",
      "Test loss: 2.302505753808102\n",
      "Step   100 of   469: \n",
      "Train loss: 2.3008670180530872\n",
      "Test loss: 2.3010239237445895\n",
      "Step   105 of   469: \n",
      "Train loss: 2.299488809149144\n",
      "Test loss: 2.299638784537881\n",
      "Step   110 of   469: \n",
      "Train loss: 2.2978646189479504\n",
      "Test loss: 2.297992649724928\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main_mnist.py\", line 172, in <module>\n",
      "    run_mnist(config['mnist'])\n",
      "  File \"main_mnist.py\", line 152, in run_mnist\n",
      "    print_step=config['print_step'])\n",
      "  File \"/mnt/d/lesenok/Linux/CNN/msalib/train.py\", line 91, in train\n",
      "    batch_size, buffer_size, print_step)\n",
      "  File \"/mnt/d/lesenok/Linux/CNN/msalib/train.py\", line 56, in _train_epoch\n",
      "    train_loss = self._compute_loss(session, trainset, buffer_size)\n",
      "  File \"/mnt/d/lesenok/Linux/CNN/msalib/train.py\", line 44, in _compute_loss\n",
      "    loss += session.run(self.loss, feed_dict)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n"
     ]
    }
   ],
   "source": [
    "!python3 -u main_mnist.py | tee mnist.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Run CNN on MNIST\n",
      "====================================================================================================\n",
      "Config:\n",
      "dataset_name         (mnist)\n",
      "seed                 (1)\n",
      "verbosity            (WARN)\n",
      "print_step           (100)\n",
      "lift_dimension       (1)\n",
      "num_train            (60000)\n",
      "num_test             (10000)\n",
      "num_channels         (5)\n",
      "filter_size          (3)\n",
      "padding              (same)\n",
      "num_layers           (2)\n",
      "rho                  (0.03)\n",
      "reg                  (0.001)\n",
      "activation           (tanh)\n",
      "delta                (0.5)\n",
      "batch_size           (128)\n",
      "buffer_size          (512)\n",
      "num_epochs           (1)\n",
      "sgd_lr               (0.01)\n",
      "adagrad_lr           (0.01)\n",
      "adam_lr              (0.001)\n",
      "msa_maxiter          (10)\n",
      "msa_perturb_init     (0.0)\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_0 (Conv2D)            (None, 28, 28, 5)         50        \n",
      "_________________________________________________________________\n",
      "avg_pool_0 (AveragePooling2D (None, 14, 14, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_init (Conv2D)         (None, 14, 14, 5)         230       \n",
      "_________________________________________________________________\n",
      "avg_pool_1 (AveragePooling2D (None, 7, 7, 5)           0         \n",
      "_________________________________________________________________\n",
      "residualconv2d_0 (ResidualCo (None, 7, 7, 5)           230       \n",
      "_________________________________________________________________\n",
      "residualconv2d_1 (ResidualCo (None, 7, 7, 5)           230       \n",
      "_________________________________________________________________\n",
      "lower (Lower)                (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                500       \n",
      "=================================================================\n",
      "Total params: 1,240\n",
      "Trainable params: 1,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "====================================================================================================\n",
      "Trainer: MSATrainer (MSA_trainer)\n",
      "Settings: {'maxiter': 10, 'perturb_init': 0.0}\n",
      "====================================================================================================\n",
      "Epoch: init\n",
      "Train loss: 2.3245041087522345\n",
      "Test loss: 2.3243458390235903\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-951ca62485c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config.yml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mnist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-836a9731c1d5>\u001b[0m in \u001b[0;36mrun_mnist\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'buffer_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         print_step=config['print_step'])\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# SGD trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/ML/Bauman ml code/Olesya_task/msalib/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, session, trainset, testset, batch_size, num_epochs, buffer_size, print_step)\u001b[0m\n\u001b[1;32m     89\u001b[0m             self._train_epoch(\n\u001b[1;32m     90\u001b[0m                 \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 batch_size, buffer_size, print_step)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/ML/Bauman ml code/Olesya_task/msalib/train.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, session, trainset, testset, batch_size, buffer_size, print_step)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprint_step\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mprint_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step {:5} of {:5}: '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/ML/Bauman ml code/Olesya_task/msalib/train.py\u001b[0m in \u001b[0;36m_compute_loss\u001b[0;34m(self, session, dataset, buffer_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             }\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = yaml.safe_load(open(\"config.yml\"))\n",
    "    run_mnist(config['mnist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
